\section{Soft Actor-Critic}\label{SACsection}

\subsection{Maximum Entropy RL}\label{maximumentropyrlsubsection}

\begin{definition}
Задачей \emph{Maximum Entropy RL} является максимизация функционала
\begin{equation}\label{merl}
    J_{\soft}(\pi) \coloneqq \E_{\Traj \sim \pi} \sum_{t \ge 0} \gamma^t \left[ r_t + \alpha \entropy(\pi(\cdot \mid s)) \right] \to \max_\pi
\end{equation}
где $\alpha$ --- гиперпараметр, называемый \emph{температурой} (temperature).
\end{definition}

\begin{proposition}
Задача \eqref{merl} эквивалентна
\begin{equation*}
    J_{\soft}(\pi) \coloneqq \E_{\Traj \sim \pi} \sum_{t \ge 0} \gamma^t \left[ r_t - \alpha \log \pi(a_t \mid s_t) \right] \to \max_\pi,
\end{equation*}
\begin{proof}
Следует из определения энтропии \eqref{entropydef}; ведь мат.ожидание по действиям присутствует в мат.ожидании по траекториям.
\end{proof}
\end{proposition}

Интуиция такого функционала проста: мы говорим, что хотим не просто оптимальную стратегию, а самую стохастичную около-оптимальную стратегию (коэффициент $\alpha$ масштабирует добавку и определяет важность добавленного слагаемого). Цель --- избегать локальных оптимумов в среде. 

\begin{exampleBox}[righthand ratio=0.3, sidebyside, sidebyside align=center, lower separated=false]{}
Представим, что у агента есть два пути, и по мере углубления награда на каждом пути одинаково растёт. Пусть первый путь заканчивается тупиком и суммарно позволяет набрать не более +100, а на втором тупик стоит чуть дальше и даёт +110. Во время обучения агент может уловить награду вдоль первого пути и учиться углубляться в него, игнорируя исследование второго пути, даже если агент умеет набирать там награду как на первом; за счёт бонуса за наиболее стохастичную стратегию агент мотивирован в течение обучения в начале эпизодов случайно выбирать между двумя путями. То есть, энтропийный бонус помогает избегать подобных <<локальных максимумов>> в среде.

\tcblower
\includegraphics[width=\textwidth]{Images/twopaths.png}
\end{exampleBox}

Можно считать, что в данном фреймворке мы на самом деле лишь чуть-чуть модифицировали награду в среде:
\begin{equation}\label{softreward}
r_{\soft}(s, a) \coloneqq r(s, a) - \alpha \log \pi(a \mid s)
\end{equation}
Построенную теорию теперь нужно перепроверять, поскольку награда $r$ стала зависеть напрямую от вероятностей, выдаваемых стратегией (такого в формализме MDP не предполагалось), да и очевидно, что не все утверждения переносятся на новую постановку:

\begin{proposition}
Оптимальной детерминированной стратегии может не существовать.
\begin{proof}
В MDP, где награда всегда 0, оптимальна стратегия с максимальной энтропией.
\end{proof}
\end{proposition}

Однако принцип Policy Iteration --- чередование этапов оценивания и улучшения стратегии --- остаётся для этой альтернативной постановки задачи неизменным. Чтобы подчеркнуть, что речь идёт именно о постановке Maximum Entropy RL, ко всем терминам из обычной теории добавляют слово \emph{soft} (<<мягкие>>), подразумевая, что энтропийный бонус <<сглаживает>> стратегию, предупреждая её вырождение в детерминированную. Поэтому будем обозначать оценочные функции в рамках данного фреймворка как $Q^\pi_{\soft}, V^\pi_{\soft}$. Также без ограничения общности далее будем считать $\alpha \HM= 1$, так как всегда можно перемасштабировать награду $r(s, a)$ без изменения задачи.

\begin{remark}
Гиперпараметр температуры является основным недостатком идеи Maximum Entropy RL --- на практике его обычно очень сложно подбирать, а он существенно влияет на то, какая стратегия получится в итоге обучения.
\end{remark}

Мы также должны договориться о том, в какой момент во время взаимодействия приходит энтропийный бонус. Формула \eqref{softreward} предполагает, что после выбора действия $a$ из состояния $s$ помимо награды $r(s, a)$ агент ещё поощряется $\log \pi(a \mid s)$; однако мы договоримся по-другому\footnote{такая договорённость удобнее, поскольку после сэмплирования действия $a$ в состоянии $s$ неудобно учитывать зависимость награды от распределения $\pi(\cdot \HM\mid s)$.}. Будем считать, что агент, приходя в некоторое состояние $s$, получает бонус в размере $\entropy(\pi(\cdot \mid s)) \HM\coloneqq - \E_{a \sim \pi(a \mid s)} \log \pi(a \mid s)$ (если состояние не терминальное), затем сэмплирует действие $a$, получает бонус $r(s, a)$ и потом наблюдает $s'$. В такой договорённости уравнения для мягких оценочных функций выглядят так:

\begin{theorem}[Мягкие уравнения связи]
\begin{equation}\label{softQV}
Q^\pi_{\soft}(s, a) = r(s, a) + \gamma \E_{s'} V^\pi_{\soft}(s')
\end{equation}
\begin{equation}\label{softVQ}
V^\pi_{\soft}(s) = \E_{\pi(a \mid s)} \left[ Q^\pi_{\soft}(s, a) - \log \pi(a \mid s) \right]
\end{equation}
\begin{proof}
По определению с учётом договорённости.
\end{proof}
\end{theorem}

\begin{theorem}[Мягкие уравнения Беллмана (soft Bellman equations)]
\begin{equation}\label{softQQ}
Q^\pi_{\soft}(s, a) = r(s, a) + \gamma \E_{s'} \E_{a'} \left[ Q^\pi_{\soft}(s', a') - \log \pi(a' \mid s') \right]
\end{equation}
\begin{equation}\label{softVV}
V^\pi_{\soft}(s) = \E_{a} \left[ r(s, a) - \log \pi(a \mid s) + \gamma \E_{s'} V^\pi_{\soft}(s') \right]
\end{equation}
\end{theorem}

\subsection{Soft Policy Evaluation}

Имея на руках мягкие уравнения Беллмана, можно сразу же построить алгоритм оценивания стратегии (\emph{Soft Policy Evaluation}), обучения мягкой Q- или V-функции по заданной стратегии $\pi$. Технически, необходимо только проверить, что правые части мягких уравнений Беллмана являются сжатиями:

\begin{theorem}
Операторы, стоящие в правой части мягких уравнений Беллмана, являются сжимающими с коэффициентом $\gamma$ по метрике
$$\rho(V_1, V_2) \coloneqq \max\limits_{s} | V_1(s) - V_2(s) |$$
\begin{proof}
Покажем для мягкой V-функции. Пусть $\B_{\soft}$ --- оператор, стоящий в правой части \eqref{softVV}, и пусть даны две V-функции $V_1, V_2$.
Тогда:
$$
| [\B_{\soft} V_1](s) - [\B_{\soft} V_2](s) | = \gamma | \E_{a} \E_{s'} \left[ V_1(s') - V_2(s') \right] |,
$$
поскольку энтропия стратегии $\pi$ вместе с наградой за шаг одинакова для $V_1$ и $V_2$ и потому сокращается. Дальше, как и для обычных V-функций, можно просто оценить это выражение сверху $\gamma \rho(V_1, V_2)$, заканчивая доказательство.
\end{proof}
\end{theorem}

\begin{proposition}
Метод простой итерации сходится к единственному решению мягких уравнений Беллмана из любого начального приближения.
\end{proposition}

Итак, мы уже можем сразу построить процедуру обучения критика. Рассмотрим обучение $Q_\omega(s, a) \HM \approx Q^\pi_{\soft}(s, a)$ с одношаговых оценок в off-policy режиме, то есть будем просто решать мягкое уравнение Беллмана \eqref{softQQ}. Тогда для заданного перехода $\T \HM= (s, a, r, s')$ целевая переменная строится как
$$y(\T) \coloneqq r + \gamma \E_{a' \sim \pi(a' \mid s')} \left[ Q_\omega(s', a') - \log \pi(a' \mid s') \right]$$

В непрерывных пространствах действий взять мат.ожидание по $a'$ аналитически может не удастся (например, если $\pi$ моделируется гауссианой). Можно, как всегда, тоже заменить его на Монте-Карло оценку (по возможности, взяв аналитически энтропию). Но при желании есть возможность ещё немножко сбить дисперсию, заведя вспомогательную нейросеть для аппроксимации $V_\psi(s) \HM \approx V^\pi_{\soft}(s)$: то есть, фактически, просто учить мат.ожидание $\E_{a'}$.

В таком варианте таргеты для критика (Q-функция с параметрами $\omega$) и для вспомогательной V-функции выглядят следующим образом: для заданного перехода $\T \HM= (s, a, r, s')$, взятого из буфера, генерируем $a_\pi$ из текущей версии стратегии $\pi$ и запоминаем вероятность $\pi(a_\pi \HM\mid s)$, после чего вычисляем несмещённые оценки правых частей уравнений связи \eqref{softVQ} и \eqref{softQV}:
$$y_Q(\T) \coloneqq r + \gamma V_\psi(s')$$
$$y_V(\T) \coloneqq Q_\omega(s, a_\pi) - \log \pi(a_\pi \mid s)$$
И с такими таргетами, как обычно, решаем задачу регрессии с функцией потерь MSE. Самое главное --- соответствующее действие $a_\pi$ для таргета $y_V$ брать не из буфера, а сгенерировать из текущей версии стратегии, поскольку иначе мы некорректно оценим среднее $\E_{a \sim \pi}$.

Естественно, в любых одношаговых таргетах оценка очень смещённая, и поэтому для стабилизации полезны две стандартные эвристики --- таргет-сети и clipped double оценки (см. раздел \ref{subsec:clippedtwin}). В реализациях места, где используются таргет-сети, могут различаться, поскольку однозначно сложно сказать, где их лучше всего использовать, но они точно должны быть; без таргет-сетей эти алгоритмы не заработают из-за сильной смещённости всех оценок.

\subsection{Soft Policy Improvement}

Теперь обсудим, как обучать актёра. Напомним, что вся теория оптимизации стратегии сводится к policy improvement и нам нужен аналог теоремы \ref{th:policyimprovement}. Построить этот аналог можно из простых соображений: вот нам дана стратегия $\textcolor{ChadBlue}{\pi_1}$ и её мягкая оценочная функция $\textcolor{ChadBlue}{Q^{\pi_1}_{\soft}}(s, a)$. Как можно улучшить эту стратегию из состояния $s$? Сейчас $\textcolor{ChadBlue}{\pi_1}$ набирает из $s$ в среднем $\textcolor{ChadBlue}{V^{\pi_1}_{\soft}}(s)$. А сколько мы будем в среднем набирать, если сейчас в состоянии $s$ сменим стратегию на $\textcolor{ChadPurple}{\pi_2}$ (теперь с учётом энтропийного бонуса, который мы тогда получим в $s$ за эту новую стратегию $\textcolor{ChadPurple}{\pi_2}$), а в будущем будем вести себя ну по крайней мере не хуже, чем стратегия $\textcolor{ChadBlue}{\pi_1}$?

\begin{theoremBox}[label=th:softpi]{Soft Policy Improvement}
Пусть стратегии $\textcolor{ChadBlue}{\pi_1}$ и $\textcolor{ChadPurple}{\pi_2}$ таковы, что для всех состояний $s$ выполняется:
$$\E_{\textcolor{ChadPurple}{\pi_2}(a \mid s)} \textcolor{ChadBlue}{Q^{\pi_1}_{\soft}}(s, a) + \entropy(\textcolor{ChadPurple}{\pi_2}(\cdot \mid s)) \ge \textcolor{ChadBlue}{V^{\pi_1}_{\soft}}(s),$$
тогда $\textcolor{ChadPurple}{\pi_2} \succeq \textcolor{ChadBlue}{\pi_1}$ с учётом энтропийного бонуса; если хотя бы для одного $s$ неравенство выполнено строго, то $\textcolor{ChadPurple}{\pi_2} \succ \textcolor{ChadBlue}{\pi_1}$.
\begin{proof}
Полностью аналогично доказательству в обычном случае (теорема \ref{th:policyimprovement}).
Покажем, что $\textcolor{ChadPurple}{V^{\pi_2}_{\soft}}(s) \ge \textcolor{ChadBlue}{V^{\pi_1}_{\soft}}(s)$ для любого $s$:
\begin{align*}
\textcolor{ChadBlue}{V^{\pi_1}_{\soft}}(s) \le \{ \text{по построению $\textcolor{ChadPurple}{\pi_2}$} \} &\le \E_{\textcolor{ChadPurple}{\pi_2}(a \mid s)} \textcolor{ChadBlue}{Q^{\pi_1}_{\soft}}(s, a) + \entropy(\textcolor{ChadPurple}{\pi_2}(\cdot \mid s)) = \\
= \{ \text{связь QV \eqref{softQV}} \} &= \E_{\textcolor{ChadPurple}{\pi_2}(a \mid s)} \left[ r  + \entropy(\textcolor{ChadPurple}{\pi_2}(\cdot \mid s)) + \gamma \E_{s'} \textcolor{ChadBlue}{V^{\pi_1}_{\soft}}(s') \right]  \le \\
\le \{ \text{применяем это же неравенство рекурсивно} \} &= \E_{\textcolor{ChadPurple}{\pi_2}(a \mid s)} \Bigl[ r + \entropy(\textcolor{ChadPurple}{\pi_2}(\cdot \mid s)) + \\ &+ \E_{s'} \E_{\textcolor{ChadPurple}{\pi_2}(a' \mid s')} \left[ \gamma r' + \gamma \entropy(\textcolor{ChadPurple}{\pi_2}(\cdot \mid s')) + \gamma^2 \E_{s''} \textcolor{ChadBlue}{V^{\pi_1}_{\soft}}(s'') \right] \Bigr] \le \\
\le \{ \text{раскручиваем цепочку далее} \} &\le \dots \le \E_{\textcolor{ChadPurple}{\Traj \sim \pi_2} \mid s_0 = s} \sum_{t \ge 0} \gamma^t r_t + \gamma^t \entropy(\textcolor{ChadPurple}{\pi_2}(\cdot \mid s_t)) = \\
= \{ \text{по определению мягкой V-функции} \} &= \textcolor{ChadPurple}{V^{\pi_2}_{\soft}}(s)
\end{align*}
Если для какого-то $s$ неравенство из условия теоремы было выполнено строго, то для него первое неравенство в этой цепочке рассуждений выполняется строго, и, значит, $\textcolor{ChadPurple}{V^{\pi_2}_{\soft}}(s) > \textcolor{ChadBlue}{V^{\pi_1}_{\soft}}(s)$.
\end{proof}
\end{theoremBox}

Таким образом, аналог общей схемы Generalized Policy Iteration в задаче Maximum Entropy RL выглядит так:
\begin{itemize}
    \item \textbf{Soft Policy Evaluation} заключается в обучении аппроксимации мягкой оценочной функции $Q_\omega \HM \approx Q^{\pi}_{\soft}$ для текущей стратегии $\pi$;
    \item \textbf{Soft Policy Improvement} заключается в оптимизации следующего функционала (для разных состояний $s$):
    \begin{equation}\label{softpolicyimprov}
    \E_{\pi(a \mid s)} Q_\omega(s, a) + \entropy(\pi(\cdot \mid s)) \to \max_{\pi}
    \end{equation}
\end{itemize}

Соответственно, для обучения актёра, заданного параметрической стратегией $\pi_\theta(a \HM\mid s)$, нужно просто оптимизировать не среднее значение Q-функции, а учесть дополнительно добавку энтропийного бонуса в рассматриваемом состоянии. У задачи оптимизации \eqref{softpolicyimprov} есть один важный альтернативный вид:

\begin{theoremBox}[label=softpi_is_kl_minimization]{}
Задача \eqref{softpolicyimprov} эквивалентна задаче
$$\KL(\pi_\theta(\cdot \mid s) \parallel \exp Q_\omega(s, \cdot)) \to \min_{\theta},$$
где $\exp Q_\omega(s, \cdot)$ --- ненормированное распределение над действиями.
\beginproof
Обозначим нормировочную константу распределения $\exp Q_\omega(s, \cdot)$ как $Z_\omega(s) \HM\coloneqq \int\limits_\A \exp Q\omega(s, a) \diff a$. Тогда:
$$\KL(\pi_\theta(\cdot \mid s) \parallel \exp Q_\omega(s, \cdot)) = \E_{a \sim \pi_\theta(a \mid s)} \log \pi_\theta(a \mid s) - Q_\omega(s, a) + \log Z_\omega(s)$$
Осталось заметить, что при домножении на минус получим \eqref{softpolicyimprov}: первое слагаемое есть энтропия, а третье слагаемое не зависит от оптимизируемых параметров $\theta$:
\begin{equation*}
\E_{a \sim \pi_\theta(a \mid s)} \log Z_\omega(s) = \log Z_\omega(s) = \const(\theta)    \tagqed
\end{equation*}
\end{theoremBox}

\begin{theorem}[Вид жадной стратегии]
Максимальное значение \eqref{softpolicyimprov} достигается на стратегии
\begin{equation}\label{softgreedy}
\pi(a \mid s) \propto \exp Q_\omega(s, a)
\end{equation}
\begin{proof}
Следует из теоремы \ref{softpi_is_kl_minimization}, поскольку минимум KL-дивергенции достигается в нуле на совпадающих распределениях.
\end{proof}
\end{theorem}

Таким образом мы показали, что актёр просто пытается выучить распределение $\pi_\theta(a \HM\mid s) \HM\propto \exp{Q_\omega(s, \cdot)}$ --- больцмановскую стратегию по отношению к Q-функции \eqref{boltzman}. Мы сталкивались с ней, когда обсуждали способы исследования. Соответственно, в Maximum Entropy RL понятие жадной стратегии немножко другое: нужно не аргмакс по действиям брать, а софтмакс.

На практике второе слагаемое функционала \eqref{softpolicyimprov} --- энтропию $\pi_\theta(a \mid s)$ --- обычно можно рассчитать аналитически для выбранной параметризации $\pi_{\theta}(a \HM\mid s)$. Первое же слагаемое берётся так, как мы обсуждали в разделе \ref{subsec:stochpolicies}: при помощи репараметризационного трюка, если такая возможность есть, и при помощи REINFORCE иначе.

\begin{exampleBox}[label=ex:gaussianactor_sac]{}
Пусть наша стратегия параметризована гауссианой (см. пример \ref{ex:gaussian_policy}). Для неё можно проводить репараметризационный трюк и также можно аналитически посчитать энтропию:
$$\entropy(\N(\mu, \sigma^2I)) = \sum_{i=1}^{A} \log \sigma_i$$

Итого, формула \eqref{softpolicyimprov} в таком случае получается следующей:
$$\E_{\eps \sim \N(0, I)} Q_\omega(s, \mu_\theta(s) + \sigma_\theta(s) \odot \eps) + \sum_{i=1}^A \log \sigma_i(s, \theta) \to \max_\theta $$
\end{exampleBox}

\begin{exampleBox}[label=ex:gaussianmixtureactor_sac]{}
Пусть наша стратегия параметризована смесью гауссиан (см. пример \ref{ex:gaussianmixture_policy}). Тогда для неё не применим репараметризационный трюк, и сложно аналитически посчитать энтропию. Тогда придётся применять REINFORCE, и  формула градиента \eqref{softpolicyimprov} получается следующей:
$$\nabla_\theta = \E_{a \sim \pi_\theta(a \mid s)} \nabla_\theta \log \pi_\theta(a \mid s) \left[ Q_\omega(s, a) - \log \pi_\theta(a \mid s) - b(s) \right],$$
где $b(s)$ --- бэйзлайн, произвольная функция от состояний. Имеет смысл делать её близкой к среднему значению $Q_\omega(s, a) \HM- \log \pi_\theta(a \mid s)$, то есть хорошо выбирать $b(s) \HM\coloneqq V_\omega(s) = \E_{a} \left[ Q_\omega(s, a) \HM- \log \pi_\theta(a \mid s) \right]$.
\end{exampleBox}

\subsection{Soft Actor-Critic (SAC)}

Мы уже можем собрать алгоритм-аналог DDPG для задачи Maximum Entropy RL. В этом алгоритме по аналогии с DDPG есть актёр и критик, критик учится оценивать актёра по одношаговым таргетам, а актёр обучается моделированием soft policy improvement-а. Таким образом, схема работает в off-policy режиме.

Перечислим ряд деталей, с которыми в целом можно играться в этом алгоритме\footnote{существует несколько версий этого алгоритма; в частности, есть версия, где Q-функции учатся через Q-функции, и вспомогательная V-сеть не используется. Тогда нужно заводить таргет-сети для двух Q-сетей.}. При обучении критика используется вспомогательная V-функция, для стабилизации можно ограничиться таргет-сетью (с параметрами $\psi^{-}$) только для V-функции:
$$y_Q(\T) \coloneqq r + \gamma V_{\psi^{-}}(s'),$$
а для борьбы с overestimation bias в таргете для V-функции используется twin оценка аналогично алгоритму TD3 (см.~раздел \ref{subsec:td3}), то есть обучается две Q-функции с параметрами $\omega_1, \omega_2$, и таргет $y_V$ строится по следующей формуле: 
$$y_V(\T) \coloneqq \min_{i = {1, 2}} Q_{\omega_i}(s, a_\pi) - \log \pi(a \mid s),$$
где $a_\pi \sim \pi_\theta(a \mid s)$.

Наконец, в функционале \eqref{softpolicyimprov} актёру предлагается <<взламывать>> минимум из двух Q-функций, то есть использовать twin-оценку и в этой формуле:
$$\E_{\pi_\theta(a \mid s)} \min_{i=1,2} Q_{\omega_i}(s, a) + \entropy(\pi(\cdot \mid s)) \to \max_{\pi}$$

Далее схема приведена для простоты для случая, если актёр моделирует гауссиану (см.~пример \ref{ex:gaussianactor_sac}). Конечно, возможно использование и для смеси гауссиан (см.~пример \ref{ex:gaussianmixture_policy}) при замене в функционале актёра репараметризационного трюка на REINFORCE. 

\begin{algorithm}[label = SACalgorithm]{Soft Actor-Critic (SAC)}
\textbf{Гиперпараметры:} $B$ --- размер мини-батчей, $\beta$ --- параметр экспоненциального сглаживания таргет-сети, $\alpha$ --- температура, $\pi_{\theta}(a \mid s) \coloneqq \N(\mu_{\theta}(s), \sigma_{\theta}(s)^2 I)$ --- гауссова стратегия с параметрами $\theta$, $Q_{\omega_1}(s, a), Q_{\omega_2}(s, a)$ --- две нейросетки с параметрами $\omega_1$ и $\omega_2$, $V_\psi(s)$ --- нейросетка с параметрами $\psi$, SGD-оптимизаторы.

\vspace{0.3cm}
Инициализировать $\theta, \omega_1, \omega_2, \psi$ произвольно \\
Инициализировать таргет-сеть $\psi^- \coloneqq \psi$ \\
Пронаблюдать $s_0$ \\
\textbf{На очередном шаге $t$:}
\begin{enumerate}
    \item выбрать $a_t \sim \pi_\theta(a_t \mid s_t)$
    \item пронаблюдать $r_t$,  $s_{t+1}$, $\done_{t+1}$
    \item добавить пятёрку $(s_t, a_t, r_t, s_{t+1}, \done_{t+1})$ в реплей буфер
    \item засэмплировать мини-батч размера $B$ из буфера
    \item для каждого $s$ из батча засэмплировать шума $\eps(s) \sim \N(0, I)$ и посчитать $\mu(s, \theta)$, $\sigma(s, \theta)$ стратегии $\pi_\theta$
    \item посчитать оценку градиента по параметрам стратегии:
    $$\nabla_\theta \coloneqq \frac{1}{B}\sum_{s \in B} \nabla_\theta \left[ \alpha \sum_{i=1}^A \log \sigma_i(s, \theta) + \min_{i=1,2} Q_{\omega_i}(s, \mu_\theta(s) + \sigma_\theta(s) \odot \eps(s)) \right]$$
    \item делаем шаг градиентного подъёма по $\theta$, используя $\nabla_\theta$
    \item для каждого перехода $\T \coloneqq (s, a, r, s', \done)$ засэмплировать $a_{\pi} \sim \pi_\theta(a_{\pi} \mid s)$ и сохранить вероятности $\pi_\theta(a_\pi \mid s)$
    \item посчитать таргеты:
    $$y_V(\T ) \coloneqq \min_{i = 1, 2} Q_{\omega_i} ( s, a_\pi ) - \alpha \log \pi_\theta(a_\pi \mid s)$$
    $$y_Q(\T ) \coloneqq r + \gamma V_{\psi^{-}}(s')$$
    \item посчитать лоссы:
    $$\Loss_V(\psi) \coloneqq \frac{1}{B}\sum_{\T} \left( V_{\psi}(s') - y_V(\T ) \right) ^2$$
    $$\Loss_{Q1}(\omega_1) \coloneqq \frac{1}{B}\sum_{\T} \left( Q_{\omega_1}(s, a) - y_Q(\T ) \right) ^2$$
    $$\Loss_{Q2}(\omega_2) \coloneqq \frac{1}{B}\sum_{\T} \left( Q_{\omega_2}(s, a) - y_Q(\T ) \right) ^2$$
    \item делаем шаг градиентного спуска по $\psi$, $\omega_1$ и $\omega_2$, используя $\nabla_\psi \Loss_V(\psi)$, $\nabla_\omega \Loss_{Q1}(\omega_1)$ и $\nabla_{\omega_2} \Loss_{Q2}(\omega_2)$ соответственно
    \item обновляем таргет-сеть: $\psi^{-} \leftarrow (1 - \beta) \psi^{-} + \beta \psi$
\end{enumerate}
\end{algorithm}

Можно провести sanity check, убедившись, что при $\alpha \HM\to 0$, мы получаем обычный DDPG (с twin-трюком и для стохастичной стратегии).

\begin{remark}
Консенсуса по поводу того, кто круче --- TD3 (алгоритм \ref{TD3}) или SAC, нету. Считается, что они оба хорошо работают, по крайней мере лучше DDPG, и на разных задачах может победить как первый, так и второй. Недостатком TD3 является некоторая <<костыльность>> предложенных эвристик, а недостатком SAC --- неудобный гиперпараметр температуры.
\end{remark}

\subsection{Аналоги других алгоритмов}

В обычном RL оптимальной стратегией была жадная по отношению к оптимальной Q-функции, то есть та, для которой нельзя провести policy improvement ни в одном состоянии. Аналогичные рассуждения верны и для Maximum Entropy RL, разве что форма жадной стратегии \eqref{softgreedy} теперь другая.

Сформулируем критерий оптимальности Беллмана в задаче Maximum Entropy RL. По аналогии с обычным случаем, введём оптимальные оценочные функции:
$$V^*_{\soft}(s) \coloneqq \max_{\pi} V^{\pi}_{\soft}(s)$$
$$Q^*_{\soft}(s, a) \coloneqq \max_{\pi} Q^{\pi}_{\soft}(s, a)$$

\begin{theorem}[Вид оптимальной стратегии для Maximum Entropy RL]
Оптимальной является единственная стратегия
$$\pi(a \mid s) \propto \exp{Q^*_{\soft}(s, a)}$$
\begin{proof} Проводим рассуждения аналогичные теореме \ref{th:nonstatbellmancriterion}. Откажемся от стационарности и будем рассматривать задачу поиска оптимальной стратегии $\pi_t(a \mid s_0)$ в предположении, что в будущем мы сможем набрать максимально возможную награду $Q^*_{\soft}(s, a, t)$:
$$\E_{\pi_t(a \mid s)} \left[ Q^*_{\soft}(s, a, t) - \log \pi_t(a \mid s) \right] \to \max\limits_{\pi_t(a \mid s)}
$$

Аналогично теореме \ref{softpi_is_kl_minimization} про soft policy improvement, можно заметить, что с точностью до константы, не зависящей от $\pi_t$, оптимизируемое выражение есть:
$$-\KL \left( \pi_t(a \mid s) \parallel \frac{\exp Q^*_{\soft}(s, a, t)}{Z(s, t)} \right) \to \max\limits_{\pi_t(a \HM\mid s)},$$
где $Z(s, t)$ --- нормировочная константа $\exp Q^*_{\soft}(s, a, t)$. Понятно, что оптимум достигается в нуле на $\pi_t(a \HM\mid s)$, совпадающей с этим распределением.

Дальнейшее рассуждение строится как раньше: $Q^*_{\soft}$ от времени не зависит по определению (аналогично утв. \ref{pr:nonstat_optimal_are_stat}), поэтому оптимальная стратегия получается стационарной, следовательно
\begin{equation*}
\pi(a \mid s) \propto \exp Q^*_{\soft}(s, a)
\end{equation*}
Заметим, что в силу однозначного определения $Q^*_{\soft}(s, a)$, такая стратегия в принципе единственна в отличие от обычной задачи RL.
\end{proof}
\end{theorem}

% Мы могли бы решать её и в лоб при помощи вычисления производной по Фреше. То есть: нам нужно решить задачу в функциональном пространстве для фиксированных $s, t$; вспомним, как в таком пространстве функций $\pi_t(\cdot \mid s)$ можно задать скалярное произведение:
% $$\langle f, g \rangle = \int\limits_\A f(a)g(a) \diff a$$
% Обозначим $Q(a) \coloneqq Q^*_{\soft}(s, \cdot, t)$, $\pi(a) \coloneqq \pi_t(\cdot \mid s)$. Тогда наша задача при фиксированных $s, t$ переписывается как:
% $$
% \begin{cases}
% \langle \pi(a), Q(a) \rangle - \langle \pi(a), \log \pi(a) \rangle \to \max\limits_{\pi(a)} \\
% \langle \pi(a), 1 \rangle = 1, \qquad \pi(a) \ge 0 
% \end{cases},
% $$
% где в равенстве 1 --- единичная функция (функция, которая для любого действия выдаёт 1).

% Решаем задачу: составляем лагранжиан (неравенство не учитываем, оптимальное решение удовлетворит ему автоматически).
% $$\mathcal{L}(\pi(a), \mu) = \langle \pi(a), Q(a) \rangle - \langle \pi(a), \log \pi(a) \rangle + \mu \langle \pi(a), 1 \rangle - \mu$$

% Дифференцируем (берём производную по Фреше) по $\pi$ и приравниваем к нулю, получаем
% $$\nabla_\pi \mathcal{L}(\pi, \mu) = Q(a) - \log \pi(a) - 1 + \mu = 0$$
% Итого:
% $$\log \pi_t(a \mid s) = Q^*_{\soft}(s, a, t) - 1 + \mu \quad \Rightarrow \quad \pi_t(a \mid s) \propto \exp Q^*_{\soft}(s, a, t)$$

Имея на руках вид оптимальной стратегии, мы можем получить уравнения оптимальности:

\begin{theorem}
\begin{equation}\label{softV*Q*}
V^*_{\soft}(s) = \log \int\limits_{\A} \exp Q^*_{\soft}(s, a) \diff a
\end{equation}
\begin{proof}
Мы знаем, что оптимальная стратегия имеет вид $\pi^*(a \mid s) \HM= \frac{\exp{Q^*_{\soft}(s, a)}}{Z(s)}$, где
$$Z(s) \coloneqq \int\limits_{\A} \exp Q^*_{\soft}(s, a) \diff a$$
является нормировочной константой. Посчитаем энтропию такого распределения:
$$-\E_{\pi^*(a \mid s)} \log \pi^*(a \mid s) = \int\limits_{\A} \frac{\exp{Q^*_{\soft}(s, a)}}{Z(s)} \left(\log Z(s) - Q^*_{\soft}(s, a) \right) \diff a = \log Z(s) - \E_{\pi^*(a \mid s)} Q^*_{\soft}(s, a)$$

Подставим оптимальную стратегию в мягкое VQ уравнение \eqref{softVQ}, которое справедливо в том числе и для оптимальной стратегии:
\begin{align*}
V^*_{\soft}(s) &= \E_{\pi^*(a \mid s)} \left[ Q^*_{\soft}(s, a) - \log \pi^*(a \mid s) \right] = \\ &= \E_{\pi^*(a \mid s)} Q^*_{\soft}(s, a) - \E_{\pi^*(a \mid s)} Q^*_{\soft}(s, a) + \log Z(s) = \\ &= \log Z(s)
\end{align*}
Вспоминая определение $Z(s)$, получаем доказываемое.
\end{proof}
\end{theorem}

\begin{proposition}
$$Q^*_{\soft}(s, a) = r(s, a) + \gamma \E_{s'} V^*_{\soft}(s')$$
\end{proposition}

\begin{proposition}[Мягкое уравнение оптимальности Беллмана (soft Bellman optimality equations)]
\begin{equation}\label{softQ*Q*}
Q^*_{\soft}(s, a) = r(s, a) + \gamma \E_{s'} \log \int\limits_{\A} \exp Q^*_{\soft}(s', a') \diff a'
\end{equation}
\end{proposition}

\begin{theorem}
Оператор, стоящий в правой части уравнения \eqref{softQ*Q*}, является сжимающим с коэффициентом $\gamma$, и, следовательно, метод простой итерации решения этой системы уравнений сходится из любого начального приближения к единственной неподвижной точке.
\begin{proof}
Пусть даны две Q-функции, $Q_1, Q_2$, и пусть 
$$\rho(Q_1, Q_2) \coloneqq \max\limits_{s, a} | Q_1(s, a) \HM- Q_2(s, a) | \HM< \eps$$
Тогда:
$$
\log \int\limits_{\A} \exp Q_1(s, a) \diff a \le \log \int\limits_{\A} \exp (Q_2(s, a) + \eps) \diff a = \eps + \log \int\limits_{\A} \exp Q_2(s, a) \diff a
$$
Аналогично можно показать, что 
$$
\log \int\limits_{\A} \exp Q_1(s, a) \diff a \ge -\eps + \log \int\limits_{\A} \exp Q_2(s, a) \diff a
$$

Пусть $\B_{\soft}$ --- оператор, стоящий в правой части \eqref{softQ*Q*}. Тогда:
\begin{align*}
| [\B_{\soft} Q_1](s, a) - [\B_{\soft} Q_2](s, a) | &= \gamma | \E_{s'} \left( \log \int\limits_{\A} \exp Q_1(s', a') \diff a' - \log \int\limits_{\A} \exp Q_2(s', a') \diff a' \right) | \le \\ 
&\le \gamma \eps
\end{align*}
Таким образом, $\rho(\B_{\soft} Q_1, \B_{\soft} Q_2)$ уменьшилось по крайней мере в $\gamma$ раз по сравнению с $\rho(Q_1, Q_2)$.
\end{proof}
\end{theorem}

\begin{proposition}
Если $\pi(a \mid s) \propto \exp Q^{\pi}_{\soft}(s, a)$, то она оптимальна.
\begin{proof}
Q-функция такой стратегии удовлетворяет мягкому уравнению оптимальности Беллмана \eqref{softQ*Q*} и в силу единственности его решения совпадает с $Q^*_{\soft}(s, a)$.
\end{proof}
\end{proposition}

Теперь можно построить аналог DQN для задачи Maximum Entropy RL, называемым \emph{Soft Q-learning}. В нём нет отдельной модели актёра, и текущая стратегия просто полагается жадной по отношению к текущему критику. Это также можно интерпретировать как моделирование \emph{Soft Value Iteration} --- решение мягкого уравнения оптимальности \eqref{softQ*Q*}. Тогда таргет для перехода $\T \coloneqq (s, a, r, s')$ строится как
$$y(\T) \coloneqq r + \gamma \log \int\limits_{\A} \exp Q_{\theta^{-}}(s', a') \diff a',$$
где $\theta^{-}$ --- параметры таргет-сети. Интересно, что это соответствует <<учёту>> Больцмановской стратегии исследования внутри оценочной функции (нечто похожее мы делали в табличном алгоритме SARSA в разделе \ref{subsec:sarsa}). Однако, такой подход применим, как и в DQN, только для дискретных пространств действий, поскольку иначе стоящий в формуле интеграл мы аналитически не возьмём. Если мы попробуем завести отдельную нейросеть, чтобы приближать больцмановскую стратегию --- получим SAC.

И, наконец, обсудим аналоги Policy Gradient алгоритмов для Maximum Entropy RL сеттинга. Многошаговые уравнения Беллмана выводятся тривиально --- достаточно учесть в наградах энтропийные бонусы. Ну а аналог формулы policy gradient тоже можно легко угадать; мы же знаем, что если из формулы градиента удалить мат.ожидание по частотам посещения состояний текущей стратегии, мы получаем формулу policy improvement-а. Это свойство в Maximum Entropy RL сохраняется, то есть можно просто к формуле soft policy improvement \eqref{softpolicyimprov} добавить мат.ожидание по частотам посещения состояний. Выпишем, что получится для метода REINFORCE:

\begin{theorem}[Soft Policy Gradient]
$$\nabla_{\theta} J_{\soft}(\theta) = \frac{1}{1 - \gamma} \E_{s \sim d_{\pi_\theta}(s)} \E_{a \sim \pi_\theta(a \mid s)} \nabla_\theta \log \pi_\theta(a \mid s) Q^{\pi}_{\soft}(s, a) + \alpha \nabla_\theta \entropy(\pi_\theta(\cdot \mid s))$$
\begin{proof}
Аналогично доказательству в обычном случае.
\end{proof}
\end{theorem}

Другими словами, сеттинг Maximum Entropy RL <<объясняет>> добавку энтропийного лосса при обучении актёра, но здесь ваджно помнить, что поменялось определение Q-функции: мягкая оценочная функция учитывает получение в будущем энтропийных бонусов (поэтому добавка энтропийного регуляризатора в обычных policy gradient алгоритмах остаётся эвристикой). 




