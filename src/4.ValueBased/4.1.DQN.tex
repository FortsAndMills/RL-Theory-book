\section{Deep Q-learning}

\subsection{Q-сетка}

В сложных средах пространство состояний может быть непрерывно или конечно, но велико (например, пространство всех экранов видеоигры). В таких средах моделировать функции от состояний, будь то стратегии или оценочные функции, мы можем только приближённо при помощи параметрических семейств.

\needspace{7\baselineskip}
\begin{wrapfigure}{r}{0.35\textwidth}
\vspace{-0.3cm}
\centering
\includegraphics[width=0.3\textwidth]{Images/Qnetwork2.png}
\vspace{-0.3cm}
\end{wrapfigure}

Попробуем промоделировать в сложных средах алгоритм \ref{alg:qlearning} Q-learning. Для этого будем приближать оптимальную Q-функцию $Q^*(s, a)$ при помощи нейронной сети $Q_{\theta}(s, a)$ с параметрами $\theta$. Заметим, что для дискретных пространств действий сетка может как принимать действия на входе, так и принимать на вход только состояние $s$, а выдавать $|\A|$ чисел $Q_{\theta}(s, a_1) \dots Q_{\theta}(s, a_{|\A|})$. В последнем случае мы можем за константу находить жадное действие $\pi(s) \HM= \argmax\limits_a Q_{\theta}(s, a)$.

В случае, если пространство действий непрерывно, выдать по числу для каждого варианта уже не получится. При этом, если непрерывное действие подаётся на вход вместе с состоянием, то оптимизировать по нему для поиска максимума или аргмаксимума придётся при помощи серии прямых и обратных проходов (для дискретного пространства --- за $|\A|$ прямых проходов), что вычислительно ни в какие ворота. Поэтому такой вариант на практике не встречается, а алгоритм пригоден в таком виде только для дискретных пространств состояний (позже мы пофиксим это при обсуждении алгоритма DDPG в главе \ref{DDPGsection}).

\begin{remark}
Использование нейросеток позволяет обучаться для сред, в которых состояния $s$ заданы, например, пиксельным представлением экранов видеоигр. Стандартным вариантом архитектуры является несколько (не очень много) свёрточных слоёв, обычно без использования макспулингов (важно не убить информацию о расположении распознанных объектов на экране). Использование батч-нормализаций и дроп-аутов сопряжено с вопросами о том, нужно ли их включать-выключать на этапах генерации таргетов (который, как мы увидим позже, тоже распадается на два этапа), сборе опыта с возможностью исследования и так далее. Чаще их не используют, чем используют, так как можно нарваться на неожиданные эффекты. Важно помнить, что все эти блоки были придуманы для решения немного других задач, и стоит осторожно переносить их в контекст обучения с подкреплением.
\end{remark}

\subsection{Переход к параметрической Q-функции}\label{toregression}

Как обучать параметры $\theta$ нейронной сети так, чтобы $Q_{\theta}(s, a) \HM \approx Q^*(s, a)$? Вообще говоря, мы помним, что мы хотим решать уравнения оптимальности Беллмана \eqref{Q*Q*}, и можно было бы, например, оптимизировать невязку:
$$\left( Q_{\theta}(s, a) - r(s, a) - \gamma \E_{s'} \max_{a'} Q_{\theta}(s', a') \right)^2 \to \min_{\theta}$$
однако мат.ожидание $\E_{s'}$ в формуле берётся по неизвестному нам распределению (а даже если известному, то почти наверняка в сложных средах аналитически ничего не возьмётся) и никак не выносится (несмещённые оценки градиента нас бы устроили).

В Q-learning-е мы смогли с сохранением теоретических гарантий побороться с этим при помощи стохастической аппроксимации, получив формулу \eqref{Qlearningupdate}. Хочется сделать какой-то аналогичный трюк:
$$Q_{\theta}(s, a) \leftarrow Q_{\theta}(s, a) + \alpha\left( r(s, a) + \gamma \max_{a'} Q_{\theta}(s', a') - Q_{\theta}(s, a) \right)$$

Мы уже отмечали ключевое наблюдение о том, что формула стохастической аппроксимации очень напоминает градиентный спуск, а $\alpha$ играет роль learning rate. Да даже условия сходимости \ref{TDconvergence}, собственно, те же, что в стохастическом градиентном спуске\footnote{это не случайность: корни теории одни и те же.}! И, действительно, табличный Q-learning является градиентным спуском для решения некоторой задачи регрессии.

Поскольку это очень принципиальный момент, остановимся в этом месте подробнее. Пусть у нас есть текущая версия $Q_{\theta_k}(s, a)$, и мы хотим проделать шаг метода простой итерации для решения уравнения Q*Q* \eqref{Q*Q*}. Зададим следующую задачу регрессии:
\begin{itemize}
\item входом является пара $s, a$
\item искомым (!) значением на паре $s, a$ --- правая часть уравнения оптимальности Беллмана \eqref{Q*Q*}, т.е.
$$f(s, a) \coloneqq r(s, a) + \gamma \E_{s'} \max_{a'} Q_{\theta_k}(s', a') $$
\item наблюдаемым (<<зашумлённым>>) значением целевой переменной или \emph{таргетом} (target)
\begin{equation}\label{guess}
y(s, a) \coloneqq r(s, a) + \gamma \max_{a'} Q_{\theta_k}(s', a')
\end{equation}
где $s' \sim p(s' \mid s, a)$
\item функцией потерь MSE:
$\Loss(y, \hat{y}) = \frac{1}{2}(y - \hat{y})^2$
\end{itemize}

Заметим, что, как и в классической постановке задачи машинного обучения, значение целевой переменной --- это её <<зашумлённое>> значение: по входу $s, a$ генерируется $s'$, затем от $s'$ считается детерминированная функция, и результат $y$ является наблюдаемым значением. Мы можем для данной пары $s, a$ засэмплировать себе таргет, взяв переход $(s, a, r, s')$ и воспользовавшись сэмплом $s'$, но существенно, что такой таргет --- случайная функция от входа. Принципиально: согласно уравнениям Беллмана, мы хотим выучить мат.ожидание такого таргета, а значит, нам нужна квадратичная функция потерь.

\begin{theorem}
Пусть $Q_{\theta_{k+1}}(s, a)$ --- достаточно ёмкая модель (model with enough capacity), выборка неограниченно большая, а оптимизатор идеальный. Тогда решением вышеописанной задачи регрессии будет шаг метода простой итерации для поиска оптимальной Q-функции:
$$Q_{\theta_{k+1}}(s, a) = r(s, a) + \gamma \E_{s'} \max_{a'} Q_{\theta_k}(s', a')$$
\beginproof
Под <<достаточно ёмкой моделью>> подразумевается, что модель может имитировать любую функцию, например для каждой пары $s, a$ из выборки выдавать любое значение. Найдём оптимальное значение для пары $s, a$: для неё $Q_{\theta_{k+1}}(s, a)$ выдаёт некоторое число, которое должно минимизировать MSE:
$$\frac{1}{2}\E_{s'} \left( y(s, a) - Q_{\theta_{k+1}}(s, a) \right)^2 \to \min_{\theta_{k+1}}$$
Оптимальным константным решением регрессии с MSE как раз является среднее, получаем
\begin{align*}
    Q_{\theta_{k+1}}(s, a) = \E_{s'} y(s, a) = r(s, a) + \gamma \E_{s'} \max_{a'} Q_{\theta_k}(s', a') \tagqed
\end{align*}
\end{theorem}

Другими словами, когда мы решаем задачу регрессии с целевой переменной $y(s, a)$ по формуле \eqref{guess}, мы в среднем сдвигаем нашу аппроксимацию в сторону правой части уравнения оптимальности Беллмана. Полезно наглядно увидеть это в формуле самого градиента. Представим на секунду, что мы решаем задачу регрессии с <<идеальной>>, незашумлённой целевой переменной $f(s, a) \coloneqq r(s, a) + \gamma \E_{s'} \max\limits_{a'} Q_{\theta_k}(s', a')$: для одного примера $s, a$ градиент MSE тогда будет равен:

$$\nabla_\theta \frac{1}{2}(f(s, a) - Q_{\theta}(s, a))^2 = \overbrace{(f(s, a) - Q_{\theta}(s, a))}^{\text{скаляр}}\underbrace{\nabla_\theta Q_{\theta}(s, a)}_{\mathclap{\text{\shortstack{направление увеличения \\ значения $Q_{\theta}(s, a)$}}}}$$

И наша <<зашумлённая>> целевая переменная $y(s, a)$ есть несмещённая оценка $f(s, a)$, поскольку специально построена так, что $\E_{s'} y(s, a) = f(s, a)$. Значит, мы можем несмещённо оценить этот градиент как 
\begin{equation}\label{DQNgradient}
(y(s, a) - Q_{\theta}(s, a))\nabla_\theta Q_{\theta}(s, a) = \nabla_\theta \frac{1}{2} \left( y(s, a) - Q_{\theta}(s, a) \right)^2
\end{equation}
Мы всегда оптимизируем нейронные сети стохастической градиентной оптимизации, поэтому несмещённая оценка градиента нам подойдёт.

В частности, формула Q-learning \eqref{Qlearningupdate} --- это частный случай решения указанной задачи регрессии стохастическим градиентным спуском для специального вида параметрических распределений:

\begin{theorem}
Пусть Q-функция задана <<табличным параметрическим семейством>>, то есть табличкой размера $|\St|$ на $|\A|$, где в каждой ячейке записан параметр $\theta_{s, a}$:
$$Q_{\theta}(s, a) = \theta_{s, a}$$
Тогда формула \eqref{Qlearningupdate} представляет собой градиентный спуск для решения обсуждаемой задачи регрессии.

\begin{proof}
Посчитаем градиент функции потерь на одном объекте $s, a$. Предсказанием модели будет $Q_{\theta}(s, a) \HM= \theta_{s, a}$, то есть градиент предсказания по параметрам модели равен
$$\nabla_\theta Q_{\theta}(s, a) = e_{s, a}$$
где $e_{s, a}$ --- вектор из $\R^{|\St| |\A|}$ из всех нулей с единственной единичкой в позиции, соответствующей паре $s, a$. Теперь посчитаем градиент функции потерь:
$$\nabla_\theta \frac{1}{2} \left( y - Q_{\theta}(s, a) \right)^2 = \left( Q_{\theta}(s, a) - y(s, a) \right) \nabla_\theta Q_{\theta}(s, a) = \left( Q_{\theta}(s, a) - y(s, a) \right) e_{s, a}$$
Итак, градиентный спуск делает следующие апдейты параметров:
$$\theta_{k+1} = \theta_k - \alpha \nabla_\theta \Loss \left( y, Q_{\theta}(s, a) \right) = \theta_k + \alpha \left( y(s, a) - Q_{\theta_k}(s, a) \right) e_{s, a}$$
В этой формуле на каждом шаге обновляется ровно одна ячейка таблички $\theta_{s, a}$, и это в точности совпадает с \eqref{Qlearningupdate}.
\end{proof}
\end{theorem}

Само собой, аналогичный приём мы в будущем будем применять не только для обучения модели $Q^*$, но и любых других оценочных функций. Для этого мы будем составлять задачу регрессии, выбирая в качестве целевой переменной несмещённую оценку правой части какого-нибудь уравнения Беллмана, неподвижной точкой которого является искомая оценочная функция. Этот приём является полным аналогом методов временной разности, поэтому и свойства получаемых алгоритмов будут следовать из классической теории табличных алгоритмов. 

\subsection{Таргет-сеть}

Рассмотренное теоретическое объяснение перехода от табличных методов к нейросетевым, конечно, предполагает, что мы решаем задачу регрессии <<полностью>>, обучая $\theta$ при фиксированных $\theta_k$. <<Замороженные>> $\theta_k$ соответствуют фиксированию формулы целевой переменной $y(s, a)$ \eqref{guess}, то есть фиксированию задачи регрессии. Так мы моделируем один шаг метода простой итерации, и только после этого объявляем выученные параметры модели $\theta_{k+1} \HM\coloneqq \theta$. В этот момент задача регрессии изменится (поменяется целевая переменная), и мы перейдём к следующему шагу метода простой итерации. 

Однако в Q-learning же, как и во всех табличных методах, теория стохастической аппроксимации позволяла <<сменять>> задачу регрессии каждый шаг, используя свежие параметры модели при построении целевой переменной. Конечно, как только мы от <<табличных параметрических функций>> переходим к произвольным параметрическим семействам, все теоретические гарантии сходимости Q-learning-а \ref{TDconvergence} теряются (теоремы сходимости использовали <<табличность>> нашего представления Q-функции). Как только мы используем ограниченные параметрические семейства вроде нейросеток, неидеальные оптимизаторы вроде Адама или не доводим каждый этап метода простой итерации до сходимости, гарантий нет.

Но возникает вопрос: сколько шагов градиентного спуска тратить на решение фиксированной задачи регрессии? Возникает естественное желание по аналогии с табличными методами использовать для построения таргета свежую модель, то есть менять целевую переменную в задаче регрессии каждый шаг после каждого градиентного шага:
$$y(s, a) \coloneqq r + \gamma \max_{a'} Q_{\theta}(s', a')$$
Принципиально важно, что зависимость целевой переменной $y(s, a)$ \eqref{guess} от параметров текущей модели $\theta$ игнорировалась. Если вдруг в неё протекут градиенты, мы будем не только подстраивать прошлое под будущее, но и будущее под прошлое, что не будет являться корректной процедурой. 

Эмпирически легко убедиться, что такой подход нестабилен примерно от слова совсем. Стохастическая оптимизация чревата тем, что после очередного шага модель может стать немножко <<сломанной>> и некоторое время выдавать неудачные значения на ряде примеров. В обычном обучении с учителем этот эффект сглаживается большим количеством итераций: обучение на последующих мини-батчах <<исправляют>> предыдущие ошибки, движение идёт в среднем в правильную сторону, но нет гарантий удачности каждого конкретного шага (на то это и стохастическая оптимизация). Здесь же при неудачном шаге сломанная модель может начать портить целевую переменную, на которую она же и обучается. Это приводит к цепной реакции: плохая целевая переменная начнёт портить модель, которая начнёт портить целевую переменную... Этот эффект особенно ярко проявляется ещё и потому, что мы используем одношаговые целевые переменные, которые, как мы обсуждали в главе \ref{sec:biasvar}, сильно смещены (слишком сильно опираются на текущую же аппроксимацию).

\needspace{7\baselineskip}
\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-0.3cm}
\centering
\includegraphics[width=0.4\textwidth]{Images/targetNetwork.png}
\vspace{-1cm}
\end{wrapfigure}

Для стабилизации процесса одну задачу регрессии нужно решать более одной итерации градиентного спуска; необходимо сделать хотя бы условно 100-200 шагов. Проблема в том, что если таргет строится по формуле $r + \gamma \max\limits_{a'} Q_{\theta}(s, a)$, то после первого же градиентного шага $\theta$ поменяется.

Поэтому хранится копия $Q$-сетки, называемая \emph{таргет-сетью} (target network), единственная цель которой --- генерировать таргеты текущей задачи регрессии для транзишнов из засэмплированных мини-батчей. Традиционно её параметры обозначаются $\theta^{-}$. Итак, целевая переменная в таких обозначениях генерится по формуле
$$y( \T ) \coloneqq r + \gamma \max\limits_{a'} Q_{\theta^{-}}(s', a')$$
а раз в $K$ шагов веса $\theta^{-}$ просто копируются из текущей модели с весами $\theta$ для <<задания>> новой задачи регрессии.

\begin{remark}
При обновлении задачи регрессии график функции потерь типично подскакивает. Поэтому распространённой, но чуть более вычислительно дорогой альтернативой является на каждом шаге устраивать экспоненциальное сглаживание весов таргет сети. Тогда на каждом шаге:
$$\theta^{-} \leftarrow (1 - \alpha) \theta^{-} + \alpha \theta,$$
где $\alpha$ --- гиперпараметр. Такой вариант тоже увеличивает стабильность алгоритма хотя бы потому, что решаемая задача регрессии меняется <<постепенно>>.
\end{remark}

\subsection{Декорреляция сэмплов}

Q-learning после одного шага в среде делал апдейт одного значения таблицы $Q(s, a) \HM \approx Q^*(s, a)$. Сейчас нас такой вариант, очевидно, не устраивает, потому что обучать нейросетки на мини-батчах размера 1 (особенно с уровнем доверия к нашим таргетам) --- это явно плохая идея, но главное, сэмплы $s, a, y$ сильно скоррелированы: в сложных средах последовательности состояний обычно очень сильно похожи. Нейросетки на скоррелированных данных обучаются очень плохо (чаще --- не обучаются вовсе). Поэтому сбор мини-батча подряд с одной траектории здесь не сгодится.

\begin{center}
    \includegraphics[width=\textwidth]{Images/correlated.jpg}
\end{center}

Есть два доступных на практике варианта \emph{декорреляции сэмплов} (sample decorrelation). Первый --- запуск параллельных агентов, то есть сбор данных сразу в нескольких параллельных средах. Этот вариант доступен всегда, по крайней мере, если среда виртуальна; иначе эта опция может быть дороговатой... Второй вариант --- реплей буфер, который, как мы помним, является прерогативой исключительно off-policy алгоритмов.

При наличии реплей буфера агент может решать задачи регрессии, сэмплируя мини-батчи переходов $\T \HM= (s, a, r', s')$ из буфера, затем делая для каждого перехода расчёт таргета\footnote{наличие реплей буфера не избавляет от необходимости использовать таргет-сеть, поскольку вычисление таргета для всего реплей буфера, конечно же, непрактично: реплей буфер обычно огромен (порядка $10^6$ переходов), а одна задача регрессии будет решаться суммарно 100-200 шагов на мини-батчах размера, там, 32 (итого таргет понадобится считать всего для порядка 3000 переходов).} $y(\T) \coloneqq r \HM+ \gamma \max\limits_{a'} Q_{\theta^{-}}(s', a')$, игнорируя зависимость таргета от параметров, и проводя шаг оптимизации по такому мини-батчу. Такой батч уже будет декоррелирован.

\begin{center}
    \includegraphics[width=\textwidth]{Images/decorrelated.jpg}
\end{center}

Почему мы можем использовать здесь реплей буфер? Мы хотим решать уравнения оптимальности Беллмана для всех пар $s, a$. Поэтому в поставленной задаче регрессии мы можем брать тройки $s, a, y$ для обучения из условно произвольного распределения до тех пор, пока оно достаточно разнообразно и нескоррелированно. Единственное ограничение --- внутри $y$ сидит $s'$, который должен приходить из и только из $p(s' \mid s, a)$. Но поскольку среда однородна, любые тройки $s, a, s'$ из любых траекторий, сгенерированных любой стратегией, таковы, что $s' \HM\sim p(s' \HM\mid s, a)$, а значит, $s'$ может быть использован для генерации таргета. Иными словами, в рамках текущего подхода мы, как и в табличном Q-learning-е, находимся в off-policy режиме, и наши условия на процесс порождения переходов $s, a, r, s'$ точно такие же.

\begin{remark}
На практике картинки в какой-то момент начинают переполнять оперативку и начинаются проблемы. Простое решение заключается в том, чтобы удалять самые старые переходы, то есть оставлять самый новый опыт, однако есть альтернативные варианты (см. приоритизированный реплей, раздел \ref{subsec:prioritizedreplay})
\end{remark}

\subsection{DQN}

Собираем алгоритм целиком. Нам придётся оставить $\eps$-жадную стратегию исследования --- с проблемой исследования-использования мы ничего пока не делали, и при стартовой инициализации есть риск отправиться тупить в ближайшую стену. 

Также лишний раз вспомним про то, что в терминальных состояниях обязательно нужно домножаться на $(1 - \done)$, поскольку шансов у приближённого динамического программирования сойтись куда-то без <<отправной точки>> не очень много. 

\begin{algorithm}[label = DQNalgorithm]{Deep Q-learning (DQN)}
\textbf{Гиперпараметры:} $B$ --- размер мини-батчей, $K$ --- периодичность апдейта таргет-сети, $\eps(t)$ --- стратегия исследования, $Q$ --- нейросетка с параметрами $\theta$, SGD-оптимизатор

\vspace{0.3cm}
Инициализировать $\theta$ произвольно \\
Положить $\theta^- \coloneqq \theta$ \\
Пронаблюдать $s_0$ \\
\textbf{На очередном шаге $t$:}
\begin{enumerate}
    \item выбрать $a_t$ случайно с вероятностью $\eps(t)$, иначе $a_t \coloneqq \argmax\limits_{a_t} Q_{\theta}(s_t, a_t)$
    \item пронаблюдать $r_t$,  $s_{t+1}$, $\done_{t+1}$
    \item добавить пятёрку $(s_t, a_t, r_t, s_{t+1}, \done_{t+1})$ в реплей буфер
    \item засэмплировать мини-батч размера $B$ из буфера
    \item для каждого перехода $\T = (s, a, r, s', \done)$ посчитать таргет:
    $$y(\T ) \coloneqq r + \gamma (1 - \done) \max\limits_{a'} Q_{\theta^{-}}(s', a')$$
    \item посчитать лосс:
    $$\Loss(\theta) \coloneqq \frac{1}{B}\sum_{\T} \left( Q_{\theta}(s, a) - y(\T ) \right) ^2$$
    \item сделать шаг градиентного спуска по $\theta$, используя $\nabla_\theta \Loss(\theta)$
    \item если $t \operatorname{mod} K = 0$: $\theta^- \gets \theta$
\end{enumerate}
\end{algorithm}

Все алгоритмы, которые относят к Value-based подходу в RL, будут основаны на DQN: само название <<value-based>> обозначает, что мы учим только оценочную функцию, а такое возможно только если мы учим модель Q-функции и полагаем, что policy improvement проводится на каждом шаге жадно. Неявно в DQN, конечно же, присутствует текущая политика, <<целевая политика>>, которую мы оцениваем --- $\argmax\limits_a Q_{\theta}(s, a)$. Тем не менее ключевое свойство алгоритма, которое стоит помнить --- это то, что он работает в off-policy режиме, и потому потенциально является достаточно sample efficient.

Есть, однако, много причин, почему алгоритм может не раскрыть этот потенциал и <<плохо>> заработать на той или иной среде: либо совсем не обучиться, либо обучаться очень медленно. Со многими из этих недостатков можно пытаться вполне успешно бороться, что и пытаются делать модификации этого алгоритма, которые мы обсудим далее в главе \ref{sec:dqnmods}. 

Выделим отдельно одну особую фундаментальную причину, почему алгоритмы на основе DQN могут не справиться с оптимизацией награды и застрять на какой-то асимптоте, с которой мало что можно сделать, и которая вытекает непосредственно из off-policy режима работы DQN. DQN, как и любые алгоритмы, основанные на одношаговых целевых переменных, страдает от проблемы \emph{накапливающейся ошибки} (compound error). Условно говоря, чтобы распространить награду, полученную в некоторый момент времени, на 100 шагов в прошлое, понадобится провести 100 этапов метода простой итерации. Каждый этап мы решаем задачу регрессии в сильно неидеальных условиях, и ошибка аппроксимации накапливается. На это накладывается необходимость обучать именно Q-функцию, которая должна дифференцировать между действиями.

\begin{example}
Рассмотрим типичную задачу: вы можете перемещаться в пространстве в разные стороны. Если вы отправитесь вправо, через 100 шагов вы получите +1. $Q^*(s, \text{вправо}) \HM= \gamma^{100}$. 

\begin{wrapfigure}{r}{0.3\textwidth}
\vspace{-0.5cm}
\centering
\includegraphics[width=0.3\textwidth]{Images/QisBad.png}
\vspace{-0.9cm}
\end{wrapfigure}

Посмотрим на значение функции в других действиях: например, $Q^*(s, \text{влево}) \HM= \gamma^{102}$. Вот с такой точностью наша $Q^*$ должна обучиться в этом состоянии, чтобы жадная стратегия выбирала правильные действия. Именно поэтому в подобных ситуациях DQN-подобные алгоритмы не срабатывают: из-за проблемы накапливающейся ошибки сигнал на 100 шагов просто не распространяется с такой точностью.
\end{example}

Если же этих проблем <<с точностью>> не возникает, если в среде нет сильно отложенного сигнала или награда за каждый шаг очень информативна, то value-based подход может раскрыть свой потенциал и оказаться эффективнее альтернатив за счёт использования реплей буфера.
