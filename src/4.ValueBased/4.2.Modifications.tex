\section{Модификации DQN}\label{sec:dqnmods}

\subsection{Overestimation Bias}\label{subsec:overestimation}

Отмечалось, что без таргет-сетки (при обновлении задачи регрессии каждый шаг) можно наблюдать, как $Q$ начинает неограниченно расти. Хотя таргет-сетка более-менее справляется с тем, чтобы стабилизровать процесс, предотвратить этот эффект полностью у неё не получается: сравнение обучающейся $Q$ с Монте-Карло оценками и зачастую просто со здравым смыслом выдаёт присутствие в алгоритме заметного \emph{смещения в сторону переоценки} (overestimation bias). Почему так происходит?

Очевидно, что источник проблемы --- оператор максимума в формуле построения таргета: 
$$y(\T ) = r + \gamma \max\limits_{a'} Q_{\theta^{-}}(s', a')$$
При построении таргета есть два источника ошибок: 1) ошибка аппроксимации 2) внешняя стохастика. Максимум здесь <<выбирает>> то действие, для которого из-за ошибки нейросети или из-за везения в прошлых играх $Q(s', a')$ больше правильного значения.

\begin{example}
Вы выиграли в лотерею +100 в силу везения. В вашем опыте нет примеров того, как вы купили билет и проиграли, и поэтому алгоритм будет учить завышенное значение $Q(s, a)$ для действия <<купить билет в лотерею>> (напомним, в утверждении \ref{prop:qlearningempiricmdp} мы отмечали, что запущенный с реплей буфера Q-learning, как и DQN, учит оптимальную Q-функцию для эмпирического MDP). Это завышение из-за везения.

Из-за того, что вы моделируете Q-сетку нейросетью, при увеличении $Q(s, a)$ случайно увеличилось значение ценности действия <<играть в казино>>, поскольку оно опиралось на примерно те же признаки описания состояний. Это завышение из-за ошибки аппроксимации.

А дальше начинается <<цепная реакция>>: при построении таргета для состояния <<казино>> $s'$ в задачу регрессии поступают завышенные значения, предвещающие кучу награды. Завышенное значение начинает распространятся дальше на другие состояния: вы начинаете верить, что вы всегда можете пойти в казино и получить кучу награды.
\end{example}

\begin{proposition}
Рассмотрим одно $s'$. Пусть $Q^*(s', a')$ --- истинные значения, и для каждого действия $a'$ есть модель $Q(s', a') \HM \approx Q^*(s', a')$, которая оценивает это действие с некоторой погрешностью $\eps(a')$:
$$Q(s', a') = Q^*(s', a') + \eps(a')$$
Пусть эти погрешности $\eps(a')$ независимы по действиям, а завышение и занижение оценки равновероятны:
$$\Prob( \eps(a') > 0) = \Prob( \eps(a') < 0) = 0.5$$
Тогда оценка максимума скорее завышена, чем занижена:
$$\Prob \left( \max_{a'} Q(s', a') > \max_{a'} Q^*(s', a') \right) > 0.5$$

\begin{proof}
С вероятностью 0.5 модель переоценит самое хорошее действие, на котором достигается максимум истинной $Q^*(s', a')$; и ещё к тому же с некоторой ненулевой вероятностью возникнет настолько завышенная оценка какого-нибудь из других действий, что $Q(s', a') \HM> \max\limits_{a'} Q^*(s', a')$.
\end{proof}
\end{proposition}

\begin{example}
Предположим, для $s'$ у нас есть три действия, и на самом деле $Q^*(s', a') \HM= 0$ для всех трёх возможных действий. Мы оцениваем каждое действие с некоторой погрешностью. Допустим даже, эта погрешность в среднем равна нулю и распределена по Гауссу: $Q(s', a') \HM \sim \N(0, \sigma^2)$. Тогда случайная величина $\max\limits_{a'} Q(s', a')$ --- взятие максимума из трёх сэмплов из гауссианы --- очевидно такова, что
$$\E \max_{a'} Q(s', a') > 0$$
Хотя истинный максимум $\max\limits_{a'} Q^*(s', a') \HM= 0$. Получается, что, несмотря на то, что каждый элемент оценён несмещённо, максимум по аппроксимациям будет смещён в сторону завышения.
\end{example}

Одно из хороших решений проблемы заключается в разделении (decoupling) двух этапов подсчёта максимума: \emph{выбор действия} (action selection) и \emph{оценка действия} (action evaluation):
$$\max\limits_{a'} Q(s', a') = \overbrace{Q(s', \underbrace{\argmax\limits_{a'} Q(s', a')}_{\text{выбор действия}})}^{\text{оценка действия}}$$

Действительно: мы словно дважды используем одну и ту же погрешность при выборе действия и оценке действия. Из-за скоррелированности ошибки на этих двух этапах и возникает эффект завышения.

Основная идея борьбы с этой проблемой заключается в следующем: предлагается\footnote{когда эту идею предлагали в 2010-ом году в рамках классического RL (тогда нейронки ещё не вставили), то назвали её Double Q-learning, но сейчас под Double DQN подразумевается алгоритм 2015-го года (см.~раздел \ref{subsec:doubledqn}), а этот трюк иногда именуется <<близнецами>> (Twin). Правда, в последнее время из-за алгоритма Twin Delayed DDPG (см. раздел \ref{subsec:td3}) под словом Twin понимается снова не эта формула, и с названиями есть небольшая путаница...} обучать два приближения Q-функции параллельно и использовать аппроксимацию Q-функции <<независимого близнеца>> для этапа оценивания действия:
$$\textcolor{ChadBlue}{y_1}(\T ) \coloneqq r + \gamma \textcolor{ChadPurple}{Q_{\theta_2}}(s', \argmax\limits_{a'} \textcolor{ChadBlue}{Q_{\theta_1}}(s', a'))$$
$$\textcolor{ChadPurple}{y_2}(\T ) \coloneqq r + \gamma \textcolor{ChadBlue}{Q_{\theta_1}}(s', \argmax\limits_{a'} \textcolor{ChadPurple}{Q_{\theta_2}}(s', a'))$$

Если обе аппроксимации Q-функции идеальны, то, понятное дело, мы всё равно получим честный максимум. Однако, если оба DQN честно запущены параллельно и даже собирают каждый свой опыт (что в реальности, конечно, дороговато), их ошибки аппроксимации и везения будут в <<разных местах>>. В итоге, Q-функция близнеца выступает в роли более пессимистичного критика действия, выбираемого текущей Q-функцией. 

\begin{remark}
Если сбор уникального опыта для каждого из близнецов не организуется, Q-функции всё равно получаются скоррелированными. Как минимум, можно сэмплировать для обучения сеток разные мини-батчи из реплей буфера, если он общий.
\end{remark}

Интуиция, почему <<независимая>> Q-функция близнеца используется именно для оценки действия, а не наоборот для выбора: если из-за неудачного градиентного шага наша сетка $ \textcolor{ChadBlue}{Q_{\theta_1}}$ пошла куда-то не туда, мы не хотим, чтобы плохие значения попадали в её же таргет $\textcolor{ChadBlue}{y_1}$. Плохие действия в таргет пусть попадают: пессимистичная оценка здесь предпочтительнее.

\begin{remark}
На практике таргет-сети в такой модели всё равно используют, и в формулах целевой переменной всюду используются именно <<замороженные>> $\textcolor{ChadBlue}{Q_{\theta^-_1}}$ и $\textcolor{ChadPurple}{Q_{\theta^-_2}}$. Это ещё больше противодействует цепным реакциям. Без них всё равно может быть такое, что сломанная $\textcolor{ChadBlue}{Q_{\theta_1}}$ поломает $\textcolor{ChadPurple}{y_2}$, та поломает $\textcolor{ChadPurple}{Q_{\theta_2}}$, та поломает $\textcolor{ChadBlue}{y_1}$, а та в свою очередь продолжить ломать $\textcolor{ChadBlue}{Q_{\theta_1}}$, хотя, конечно, такая <<цепь>> менее вероятна, чем в обычном DQN.
\end{remark}

\begin{example}
На картинке для каждого из четырёх действий указаны значения идеальной $Q^*(s', a')$ и две аппроксимации $Q_1, Q_2$. Каждая аппроксимация условно выдаёт значение с погрешностью, которая в среднем равна нулю (вероятности завышенной оценки или заниженной оценки равны).

\begin{center}
    \includegraphics[width=0.6\textwidth]{Images/OverestimationIssue.png}
\end{center}

Видно, что обе аппроксимации оценивают максимум по действиям завышенно. Но если одна из аппроксимаций выберет действие (то $a'$, на котором достигается её максимум), а другая аппроксимация выдаст значение в выбранном индексе, то получится более близкое к адекватной оценки истинного максимума значение.
\end{example}

\subsection{Twin DQN}\label{subsec:clippedtwin}

Разовьём интуицию дальше. Вот мы строим таргет для $Q_{\theta_1}$. Мы готовы выбрать при помощи неё же действия, но не готовы оценить их ею же самой в силу потенциальной переоценки. Для этого мы и берём <<независимого близнеца>> $Q_{\theta_2}$. Но что, если он выдаёт ещё больше? Что, если его оценка выбранного действия, так случилась, потенциально ещё более завышенная? Давайте уж в таких ситуациях всё-таки брать то значение, которое поменьше! Получаем следующую интересную формулу, которую называют twin-оценкой (или ещё clipped double оценкой):
$$\textcolor{ChadBlue}{y_1}(\T ) \coloneqq r + \gamma \min_{i = \textcolor{ChadBlue}{1}, \textcolor{ChadPurple}{2}}Q_{\theta_i}(s', \argmax\limits_{a'} \textcolor{ChadPurple}{Q_{\theta_2}}(s', a'))$$
$$\textcolor{ChadPurple}{y_2}(\T ) \coloneqq r + \gamma \min_{i = \textcolor{ChadBlue}{1}, \textcolor{ChadPurple}{2}}Q_{\theta_i}(s', \argmax\limits_{a'} \textcolor{ChadBlue}{Q_{\theta_1}}(s', a'))$$

Это очень забавная формула, поскольку она говорит бороться с проблемой <<клин клином>>: зная, что взятие максимума по аппроксимациям приводит к завышению, мы вводим в оценку искусственное занижение, добавляя в формулу минимум по аппроксимациям! По сути, это формула <<ансамблирования>> Q-функций: только вместо интуитивного среднего берём минимум, <<для борьбы с максимумом>>. 

\begin{remark}
Конечно, в отличие от обычного ансамблирования в машинном обучении, такой подход плохо масштабируется с увеличением числа обучаемых Q-функций (обычно учат всё-таки только две). В случае ансамбля имеет смысл брать не среднее, и не минимум, а, например, какой-то квантиль, более близкий к минимуму, чем к медиане --- но возникает неудобный гиперпараметр, что же именно брать.
\end{remark}

\subsection{Double DQN}\label{subsec:doubledqn}

Запускать параллельно обучение двух сеток дороговато, а при общем реплей буфере корреляция между ними всё равно будет. Поэтому предлагается простая идея: запускать лишь один DQN, а в формуле таргета для оценивания вместо <<близнеца>> использовать таргет-сеть. То есть: пусть $\theta$ --- текущие веса, $\theta^{-}$ --- веса таргет-сети, раз в $K$ шагов копирующиеся из $\theta$. Тогда таргет вычисляется по формуле:
$$y(\T ) \coloneqq r + \gamma Q_{\theta^{-}}(s', \argmax\limits_{a'} Q_{\theta}(s', a'))$$

Хотя понятно, что таргет-сеть и текущая сетка очень похожи, такое изменение формулы целевой переменной всё равно <<избавляет>> нас от взятия оператора максимума; эмпирически оказывается, что такая декорреляция действительно помогает стабилизации процесса. При этом, в отличие от предыдущих вариантов, такое изменение бесплатно: не требует обучения второй нейросети.

\begin{remark}
Если обучение второй Q-сети тяжеловесно, то рекомендуется использовать формулу Double DQN. Если же речь идёт об обучении маленьких полносвязных нейросеток, то вероятно, обучение <<ансамбля>> хотя бы из двух Q-функций по крайней мере с общего буфера не должно быть особо дорогим, и тогда имеет смысл пользоваться Twin-оценкой из раздела \ref{subsec:clippedtwin}.
\end{remark}

\subsection{Dueling DQN}\label{subsec:duelingdqn}

Рассмотрим ещё одну очевидную беду Q-обучения на примере. 

\begin{example}
Вы в целях исследования попробовали кинуться в яму $s$. Сидя в яме, вы попробовали $a$ --- поднять правую руку. В яме холодно и грустно, поэтому вы получили -100. Какой вывод делает агент? Правильно: для этого состояния $s$ оценку этого действия $a$ нужно понизить. Остальные не трогать; оценка самого состояния (по формуле \eqref{V*Q*} --- максимум Q-функции по действиям) скорее всего не изменится, и надо бы вернуться в это состояние и перепроверить ещё и все остальные действия: попробовать поднять левую руку, свернуться калачиком...
\end{example}

В сложных MDP ситуация зачастую такова, что получение негативной награды в некоторой области пространства состояний означает в целом, что попадание в эту область нежелательно. Верно, что с точки зрения теории остальные действия должны быть <<исследованы>>, но неудачный опыт должен учитываться внутри оценки самого состояния; иначе агент будет возвращаться в плохие состояния с целью перепробовать все действия без понимания, что удача здесь маловероятна, вместо того, чтобы исследовать те ветки, где негативного опыта <<не было>>. Понятно, что проблема тем серьёзнее, чем больше размерность пространства действий $|\A|$.

\needspace{7\baselineskip}
\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-0.5cm}
\centering
\includegraphics[width=0.4\textwidth]{Images/DuelingDQN.png}
\vspace{-0.5cm}
\end{wrapfigure}

Формализуя идею, мы хотели бы в модели учить не $Q^*(s, a)$ напрямую, а получать их с учётом $V^*(s)$. Иными словами, модель должна знать ценность самих состояний и с её учётом выдавать ценности действий. При этом при получении, скажем, негативной информации о ценности одного из действий, должна понижаться в том числе и оценка V-функции. \emph{Дуэльная} (dueling) архитектура --- это модификация вычислительного графа нашей модели с параметрами $\theta$, в которой на выходе предлагается иметь две головы, V-функцию $V(s) \HM \approx V^*(s)$ и Advantage-функцию $A(s, a) \HM \approx A^*(s, a)$: 
\begin{equation}\label{naivedueling}
Q_{\theta}(s, a) \coloneqq V_{\theta}(s) + A_{\theta}(s, a)
\end{equation}

Это интересная идея решить проблему просто сменой архитектуры вычислительного графа: при апдейте значения для одной пары $s, a$ неизбежно поменяется значение $V_{\theta}(s)$ ценности всего состояния, которое общее для всех действий. Мы таким образом вводим следующий <<прайор>>: ценности действий в одном состоянии всё-таки связаны между собой, и если одно действие в состоянии плохое, то вероятно, что и остальные тоже плохие (<<само состояние плохое>>).  

Здесь есть подвох: если $V^*(s)$ --- это произвольный скаляр, то $A^*(s, a)$ --- не произвольный. Действительно: мы моделируем $|\A|$ чисел, выдавая почему-то $|\A| \HM+ 1$ число: то есть почему-то вводим <<лишнюю степень свободы>>. Вообще-то, Advantage-функция не является произвольной функцией и обязана подчиняться \eqref{pr:advantageiszero}. Для оптимальных стратегий в предположении жадности нашей стратегии это утверждение вырождается в следующее свойство:

\begin{proposition}
$$\forall s \colon \max_a A^*(s, a) = 0$$
\beginproof
\begin{align*}
    \max_a A^*(s, a) = \max_a Q^*(s, a) - V^*(s) = \{ \text{связь V*Q* \eqref{V*Q*}} \} = 0 \tagqed
\end{align*}
\end{proposition}

Это условие можно легко учесть, вычтя максимум в формуле \eqref{naivedueling}:
\begin{equation}\label{dueling}
Q_{\theta}(s, a) \coloneqq V_{\theta}(s) + A_{\theta}(s, a) - \max_{\hat{a}} A_{\theta}(s, \hat{a})    
\end{equation}
Таким образом мы гарантируем, что максимум по действиям последних двух слагаемых равен нулю, и они корректно моделируют Advantage-функцию.

Заметим, что $V$ и $A$ не учатся по отдельности (для $V^*$ уравнение оптимальности Беллмана не сводится к регрессии, для $A^*$ уравнения Беллмана не существует вообще); вместо этого минимизируется лосс для Q-функции точно так же, как и в обычном DQN.

\begin{remark}
В нейросетях формулу \eqref{dueling} реализовать очень просто при помощи двух голов: одна выдаёт скаляр, другая $|\A|$ чисел, из которых вычитается максимум. Дальше к каждой компоненте второй головы добавляется скаляр, выданный первой головой, и результат считается выданным моделью $Q_{\theta}(s, a)$.
\end{remark}

Нюанс: авторы статьи эмпирически обнаружили, что замена максимума на среднее даёт чуть лучшие результаты. Вероятно, в реплей буфере очень часто встречаются пары $s, a$ такие, что $a$ --- наилучшее действие в этом состоянии по мнению текущей модели (а то есть $a \HM = \argmax\limits_{a} A_{\theta}(s, a)$), и поэтому градиент $A_{\theta}(s, a) \HM- \max\limits_{\hat{a}} A_{\theta}(s, \hat{a})$ часто зануляется. В результате, на текущий день под дуэльной архитектурой понимают альтернативную формулу:
\begin{equation}\label{kludgedueling}
Q_{\theta}(s, a) \coloneqq V_{\theta}(s) + A_{\theta}(s, a) - \frac{1}{|\A|}\sum_{\hat{a}} A_{\theta}(s, \hat{a})    
\end{equation}

\subsection{Шумные сети (Noisy Nets)}\label{subsec:noisynets}

По дефолту, алгоритмы на основе DQN решают дилемму исследования-использования при помощи примитивной $\eps$-жадной стратегии взаимодействия со средой. Этот бэйзлайн-подход плох примерно всем, в первую очередь тем, что крайне чувствителен к гиперпараметрам: для $\eps$ обязательно нужно составлять какое-нибудь расписание, чтобы в начале обучения он был побольше, а потом постепенно затухал, и откуда брать это расписание --- непонятно. При этом слишком большие значения шума существенно замедляют обучение, заставляя агента вести себя случайно, а раннее затухание приведёт к застреванию алгоритма в каком-нибудь локальном оптимуме (агент будет биться головой об стенку, не пробуя её обойти). 
\begin{remark}
Чтобы понять, что случилось именно это, можно посмотреть игры агента: если, например, Марио всё время доходит до середины первого уровня и прыгает в одну и ту же яму, то у него просто нет положительного опыта перепрыгивания этой ямы, и поэтому он не знает, что можно набрать больше награды.
\end{remark}

\needspace{7\baselineskip}
\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-0.3cm}
\centering
\includegraphics[width=0.4\textwidth]{Images/NoisyNets1.png}
\vspace{-0.3cm}
\end{wrapfigure}

Ключевая причина, почему $\eps$-жадная стратегия примитивна, заключается в независимости добавляемого шума от текущего состояния. Мы выдаём оценки Q-функции и в зависимости только от времени принимаем решение, использовать ли эти знания или эксплорить. Интуитивно, правильнее было бы принимать это решение в зависимости от текущего состояния: если состояние исследовано, чаще принимать решение в пользу использования знаний, если ново --- в пользу исследования. Открытие новой области пространства состояний скорее всего означает, что в ней стоит поделать разные действия, когда двигаться к ней нужно за счёт использования уже накопленных знаний.

\needspace{7\baselineskip}
\begin{wrapfigure}{l}{0.4\textwidth}
\vspace{-0.3cm}
\centering
\includegraphics[width=0.4\textwidth]{Images/NoisyNets2.png}
\vspace{-0.3cm}
\end{wrapfigure}

\emph{Шумные сети} (noisy nets) --- добавление шума с обучаемой и, главное, зависимой от состояния (входа в модель) дисперсией. Хак чисто инженерный: давайте каждый параметр в модели заменим на
$$\theta_i \coloneqq w_i + \sigma_i \eps_i, \qquad \eps_i \sim \N(0, 1),$$
или, другими словами, заменим веса сети на сэмплы из $\N(w_i, \sigma^2_i)$, где $w, \sigma \in \R^h$ --- параметры модели, обучаемые градиентным спуском. Очевидно, что выход сети становится случайной величиной, и, в зависимости от шума $\eps$, будет меняться выбор действия $a \HM= \argmax\limits_a Q_{\theta}(s, a, \eps)$. При этом влияние шума на принятое решение зависит от поданного в модель входного состояния.

\begin{remark}
Если состояния --- изображения, шум в свёрточные слои обычно не добавляется (зашумлять выделение объектов из изображения кажется бессмысленным).
\end{remark}

Формально, коли наша модель стала стохастичной, мы поменяли оптимизируемый функционал: мы хотим минимизировать функцию потерь в среднем по шуму:
$$\E_{\eps} \Loss(\theta, \eps) \to \min_\theta$$
Видно, что градиент такого функционала можно несмещённо оценивать по Монте-Карло:
$$\nabla_\theta \E_{\eps} \Loss(\theta, \eps) = \E_{\eps} \nabla_\theta \Loss(\theta, \varepsilon) \approx \nabla_\theta \Loss(\theta, \eps), \qquad \eps \sim \N(0, I)$$

Гарантий, что магнитуда шума в среднем будет падать для исследованных состояний, вообще говоря, нет. Надежда этой идеи в том, что магнитуда будет подстраиваться в зависимости от текущих в модель градиентов: если модель часто видит какое-то $s$ и функция потерь говорит, что на этом состоянии нужно выдавать, скажем, низкое значение, модель будет учиться при любых сэмплах $\eps$ выдавать указанное низкое значение. Для этого модели будет удобно уменьшать дисперсию впрыскиваемого шума и больше опираться на те нейронные связи, которые мало зашумлены. Если же в сеть поступают противоречивые сигналы о паре $s, a$, или это какое-то новое $s$, которого модель ещё не видела, выходное значение модели будет, интуитивно, сильно зашумлено, и часто аргмаксимум будет достигаться именно на нём. 

Ещё одно ключевое преимущество идеи в том, что в этом подходе отсутствуют гиперпараметры.

\begin{remark}
Кроме инициализации. Неудачная инициализация $\sigma$ всё равно может замедлить процесс обучения; обычно дисперсию шума инициализируют какой-нибудь константой, и эта константа становится в некотором смысле важным гиперпараметром алгоритма.
\end{remark}

Заметим, что таргет $y(\T)$, который мы генерируем для каждого перехода $\T$ из батча, формально теперь тоже должен вычисляться как мат.ожидание по шуму:
$$y(\T) \coloneqq \E_\eps \left[ r + \gamma \max_{a'} Q_{\theta}(s', a', \eps) \right]$$
Опять же, мат.ожидание несмещённо оценивается по Монте-Карло, однако с целью декорреляции полезно использовать в качестве $\eps$ другие сэмплы, нежели используемые при вычислении лосса. Считается, что подобное <<зашумление>> целевой переменной в DQN может даже пойти на пользу.

\begin{remark}
Генерация сэмплов шума по числу параметров нейросети на видеокарте может сильно замедлить время прохода через сеть. Для оптимизации авторы предлагают для матриц полносвязных слоёв генерировать шум следующим образом. Пусть $n$ --- число входов, $m$ --- число выходов в слое. Сэмплируются $\eps_1 \sim \N(0, I_{m \times m}), \eps_2 \sim \N(0, I_{n \times n})$, после чего полагается шум для матрицы равным
$$\eps \coloneqq f(\eps_1) f(\eps_2)^T$$
где $f$ --- масштабирующая функция, например $f(x) = \operatorname{sign}(x)\sqrt{|x|}$ (чтобы каждый сэмпл в среднем всё ещё имел дисперсию 1). Процедура требует всего $m + n$ сэмплов вместо $mn$, но жертвует независимостью сэмплов внутри слоя. К сожалению, даже при таком хаке время работы алгоритма заметно увеличивается, поскольку проходов через нейросеть в DQN нужно делать очень много.
\end{remark}

\begin{remark}
Альтернативно, можно зашумлять выходы слоёв (тогда сэмплов понадобится на порядок меньше) или просто добавлять шум на вход. В обоих случаях, <<зашумлённость>> выхода будет обучаемой, а степень влияния шума на выход сети будет зависеть от состояния.
\end{remark}

\subsection{Приоритизированный реплей (Prioritized DQN)}\label{subsec:prioritizedreplay}

%TODO: взгляд через importance sampling приоритета на градиент Q-шки

\needspace{7\baselineskip}
\begin{wrapfigure}{r}{0.3\textwidth}
\vspace{-0.3cm}
\centering
\includegraphics[width=0.3\textwidth]{Images/PER.png}
\vspace{-0.3cm}
\end{wrapfigure}

Off-policy алгоритмы позволяют хранить и переиспользовать весь накопленный опыт. Однако, интуитивно ясно, что встречавшиеся переходы существенно различаются по важности. Зачастую большая часть буфера, особенно поначалу обучения, состоит из записей изучения агентом ближайшей стенки, а переходы, включавшие, например, получение ещё не выученной внутри аппроксимации Q-функции награды, встречаются в буфере сильно реже и при равномерном сэмплировании редко оказываются в мини-батчах.

Важно, что при обучении оценочных функций информация о награде распространяется от последних состояний к первым. Например, на первых итерациях довольно бессмысленно обновлять те состояний, где сигнала награды не было ($r(s, a) \HM= 0$), а Q-функция для следующего состояния примерно случайна (а именно такие переходы чаще всего и попадаются алгоритму). Такие обновления лишь схлопывают выход аппроксимации к константе (которая ещё и имеет тенденцию к росту из-за оператора максимума). Ценной информацией поначалу являются терминальные состояния, где целевая переменная по определению равна $y(\T ) = r(s, a)$ и является абсолютно точным значением $Q^*(s, a)$. Типично, что на таких переходах значения временной разницы (лосса DQN) довольно высоко. Аналогичная ситуация в принципе справедлива для любых наград, которые для агента новы и ещё не распространились в аппроксимацию через уравнение Беллмана.

Очень хочется сэмплировать переходы из буфера не равномерно, а приоритизировано. Приоритет установим, например, следующим образом:
\begin{equation}\label{priority}
    \rho(\T ) \coloneqq \left( y(\T ) - Q_{\theta}(s, a) \right)^2 = \Loss(y(\T ), Q_{\theta}(s, a))
\end{equation}
Сэмплирование переходов из буфера происходит по следующему правилу:
$$\Prob ( \T ) \propto \rho( \T )^\alpha$$
где гиперпараметр $\alpha > 0$ контролирует масштаб приоритетов (в частности, $\alpha = 0$ соответствует равномерному сэмплированию, когда $\alpha \to +\infty$ соответствовало бы жадному сэмплированию самых <<важных>> переходов).

\begin{remark}
Добиться эффективного сэмплирования с приоритетами можно благодаря структуре данных SumTree: бинарному дереву, у которого в каждом узле хранится сумма значений в двух детях. Сам массив приоритетов для буфера хранится на нижнем уровне дерева; в корне, соответственно, лежит сумма всех приоритетов, нормировочная константа. Для сэмплирования достаточно взять случайную равномерную величину и спуститься по дереву. Таким образом, процедура сэмплирования имеет сложность $O(\log M)$, где $M$ --- размер буфера. За ту же сложность проходит обновление приоритета одного элемента, для чего достаточно обновить значения во всех его предках для поддержания структуры.
\end{remark}

Техническая проблема идеи \eqref{priority} заключается в том, что после каждого обновления весов сети $\theta$ приоритеты переходов меняются для всего буфера (состоящего обычно из миллионов переходов). Пересчитывать все приоритеты, конечно же, непрактично, и необходимо ввести некоторые упрощения. Например, можно обновлять приоритеты только у переходов из текущего батча, для которых значение лосса так и так считается. Это, вообще говоря, означает, что если у перехода был низкий приоритет, и до него дошла, условно, <<волна распространения>> награды, алгоритм не узнает об этом, пока не засэмплирует переход с тем приоритетом, который у него был.

\begin{remark}
Новые переходы добавляются в буфер с наивысшим приоритетом $\max\limits_{\T } \rho(\T )$, который можно поддерживать за константу, или вычислять текущий приоритет, дополнительно рассчитывая \eqref{priority} для онлайн-сэмплов.
\end{remark}

В чём у такого подхода есть фундаментальная проблема? После неконтролируемой замены равномерного сэмплирования на какое-то другое, могло случиться так, что для наших переходов $s' \not\sim p(s' \mid s, a)$. Почему это так, проще понять на примере.

\begin{example}
Пусть для данной пары $s, a$ с вероятностью 0.9 мы попадаем в $s' \HM= A$, а с вероятностью 0.1 мы попадаем в $s' \HM= B$. В условно бесконечном буфере для этой пары $s, a$ среди каждых 10 сэмплов будет 1 сэмпл с $s' \HM= B$ и 9 сэмплов с $s' \HM= A$, и равномерное сэмплирование давало бы переходы, удовлетворяющие $s' \sim p(s' \mid s, a)$. 

\needspace{7\baselineskip}
\begin{wrapfigure}{r}{0.15\textwidth}
\vspace{-0.3cm}
\centering
\includegraphics[width=0.15\textwidth]{Images/PrioritizedIssue.png}
%\vspace{-0.3cm}
\end{wrapfigure}

Для приоритизированного реплея, веса у переходов с $s' \HM= A$ могут отличаться от весов для переходов с $s' \HM= B$. Например, если мы оцениваем $V(A) \HM= 0, V(B) \HM= 1$, и уже даже правильно выучили среднее значение $Q(s, a) \HM= 0.1$, то $\Loss(s, a, s'=A)$ будет равен $0.1^2$, а для $\Loss(s, a, s'=B) = 0.9^2$. Значит, $s' = B$ будет появляться в засэмплированных переходах чаще, чем с вероятностью 0.1, и это выбьет $Q(s, a)$ с её правильного значения.
\end{example}

Иными словами, приоритизированное сэмплирование приводит к \emph{смещению} (bias). Этот эффект не так страшен поначалу обучения, когда распределение, из которого приходят состояния, всё равно скорее всего не сильно разнообразно. Более существенно нивелировать этот эффект по ходу обучения, в противном случае процесс обучения может полностью дестабилизироваться или где-нибудь застрять.

Заметим, что равномерное сэмплирование не является единственным <<корректным>> способом, но основным доступным. Мы не очень хотим <<возвращаться>> к нему постепенно с ходом обучения, но можем сделать похожую вещь: раз мы хотим подменить распределение, то можем при помощи importance sampling сохранить тот же оптимизируемый функционал:

\begin{theorem}
При сэмплировании с приоритетами $\Prob( \T )$ использование весов $w(\T) \coloneqq \frac{1}{\Prob (\T )}$ позволит избежать эффекта смещения.
\begin{proof} Пусть $M$ --- размер буфера.
\begin{align*}
\mathbb{E}_{\T \sim \mathop{Uniform}} \Loss(\T ) &= \sum_{i=1}^M \frac{1}{M} \Loss(\T_i) = \\ 
&= \sum_{i=1}^M \Prob (\T_i) \frac{1}{M\Prob (\T_i)} \Loss(\T_i) = \\
&= \mathbb{E}_{\T \sim \Prob (\T)} \frac{1}{M\Prob (\T )} \Loss(\T),
\end{align*}
что с точностью до константы $\frac{1}{M}$ и есть перевзвешивание функции потерь.
\end{proof}
\end{theorem}

Importance sampling подразумевает, что мы берём <<интересные>> переходы, но делаем по ним меньшие шаги (вес меньше именно для <<приоритетных>> переходов). Цена за такую корректировку, конечно, в том, что полезность приоритизированного сэмплирования понижается. Раз поначалу смещение нас не так беспокоит, предлагается вводить веса постепенно: а именно, использовать веса
$$w(\T) \coloneqq \frac{1}{\Prob (\T )^{\beta(t)}}$$
где $\beta(t)$ --- гиперпараметр, зависящий от итерации алгоритма $t$. Изначально $\beta(t=0) = 0$, что делает веса равномерными (корректировки не производится), но постепенно $\beta(t)$ растёт к 1 и полностью избавляет алгоритм от эффекта смещения.

\begin{remark}
На практике веса, посчитанные по такой формуле, могут оказаться очень маленькими или большими, и их следует нормировать. Вариации, как нормировать, различаются в реализациях: можно делить веса на $\max\limits_\T w(\T)$, где максимум берётся, например, только по текущему мини-батчу, чтобы гарантировать максимальный вес 1.
\end{remark}

\subsection{Multi-step DQN}\label{subsec:multistepdqn}

Уже упоминалось, что DQN из-за одношаговых целевых переменных страдает от проблемы отложенного сигнала и сопряжённой с ней в контексте нейросетевой аппроксимации проблемы накапливающейся ошибки. Эта проблема фундаментальна для off-policy подхода: в разделе \ref{sec:biasvar} про bias-variance trade-off упоминалось, что разрешать дилемму смещения-разброса (то есть <<проводить умный credit assingment>>) мы можем только в on-policy режиме. 

\emph{Многошаговый} (multi-step) DQN --- теоретически некорректная эвристика для занижения этого эффекта. Грубо говоря, нам очень хочется распространять за одну итерацию награду сразу на несколько шагов вперёд, то есть решать многошаговые уравнения Беллмана \eqref{NstepBellman}. Мы как бы и можем уравнение оптимальности многошаговое выписать...

\begin{proposition}[$N$-шаговое уравнение оптимальности Беллмана]
\begin{equation}\label{NstepOptimBellman}
 Q^*(s_0, a_0) = \E_{\Traj_{:N} \sim \pi^* \mid s_0, a_0} \left[ \sum_{t=0}^{N-1} \gamma^{t}r_t + \gamma^N \E_{s_N} \max_{a^*_N} Q^*(s_N, a^*_N) \right]   
\end{equation}
\end{proposition}

Что мы можем сделать? Мы можем прикинуться, будто решаем многошаговые уравнения Беллмана, задав целевую переменную следующим образом:
\begin{equation}\label{Nsteptarget}
y(s_0, a_0) \coloneqq \sum_{t=0}^{N-1} \gamma^{t}r_t + \gamma^N \max_{a_N} Q_{\theta}(s_N, a_N)
\end{equation}
где $s_1, a_1 \dots a_{N-1}, s_N$ взяты из буфера. Для этого в буфере вместо одношаговых переходов $\T \HM\coloneqq (s, a, r, s', \done)$ достаточно просто хранить другую пятёрку:
$$\T \coloneqq \left( s, \, a, \, \sum_{n=0}^{N-1} \gamma^{n}r^{(n)}, \, s^{(N)}, \, \done \right)$$
где $r^{(n)}$ --- награда, полученная через $n$ шагов после посещения рассматриваемого состояния $s$, $s^{(N)}$ --- состояние, посещённое через $N$ шагов, и, что важно, флаг $\done$ указывает на то, завершился ли эпизод в течение этого $N$-шагового роллаута\footnote{естественно, алгоритм должен рассматривать все $N$-шаговые роллауты, включая те, которые привели к завершению эпизода за $k \HM< N$ шагов. Для них, естественно, $r^{(k')} \HM= 0$ для $k' \HM> k$, и $Q^*(s^{(N)}, a_N) \equiv 0$ для всех $a_N$.}. Все остальные элементы алгоритма не изменяются, в частности, можно видеть, что случай $N \HM= 1$ соответствует обычному DQN.

Видно, что теперь награда, полученная за один шаг, распространяется на $N$ состояний в прошлое, и мы таким образом не только ускоряем обучение оценочной функции стартовых состояний, но и нивелируем проблему накапливающейся ошибки. 

Почему теоретически это некорректно? Беря $s_1, a_1 \dots a_{N-1}, s_N$ из буфера, мы получаем состояния из функции переходов, которая стационарна и соответствует тому мат.ожиданию, которое стоит в уравнении \eqref{NstepOptimBellman}. Но вот действия в этом мат.ожидании должны приходить из оптимальной стратегии! А в буфере $a_1, a_2 \dots a_{N-1}$ --- действия нашей стратегии произвольной давности (то есть сколь угодно неоптимальные). Вместо того, чтобы оценивать оптимальное поведение за хвост траектории (по определению $Q^*$), мы $N \HM- 1$ шагов ведём себя сколько угодно неоптимально, а затем в $s^{(N)}$ подставляем оценку оптимального поведения за хвост. Иными словами, мы недооцениваем истинное значение правой части $N$-шагового уравнения Беллмана при $N \HM> 1$. Вместо уравнения оптимальности мы решаем такое уравнение: что, если я следующие $N$ шагов веду себя как стратегия $\mu$, когда-то породившая данный роллаут, и только потом соберусь вести себя оптимально? Причём из-за нашего желания делать так в off-policy режиме $\mu$ для каждого перехода своё, то есть схеме Generalized Policy Iteration (алг. \ref{generalizedpolicyiteration}) это не соответствует: в ней мы всегда должны оценивать именно текущую стратегию $\pi$, а текущей стратегией в DQN является $\pi(s) \HM= \argmax\limits_{a} Q_{\theta}(s, a)$.

\begin{exampleBox}[righthand ratio=0.2, sidebyside, sidebyside align=center, lower separated=false]{}
Пример, когда многошаговая оценка приводит к некорректным апдейтам. На втором шаге игры в $s_1$ агент может скушать тортик или прыгнуть в лаву, и в первом эпизоде обучения агент совершил ошибку и получил огромную негативную награду. В буфер при $N \HM> 1$ запишется пример со стартовым состоянием $s_0$ и этой большой отрицательной наградой (в качестве $s^{(N)}$ будет записана лава). Пока этот пример живёт в реплей буфере, каждый раз, когда он сэмплируется в мини-батче, оценочная функция для $s_0$ обновляется этой отрицательной наградой, даже если агент уже научился больше не совершать эту ошибку и в $s_1$ наслаждается тортиками.

\tcblower
\vspace{-0.3cm}
\begin{adjustwidth}{-0.6cm}{}
\includegraphics[width=1.15\textwidth]{Images/MultiStepBad.png} \hspace*{-1cm}
\end{adjustwidth}
\end{exampleBox}

\begin{remark}
Эмпирически большое значение $N$ действительно может полностью дестабилизировать процесс, как и подсказывает теория, поэтому рекомендуется выставлять небольшие значение 2-3, от силы 5. 
\end{remark}

Большие значения могут быть работоспособны в средах, где сколь угодно неоптимальное поведение в течение $N$ шагов не приводит к существенному изменению награды по сравнению с оптимальным поведением, то есть в средах, где нет моментов с <<критическими решениями>> (когда $\max\limits_a Q^*(s, a) \HM- \min\limits_a Q^*(s, a)$ мало, то есть неоптимальное поведение в течение одного шага не приводит к сильно меньшей награде, чем оптимальное).

\begin{example}
Пример среды без <<критических решений>>: вы робот, который хочет добраться до соседней комнаты. Действия вверх-вниз-вправо-влево чуть-чуть сдвигают робота в пространстве. Тогда <<вести себя как угодно>> в течение $N \HM- 1$ шагов и потом отправиться кратчайшим маршрутом до соседней комнаты приносит практически столько же награды, сколько и сразу отправиться кратчайшим маршрутом до соседней комнаты. Поэтому в таких ситуациях использование относительно большого $N$ (5-10) может помочь, хоть алгоритм и может полностью дестабилизироваться (процедура некорректна).
\end{example}

\subsection{Retrace}

Как мы обсуждали в разделе \ref{subsec:retrace}, теоретически корректным способом обучаться в off-policy с многошаговых оценок является использование Retrace оценки. Конечно, она может на практике схлопываться в одношаговые обновления, но по крайней мере гарантирует, что алгоритм не ломается; и важно, что если записанные в засэмплированном из буфера роллауте действия достаточно вероятны для оцениваемой политики, то оценка получается достаточно длинной.

Конечно, сложно говорить про <<достаточно вероятны>>, когда оцениваемая политика детерминирована. Поэтому в практическом алгоритме Retrace предлагается перейти от моделирования Q-learning к моделированию SARSA (см. раздел \ref{subsec:sarsa}): то есть, считать целевой политикой $\pi(a \HM \mid s)$ $\eps$-жадную стратегию по отношению к текущей модели Q-функции. Преимущество в том, что это делает стратегию стохастичной, и любые действия в буфере не приведут к занулению следа и полному схлопыванию в одношаговую оценку.

В буфере также нужно сохранять вероятности выбора сохранённых действий $\mu(a \HM\mid s)$ в момент сбора данных (для $\eps$-жадных стратегий эти значения всё время будут или $\frac{\eps}{|\A|}$, или $1 \HM- \eps \HM+ \frac{\eps}{|\A|}$, где $\eps$ --- параметр эксплорейшна на момент сбора перехода). Вместо отдельных переходов теперь хранятся роллауты --- фрагменты траекторий некоторой длины $N$.

\begin{remark}
Если в DQN обучение проводилось на мини-батчах из, скажем, 64 переходов, то в Retrace (при том же масштабе задачи) нужно засэмплировать для одного мини-батча 4 роллаута длины $N\HM=16$. Такой выбор позволит надеяться на получение оценок длины вплоть до 16-шаговой (что в Retrace будет достигнуто, если политика сбора данных совпадёт с оцениваемой политикой на оцениваемом роллауте). Важно помнить про проблему декорреляции: нужно, чтобы в мини-батче должны оказаться разнообразные примеры, поэтому нельзя, например, взять только один роллаут длины 64.  
\end{remark}

Далее для каждого засэмплированного из буфера роллаута $s_0, a_0, r_0, s_1, a_1, r_1, \dots s_N$ мы сначала для каждой пары $s_t, a_t$ считаем, используя формулы из теории Retrace, следующие вспомогательные величины: значение коэффициента затухания следа $c_t$ по формуле \eqref{retracecoeff} (коэффициент $\lambda$ обычно полагают равным единице) и значение одношаговой ошибки $\Psi_{(1)}(s_t, a_t)$ по формуле \eqref{onestepoffpolicyQdelta}:
$$c_t \coloneqq \min \left( 1, \frac{\pi(a_t \mid s_t) }{\mu(a_t \mid s_t)} \right) $$
$$\Psi_{(1)}(s_t, a_t) = r_t + \gamma \E_{\hat{a}_{t+1} \sim \pi} Q_{\theta^{-}}(s_{t+1}, \hat{a}_{t+1}) - Q_{\theta^{-}}(s_t, a_t)$$

Всюду, где используется $\pi$, используется $\eps$-жадная стратегии по отношению к таргет-сети (хотя, если использовать идею Double DQN из раздела \ref{subsec:doubledqn}, то как раз во всех местах, где используется $\pi$ --- оцениваемая стратегия --- имеет смысл использовать свежую версию Q-функции). В частности, мат.ожидание по $\pi$ можно посчитать явно:
$$\E_{\hat{a} \sim \pi} Q_{\theta^{-}}(s, \hat{a}) = (1 - \eps) \max_{\hat{a}} Q_{\theta^{-}}(s, \hat{a}) + \frac{\eps}{|\A|} \sum_{\hat{a}} Q_{\theta^{-}}(s, \hat{a}) $$

После этого в Retrace для одной пары $s_t, a_t$ все будущие одношаговые ошибки нужно просуммировать (воспользуемся индексом $\hat{t}$ для обозначения этого перебора), но заглядывание на каждый следующий $i$-ый шаг в будущее обязывает нас потушить след в $c_i$ раз: 
$$\Psi^{\mathrm{retrace}}(s_t, a_t) \coloneqq \sum_{\hat{t} \ge t}^N \gamma^{\hat{t} - t}  \left( \prod_{i = t+1}^{i = \hat{t}} c_{i} \right) \Psi_{(1)}(s_{\hat{t}}, a_{\hat{t}})$$

Заметим, что в этой формуле внешняя сумма по $\hat{t}$ идёт не до бесконечности, как в теории Retrace, а до $N$, до конца роллаута. После этого считаем, что след зануляется: это корректно, хотя иногда можно и потерять возможность получить более длинную оценку.

Далее в табличном методе мы бы провели обновление по формуле
$$Q(s, a) \leftarrow Q(s, a) + \alpha \Psi^{\mathrm{retrace}}(s, a),$$
то есть воспользовались бы $\Psi^{\mathrm{retrace}}(s, a)$ как градиентом. Другими словами, оценка указывает, нужно ли увеличивать выход модели для рассматриваемой пары $s, a$ или уменьшать. Чтобы получить задачу регрессии, целевая переменная строится по формуле
$$y(s_t, a_t) \coloneqq \Psi^{\mathrm{retrace}}(s_t, a_t) + Q_{\theta}(s_t, a_t)$$
и дальше оптимизируется MSE, игнорируя зависимость $y$ от $\theta$:
$$\left( y(s_t, a_t) - Q_{\theta}(s_t, a_t) \right)^2 \to \min_{\theta}$$

Тогда градиент функции потерь по $\theta$ для одного примера равен:
$$\nabla_\theta \frac{1}{2}\left( y(s, a) - Q_{\theta}(s, a) \right)^2 = \left( y(s, a) - Q_{\theta}(s, a) \right) \nabla_\theta Q_{\theta}(s, a) = \Psi^{\mathrm{retrace}}(s, a) \nabla_\theta Q_{\theta}(s, a)$$

Это полностью аналогично градиенту обычного DQN \eqref{DQNgradient}, только там оценка $\Psi(s, a) \HM= r + \gamma \max\limits_{a'} Q_{\theta^{-}}(s, a) - Q_{\theta}(s, a)$ была одношаговой, а здесь мы заглядываем настолько максимально далеко вперёд, насколько возможно (в силу использования $\lambda \HM= 1$). 

\begin{remark}
В большинстве последних алгоритмов на основе DQN, таких как Agent57, используются формулы Retrace. Они позволяют максимально возможным образом побороться с ключевыми фундаментальными проблемами off-policy подхода, вытекающими из одношаговых целевых переменных, когда из недостатков можно выделить, пожалуй, лишь громоздкость формул.
\end{remark}
