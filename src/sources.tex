\newcommand*{\sectionsources}[1]{\textbf{\textcolor{ChadBlue}{\underline{#1}}}}   % а зачем это?

\newpage
\begin{center}
\huge \textbf{\textcolor{ChadBlue}{\underline{Материалы}}}

\normalsize
\vspace{0.3cm}
Большая часть материалов взята из основных курсов по обучению с подкреплением:

\vspace{0.3cm}
\href{https://www.youtube.com/playlist?list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc}{Курс Сергея Левина}; %CS 285 Deep Reinforcement Learning, UC Berkeley

\vspace{0.3cm}
\href{https://github.com/yandexdataschool/Practical_RL}{Курс Practical RL};

\vspace{0.3cm}
\href{https://deeppavlov.ai/rl_course_2020}{Цикл докладов Advanced RL};

\vspace{0.3cm}
\href{https://www.davidsilver.uk/teaching/}{Курс Дэвида Сильвера};

\vspace{0.3cm}
\href{https://www.youtube.com/watch?v=ISk80iLhdfU&list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb&index=1}{Курс DeepMind};

\vspace{0.3cm}
\href{https://www.udacity.com/course/reinforcement-learning--ud600}{Курс GeorgiaTech};
\end{center}

В \underline{\textbf{главе 2}} большинство материала взято из \href{https://cs.gmu.edu/~sean/book/metaheuristics/Essentials.pdf}{книги} \cite{luke2013essentials}. Хороший обзор эволюционных стратегий можно найти в \href{https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html}{блоге Lil'log} \cite{weng2019ES}. Алгоритм NEAT предложен в \cite{stanley2002evolving}. Алгоритм WANN развил его идею в \cite{gaier2019weight}. Кросс-энтропийный метод как метод оптимизации и метод вычисления вероятности маловероятных событий предложен в \cite{botev2013cross}; его применение к задаче RL обычно связывают с \cite{szita2006learning}, где его применили к тетрису. OpenAI-ES описана в \cite{salimans2017evolution}; упомянутый алгоритм ARS, действующий примерно также, предложен в \cite{mania2018simple}. Идея адаптировать ковариационную матрицу в эволюционных стратегиях восходит корнями к \cite{hansen1996adapting}; полный технический обзор всего набора эвристик, использующихся как алгоритм CMA-ES, можно прочитать \href{https://arxiv.org/pdf/1604.00772.pdf}{здесь} \cite{hansen2016cma}. Доказательство того, что адаптация матрицы ковариации по сути является натуральным градиентным спуском, было независимо получено в \cite{akimoto2010bidirectional} и \cite{glasmachers2010exponential}.

Больше информации по \underline{\textbf{главе 3}} и более подробную библиографию можно получить \href{https://drive.google.com/file/d/1Z4W_-0IaMNpZnhnMkqcDVM_EA79GFJo-/view}{в классической книге Саттона-Барто} \cite{sutton2018reinforcement}; отмечу только некоторые дополнительные ссылки. Лемма RPI была представлена в \cite{kakade2002approximately}. Алгоритм Q-learning, изначально придуманный в \cite{watkins1989learning}, был придуман как эвристика, но позже авторам удалось доказать сходимость в \cite{watkins1992q}; это доказательство через ARP и приведено в приложении. Связь с теорией стохастической аппроксимации, начатой ещё в далёком 1951-ом году статьёй \cite{robbins1951stochastic}, была обнаружена после этого в \cite{tsitsiklis1994asynchronous}, что позволило доказать более сильные утверждения вроде сходимости TD($\lambda$). Наиболее общую форму алгоритмов off-policy оценивания стратегии и оценку Retrace представили в \cite{munos2016safe} уже в 2016-ом году.

\underline{\textbf{Глава 4}} основана на алгоритме DQN \cite{mnih2013playing}, продемонстрировавшем потенциал совмещения глубокого обучения с классической теорией. Идея борьбы с переоценкой при помощи двух аппроксимаций Q-функций была предложена в \cite{hasselt2010double} для табличного алгоритма. Twin (<<Clipped Double>>) оценка предложена была позже (в рамках алгоритма TD3) в \cite{fujimoto2018addressing}. Double DQN предложен в \cite{van2016deep}; Dueling DQN в \cite{wang2015dueling}; Noisy Nets в \cite{fortunato2017noisy}. Приоритизированный реплей был использован в DQN в \cite{schaul2015prioritized}; идею высчитывать приоритеты онлайн и добавлять в буфер уже <<с правильным>> приоритетом реализовали в алгоритме R2D2 \cite{horgan2018distributed}. Эвристика многошагового DQN была описана в составе Rainbow \cite{hessel2018rainbow}. Distributional подход и алгоритм c51 был инициирован в \cite{bellemare2017distributional}; идея перехода к квантильной аппроксимации и алгоритм QR-DQN был описан в \cite{dabney2018distributional}; алгоритм IQN предложен в \cite{dabney2018implicit}. Эквивалентность distributional-алгоритмов с обычным подходом в табличном сеттинге была показана в \cite{lyle2019comparative}.

В \underline{\textbf{главе 5}} метод пробрасывания градиентов через стохастические узлы вычислительного графа REINFORCE и его применение к задаче RL были придуманы в \cite{williams1992simple}. Actor-Critic методы, в которых учится как стратегия, так и оценочная функция, позволяющая обучаться с неполных эпизодов, предложены в \cite{sutton2000policy}. Применение Policy Gradient подхода с нейросетевой аппроксимацией и алгоритм A2C предложен в \cite{mnih2016asynchronous}. Список разных вариаций Policy Gradient алгоритмов можно найти в \href{https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html}{блоге Lil'log} \cite{weng2018PG}. TRPO был предложен в \cite{schulman2015trust}; обучение с длинных роллаутов привело к использованию GAE оценок, что было предложено в \cite{schulman2015high}. Алгоритм PPO описан в \cite{schulman2017proximal}, однако стандартные реализации, добившиеся высоких результатов на бенчмарках и способствовавшие распространению алгоритма, использовали дополнительные инженерные эвристики; на их существенное влияние на результаты обращено внимание в \cite{engstrom2019implementation}. Позже в \cite{achiam2017constrained} была представлена более точная нижняя оценка на погрешность суррогатной функции, давшая теоретическое обоснование использованию усреднённой по состояниям KL-дивергенции, которая и была представлена в тексте.

Применение идей policy gradient и value-based подхода для обучения нейросетей в задаче непрерывного управления, описанное в \underline{\textbf{главе 6}}, предложено в \cite{lillicrap2015continuous} в алгоритме DDPG. Более стабильная версия этого алгоритма TD3 описана в \cite{fujimoto2018addressing}. Алгоритм Soft Q-learning, использующий теорию Maximum Entropy RL для обучения нейросетей, описан в \cite{haarnoja2017reinforcement}; алгоритм Soft Actor-Critic, ставший практическим алгоритмом для работы с непрерывными действиями в рамках этого сеттинга, предложен в \cite{haarnoja2018soft}. 

За полным погружением в математику, стоящую за многорукими бандитами (\underline{\textbf{глава 7.1}}), можно обратиться к \href{https://tor-lattimore.com/downloads/book/book.pdf}{книге} \cite{lattimore2020bandit}. Нижняя оценка регрета (теорема Лаи-Роббинса) получена в \cite{lai1985asymptotically}. Асимптотическая оптимальность алгоритма UCB показана в \cite{auer2002finite}. Сама задача многоруких бандитов была впервые рассмотрена Томпсоном в \cite{thompson1933likelihood}; он же эвристически предложил сэмплирование Томпсона, которое позже тоже оказалось асимптотически оптимальным \cite{kaufmann2012thompson}. Пример \ref{ex:onlineshortestpath} c нахождением кратчайшего маршрута в графе взят из туториала по сэмплированию Томпсона \cite{russo2017tutorial}. Алгоритм, обучающий байесовские модели динамики и награды для табличных MDP и использующих сэмплирование Томпсона для разрешения дилеммы exploration-exploitation в MDP взят из \cite{osband2013more}.

Несмотря на то, что описанный в \underline{\textbf{главе 7}} model-based подход исследовался в RL всегда, применение моделей мира и концепция сновидений популяризовалась благодаря статье \cite{ha2018world}. Победа алгоритма AlphaGo в го на основе совмещения MCTS и нейросетей в итоге была обобщена сначала в алгоритм AlphaZero \cite{silver2018general}, а затем и на случай неизвестной динамики в алгоритм $\mu$-Zero \cite{schrittwieser2019mastering}. Теория линейно-квадратичных регуляторов восходит ещё к оптимальному управлению; по LQR обычно ссылаются на \cite{bemporad2002explicit}, а по расширению iLQR --- на \cite{li2004iterative}.

Фреймворк Maximum Entropy Inverse RL, рассмотренный в \underline{\textbf{главе 8.1}}, был предложен в \cite{ziebart2008maximum}. Обобщение алгоритма из этой статьи с табличного случая на произвольный в виде процедуры Guided Cost Learning описана в \cite{finn2016guided}. Связь задачи с минимизацией расстояния между occupancy measure была исследована в \cite{ho2016generative}, где был предложен алгоритм GAIL. Расширение GAIL на случай, когда в записях эксперта доступны только наблюдения (алгоритм GAIfO) представлен в \cite{torabi2018generative}. Пример \ref{ex:quadrocopter} со сведением к классификации задачи обучения квадрокоптера полётам по лесу описан в статье \cite{giusti2015machine}. 

Моделирование любопытства и скуки, описанное в \underline{\textbf{главе 8.2}} у агентов было описано ещё Шмидхубером в далёком 1991 году \cite{schmidhuber1991possibility} вместе с проблемой шумных телевизоров. Эвристика RND придумана в \cite{burda2018exploration}; фильтрующие свойства модели обратной динамики, понятие контролируемого состояния и алгоритм ICM описаны в \cite{pathak2017curiosity}. Пример минимизации хаоса \ref{ex:chaosminimization} основан на \cite{berseth2019smirl}.

В \underline{главе \textbf{8.3}} переразметка траекторий произвольными целями, также называемая алгоритмом Intentional-Unintentional, описана в \cite{cabi2017intentional}. Идея HER придумана в \cite{andrychowicz2017hindsight}. Обобщение идеи для переразметки произвольных траекторий и связь этой задачи с обратным обучением с подкреплением одновременно замечена в \cite{eysenbach2020rewriting} и \cite{li2020generalized}. Мета-контроллеры для автоматического подбора гиперпараметров использовались в алгоритме Agent57, обошедшем человека сразу во всех 57 играх Atari \cite{badia2020agent57}.

В иерархическом RL, описанном в \underline{главе \textbf{8.4}}, алгоритм Option-Critic и формулы градиентов для обучения политики терминальности представлены в \cite{bacon2017option}. Концепция феодализма и феодальные сети FuN предложены в \cite{vezhnevets2017feudal}. Обучение похожей иерархической схемы в off-policy при помощи переразметки в виде алгоритма HIRO описано в \cite{nachum2018data}.

Задача обучения в условиях частичной наблюдаемости \underline{главы \textbf{8.5}}  поставлена в \cite{smallwood1973optimal}. Обучение рекуррентных сетей в RL в рамках алгоритма DRQN описано в \cite{hausknecht2015deep}; эвристики разогрева и хранения скрытого состояния предложены в алгоритме R2D2 \cite{horgan2018distributed}. Эпизодичная память NEC предложена в \cite{pritzel2017neural}.

В \underline{главе \textbf{8.6}} Adversarial-атаки на алгоритмы, обученные в режиме self-play, продемонстрированы в \cite{gleave2019adversarial}. Алгоритм QMix для кооперативных игр и идея моделировать смешивающую сеть в классе монотонных функций предложена в \cite{rashid2018qmix}. Общий алгоритм MADDPG и идея моделирования других агентов предложены в \cite{lowe2017multi}. Идея моделировать и учить протоколы коммуникации между агентами описаны в \cite{foerster2016learning}.

\needspace{7\baselineskip}
\begin{wrapfigure}{r}{0.15\textwidth}
\vspace{-0.5cm}
\centering
\includegraphics[width=0.15\textwidth]{Images/Scrat.png}
\vspace{-0.8cm}
\end{wrapfigure}

%Изображения c роботом взяты из \href{https://inst.eecs.berkeley.edu/~cs188/fa20/}{курса CS-188 Introduction to Artificial Intelligence, Berkeley}. 
Изображения взяты из рассмотренных статей, книги \cite{sutton2018reinforcement} и из распространённых сред (OpenAI Gym \cite{brockman2016openai}, Mario \cite{gym-super-mario-bros}, Unity ML Agents \cite{juliani2018unity}). Кастомные изображения для примеров и схем были нарисованы в \href{https://www.draw.io/}{draw.io}; изображение белки на них взято из \href{https://twitter.com/racefornuts/status/690043558208913408}{вот этого твита}; судя по всему, это незаюзанный концепт-арт для некой игры <<Трагедия белок>>... а вот, пригодился!

\newpage
\huge \textbf{\textcolor{ChadBlue}{Литература}}
\normalsize
\bibliography{DRL}