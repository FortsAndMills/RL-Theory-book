\section{Модель взаимодействия агента со средой}

\subsection{Связь с оптимальным управлением}

Математика поначалу пришла к очень похожей формализации следующим образом. Для начала, нужно ввести какую-то модель мира; в рамках \href{https://ru.wikipedia.org/wiki/\%D0\%94\%D0\%B5\%D0\%BC\%D0\%BE\%D0\%BD_\%D0\%9B\%D0\%B0\%D0\%BF\%D0\%BB\%D0\%B0\%D1\%81\%D0\%B0}{лапласовского детерминизма} можно предположить, что положение, скорости и ускорения всех атомов вселенной задают её текущее состояние, а изменение состояния происходит согласно некоторым дифференциальным уравнениям:
$$\dot{s} = f(s, t)$$

Коли агент может как-то взаимодействовать со средой или влиять на неё, мы можем промоделировать взаимодействие следующим образом: скажем, что в каждый момент времени $t$ агент в зависимости от текущего состояния среды $s$ выбирает некоторое действие (<<управление>>) $a(s, t)$:
$$\dot{s} = f(s, a(s, t), t)$$

Для моделирования награды положим, что в каждый момент времени агент получает наказание или штраф (cost)\footnote{теория оптимального управления строилась в СССР в формализме минимизации штрафов (потерь, убытков и наказаний), когда построенная в США теория обучения с подкреплением --- в формализме максимизации награды (прибыли, счастья, тортиков). Само собой, формализмы эквивалентны, и любой штраф можно переделать в награду домножением на минус, и наоборот.} в объёме $L(s, a(s, t), t)$. Итоговой наградой агента полагаем суммарную награду, то есть интеграл по времени:
$$
\begin{cases}
-\int\limits L(s, a(s, t), t) \diff t \to \max\limits_{a(s, t)} \\
\dot{s} = f(s, a(s, t), t)
\end{cases}
$$

За исключением того, что для полной постановки требуется задать ещё начальные и конечные условия, поставленная задача является общей формой задачи \emph{оптимального управления} (optimal control). При этом обратим внимание на сделанные при постановке задачи предположения:
\begin{itemize}
    \item время непрерывно;
    \item мир детерминирован;
    \item мир нестационарен (функция $f$ напрямую зависит от времени $t$);
    \item модель мира предполагается известной, то есть функция $f$ задана явно в самой постановке задачи;
\end{itemize}

Принципиально последнее: теория рассматривает способы для заданной системы дифференциальных уравнений поиска оптимальных $a(s, t)$ аналитически. Проводя аналогию с задачей максимизации некоторой, например, дифференцируемой функции $f(x) \to \max\limits_x$, теория оптимального управления ищет необходимые условия решения: например, что в экстремуме функции обязательно $\nabla_x f(x) = 0$. Никакого <<обучения методом проб и ошибок>> здесь не предполагается.

В обучении с подкреплением вместо поиска решения аналитически мы будем строить итеративные методы оптимизации, искать аналоги, например, градиентного спуска. В такой процедуре неизбежно появится цепочка приближений решений: мы начнём с какой-то функции, выбирающей действия, возможно, не очень удачной и не способной набрать большую награду, но с ходом работы алгоритма награда будет оптимизироваться и способ выбора агентом действий будет улучшаться. Так будет выглядеть обучение.

Также RL исходит из других предположений: время дискретно, а среда --- стохастична, но стационарна. В частности, в рамках этой теории можно построить алгоритмы для ситуации, когда модель мира агенту неизвестна, и единственный способ поиска решений --- обучение на собственном опыте взаимодействия со средой.  

\subsection{Марковская цепь}

В обучении с подкреплением мы зададим модель мира следующим образом: будем считать, что существуют некоторые <<законы физики>>, возможно, стохастические, которые определяют следующее состояние среды по предыдущему. При этом предполагается, что в текущем состоянии мира содержится вся необходимая информация для выполнения перехода, или, иначе говоря, выполняется \emph{свойство Марковости} (Markov property): процесс зависит только от текущего состояния и не зависит от всей предыдущей истории.

\begin{definition} 
\emph{Марковской цепью (Markov chain)} называется пара $(\St, \Trans)$, где: 
\begin{itemize}
    \item $\St$ --- множество состояний.
    \item $\Trans$ --- вероятности переходов $\{p(s_{t+1} \HM\mid s_t) \HM\mid t \HM\in \{0, 1, \dots \}, s_t, s_{t+1} \HM\in \St\}$.
\end{itemize}
\end{definition}

Если дополнительно задать стартовое состояние $s_0$, можно рассмотреть процесс, заданный марковской цепью. В момент времени $t \HM= 0$ мир находится в состоянии $s_0$; сэмплируется случайная величина $s_1 \HM\sim p(s_1 \HM\mid s_0)$, и мир переходит в состояние $s_1$; сэмплируется $s_2 \HM\sim p(s_2 \HM\mid s_1)$, и так далее до бесконечности.

Мы далее делаем важное предположение, что законы мира не изменяются с течением времени. В аналогии с окружающей нас действительностью это можно интерпретировать примерно как <<физические константы не изменяются со временем>>.\footnote{вопрос на засыпку для любителей философии: а это вообще правда? Вдруг там через миллион лет гравитационная постоянная или скорость света уже будут другими в сорок втором знаке после запятой?..}

\begin{definition} 
Марковская цепь называется \emph{однородной} (time-homogeneous) или \emph{стационарной} (stationary), если вероятности переходов не зависят от времени:
\begin{equation*}
\forall t \colon p(s_{t+1} \mid s_t) = p(s_1 \mid s_0)
\end{equation*}
\end{definition}

По определению, переходы $\Trans$ стационарных марковских цепей задаются единственным условным распределением $p(s' \HM\mid s)$. Апостроф $'$ канонично используется для обозначения <<следующих>> моментов времени, мы будем активно пользоваться этим обозначением.

\begin{exampleBox}[righthand ratio=0.35, sidebyside, sidebyside align=center, lower separated=false]{Конечные марковские цепи}
Марковские цепи с конечным числом состояний можно задать при помощи ориентированного графа, дугам которого поставлены в соответствие вероятности переходов (отсутствие дуги означает нулевую вероятность перехода). 

На рисунке приведён пример стационарной марковской цепи с 4 состояниями. Такую цепь можно также задать в виде матрицы переходов:
\begin{center}
\begin{tabular}{c|cccc}
 & \colorsquare{black} & \colorsquare{ChadRed} & \colorsquare{ChadPurple} & \colorsquare{ChadBlue} \\
 \hline
\colorsquare{black}      &      & 0.5 & 0.5 &      \\ 
\colorsquare{ChadRed}    & 0.25 &     &     & 0.75 \\ 
\colorsquare{ChadPurple} & 0.1  & 0.2 & 0.1 & 0.6  \\ 
\colorsquare{ChadBlue}   &      & 0.5 &     & 0.5  \\
\end{tabular}
\end{center}

\tcblower
\vspace{-0.3cm}
\animategraphics[controls, width=\linewidth]{1}{Images/MC/MC-}{1}{15}
\end{exampleBox}

\subsection{Среда}

Как и в оптимальном управлении, для моделирования влияния агента на среду в вероятности переходов достаточно добавить зависимость от выбираемых агентом действий. Итак, наша модель среды --- это <<управляемая>> марковская цепь.

\begin{definition} 
\emph{Средой} (environment) называется тройка $(\St, \A, \Trans)$, где: 
\begin{itemize}
    \item $\St$ --- \emph{пространство состояний} (state space), некоторое множество.
    \item $\A$ --- \emph{пространство действий} (action space), некоторое множество.
    \item $\Trans$ --- \emph{функция переходов} (transition function) или \emph{динамика среды} (world dynamics): вероятности $p(s' \HM\mid s, a)$.
\end{itemize}
\end{definition}

В таком определении среды заложена марковость (независимость переходов от истории) и стационарность (независимость от времени). Время при этом дискретно, в частности, нет понятия <<времени принятия решения агентом>>: среда, находясь в состоянии $s$, ожидает от агента действие $a \HM\in \A$, после чего совершает шаг, сэмплируя следующее состояние $s' \HM\sim p(s' \HM\mid s, a)$.

По дефолту, среда также предполагается \emph{полностью наблюдаемой} (fully observable): агенту при выборе $a_t$ доступно всё текущее состояние $s_t$ в качестве входа. Иными словами, в рамках данного предположения понятия состояния и наблюдения для нас будут эквивалентны. Мы будем строить теорию в рамках этого упрощения; если среда не является полностью наблюдаемой, задача существенно усложняется, и необходимо переходить к формализму \emph{частично наблюдаемых MDP} (partially observable MDP, PoMDP). Обобщение алгоритмов для PoMDP будет рассмотрено отдельно в разделе \ref{sec:PoMDP}.

\begin{exampleBox}[righthand ratio=0.5, sidebyside, sidebyside align=center, lower separated=false]{Конечные среды}
Среды с конечным числом состояний и конечным числом действий можно задать при помощи ориентированного графа, где для каждого действия задан свой комплект дуг.

На рисунке приведён пример среды с 2 действиями $\A = \bigl\{ \text{\colorsquare{ChadBlue}}, \text{\colorsquare{ChadRed}} \bigr\} $; дуги для разных действий различаются цветом. Возле каждого состояния выписана стратегия агента.

\tcblower
\animategraphics[controls, width=\linewidth]{1}{Images/ENV/Environment-}{1}{15}
\end{exampleBox}

\begin{exampleBox}[righthand ratio=0.15, sidebyside, sidebyside align=center, lower separated=false]{Кубик-Рубик}
Пространство состояний --- пространство конфигураций Кубика-Рубика. Пространство действий состоит из 12 элементов (нужно выбрать одну из 6 граней, каждую из которых можно повернуть двумя способами). Следующая конфигурация однозначно определяется текущей конфигурацией и действием, соответственно среда Кубика-Рубика \emph{детерминирована}: задаётся вырожденным распределением, или, что тоже самое,  обычной детерминированной функцией $s' = f(s, a)$. 

\tcblower
\includegraphics[width=\textwidth]{Images/rubik.png}
\end{exampleBox}

\subsection{Действия}

Нас будут интересовать два вида пространства действий $\A$:
\begin{itemize}
    \item[а)] \emph{конечное}, или \emph{дискретное пространство действий} (discrete action space): $|\A| \HM< +\infty$. Мы также будем предполагать, что число действий $|\A|$ достаточно мало.
    \item[б)] непрерывное пространство действий (continuous domain): $\A \HM\subseteq [-1, 1]^m$. Выбор именно отрезков $[-1, 1]$ является не ограничивающим общности распространённым соглашением. Задачи с таким пространством действий также называют задачами \emph{непрерывного управления} (continuous control).
\end{itemize}

\begin{remark}
Заметим, что множество действий не меняется со временем и не зависит от состояния. Если в практической задаче предполагается, что множество допустимых действий разное в различных состояний, в <<законы физики>> прописывается реакция на некорректное действие, например, случайный выбор корректного действия за агента.
\end{remark}

В общем случае процесс выбора агентом действия в текущем состоянии может быть стохастичен. Таким образом, объектом поиска будет являться распределение $\pi(a \mid s), a \HM\in \A, s \HM\in \St$. Заметим, что факт, что нам будет достаточно искать стратегию в классе стационарных (не зависящих от времени), вообще говоря, потребует обоснования.

\subsection{Траектории}

\begin{definition} 
Набор $\Traj \coloneqq \left( s_0, a_0, s_1, a_1, s_2, a_2, s_3, a_3 \dots \right) $ называется \emph{траекторией}.
\end{definition}

\begin{exampleBox}[label=ex:trajectory]{}
Пусть в среде состояния описываются одним вещественным числом, $\St \HM\equiv \R$, у агента есть два действия $\A \HM= \{+1, -1\}$, а следующее состояние определяется как $s' \HM= s + a + \eps$, где $\eps \sim \N(0, 1)$. Начальное состояние полагается равным нулю $s_0 \HM= 0$. Сгенерируем пример траектории для случайной стратегии (вероятность выбора каждого действия равна 0.5):
    \begin{center}
    \includegraphics[width=\textwidth]{Images/traj.png}
    \end{center}
\end{exampleBox}

Поскольку траектории --- это случайные величины, которые заданы по постановке задачи конкретным процессом порождения (действия генерируются из некоторой стратегии, состояния --- из функции переходов), можно расписать распределение на множестве траекторий:

\begin{definition} 
Для данной среды, политики $\pi$ и начального состояния $s_0 \HM\in \St$ распределение, из которого приходят траектории $\Traj$, называется \emph{trajectory distribution}:
$$p\left(\Traj \right) = p(a_0, s_1, a_1 \dots) = \prod_{t \ge 0} \pi(a_t \mid s_t) p(s_{t+1} \mid s_t, a_t)$$
\end{definition}

Мы часто будем рассматривать мат.ожидания по траекториям, которые будем обозначать $\E_{\Traj}$. Под этим подразумевается бесконечная цепочка вложенных мат.ожиданий:
\begin{equation}\label{traj_expectation}
\E_{\Traj} \left( \cdot \right) \coloneqq \E_{\pi(a_0 \mid s_0)}\E_{p(s_1 \mid s_0, a_0)}\E_{\pi(a_1 \mid s_1)} \dots \left( \cdot \right)
\end{equation}

Поскольку часто придётся раскладывать эту цепочку, договоримся о следующем сокращении:
$$\E_{\Traj} \left( \cdot \right) = \E_{a_0}\E_{s_1}\E_{a_1} \dots \left( \cdot \right)$$

Однако в такой записи стоит помнить, что действия приходят из некоторой зафиксированной политики $\pi$, которая неявно присутствует в выражении. Для напоминания об этом будет, где уместно, использоваться запись $\E_{\Traj \sim \pi}$.

\subsection{Марковский процесс принятия решений (MDP)}

Для того, чтобы сформулировать задачу, нам необходимо в среде задать агенту цель --- некоторый функционал для оптимизации. По сути, марковский процесс принятия решений --- это среда плюс награда. Мы будем пользоваться следующим определением:

\begin{definition} 
\emph{Марковский процесс принятия решений} (Markov Decision Process, MDP) --- это четвёрка $(\St, \A, \Trans, r)$, где: 
\begin{itemize}
    \item $\St, \A, \Trans$ --- среда.
    \item $r: \St \times \A \to \R$ --- \emph{функция награды} (reward function).
\end{itemize}
\end{definition}

\needspace{7\baselineskip}
\begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-0.4cm}
\centering
\includegraphics[width=0.5\textwidth]{Images/RL.png}
\vspace{-0.8cm}
\end{wrapfigure}

Сам процесс выглядит следующим образом. Для момента времени $t \HM= 0$ начальное состояние мира полагается $s_0$; будем считать, оно дано дополнительно и фиксировано\footnote{формально его можно рассматривать как часть заданного MDP.}. Агент наблюдает всё состояние целиком и выбирает действие $a_0 \HM\in \A$. Среда отвечает генерацией награды $r(s_0, a_0)$ и сэмплирует следующее состояние $s_1 \HM\sim p(s' \HM\mid s_0, a_0)$. Агент выбирает $a_1 \HM\in \A$, получает награду $r(s_1, a_1)$, состояние $s_2$, и так далее до бесконечности.

\begin{example}
Для среды из примера \ref{ex:trajectory} зададим функцию награды как $r(s, a) = -|s + 4.2|$. Независимость награды от времени является требованием стационарности к рассматриваемым MDP:
\begin{center}
\includegraphics[width=\textwidth]{Images/traj_rew.png}
\end{center}
\end{example}

Формальное введение MDP в разных источниках чуть отличается, и из-за различных договорённостей одни и те же утверждения могут выглядеть совсем непохожим образом в силу разных исходных обозначений. Важно, что суть и все основные теоретические результаты остаются неизменными.

\begin{theorem}[Эквивалентные определения MDP]
Эквивалентно рассматривать MDP, где
\begin{itemize}
    \item функция награды зависит только от текущего состояния;
    \item функция награды является стохастической;
    \item функция награды зависит от тройки ($s$, $a$, $s'$);
    \item переходы и генерация награды задаётся распределением $p(r, s' \HM\mid s, a)$;
    \item начальное состояние стохастично и генерируется из некоторого распределения $s_0 \HM\sim p(s_0)$.
\end{itemize}

\beginproof[Скетч доказательства]{}
Покажем, что всю стохастику можно <<засовывать>>  в $p(s' \HM\mid s, a)$. Например, стохастичность начального состояния можно <<убрать>>, создав отдельное начальное состояние, из которого на первом шаге агент вне зависимости от выбранного действия перейдёт в первое по стохастичному правилу.

\needspace{6\baselineskip}
\begin{wrapfigure}[9]{r}{0.35\textwidth}
\vspace{-0.5cm}
\centering
\includegraphics[width=0.35\textwidth]{Images/EquivMDP.png}
\vspace{-0.9cm}
\end{wrapfigure}

Покажем, что от самого общего случая (генерации награды и состояний из распределения $p(r, s' \HM\mid s, a)$) можно перейти к детерминированной функции награды только от текущего состояния. Добавим в описание состояний информацию о последнем действии и последней полученной агентом награде, то есть для каждого возможного (имеющего ненулевую вероятность) перехода $(s, a, r, s')$ размножим $s'$ по числу\footnote[*]{которое в худшем случае континуально, так как награда --- вещественный скаляр.} возможных исходов $p(r \HM\mid s, a)$ и по числу действий. Тогда можно считать, что на очередном шаге вместо $r(s, a)$ агенту выдаётся $r(s')$, и вся стохастика процесса формально заложена только в функции переходов. \QED
\end{theorem}

Считать функцию награды детерминированной удобно, поскольку позволяет не городить по ним мат. ожидания (иначе нужно добавлять сэмплы наград в определение траекторий). В любом формализме всегда принято считать, что агент сначала получает награду и только затем наблюдает очередное состояние. 

\begin{definition}
MDP называется \emph{конечным} (finite MDP) или \emph{табличным}, если пространства состояний и действий конечны: $|\St| < \infty, |\A| < \infty$.
\end{definition}

\begin{exampleBox}[label=ex:mdp]{}
Для конечных MDP можно над дугами в графе среды указать награду за переход по ним или выдать награду состояниям (в рамках нашего формализма награда <<за состояние>> будет получена, давайте считать, при выполнении любого действия из данного состояния).
\begin{center}
\animategraphics[controls, width=0.9\linewidth]{1}{Images/MDP/MDP-}{1}{6}
\end{center}
\end{exampleBox}

\subsection{Эпизодичность}

Во многих случаях процесс взаимодействия агента со средой может при определённых условиях <<заканчиваться>>, причём факт завершения доступен агенту.

\begin{definition}
Состояние $s$ называется \emph{терминальным} (terminal) в MDP, если $\forall a \in \A \colon$
$$\Prob (s' = s \mid s, a) = 1 \qquad r(s, a) = 0,$$
то есть с вероятностью 1 агент не сможет покинуть состояние.
\end{definition}

\begin{example}
В примере \ref{ex:mdp} самое правое состояние является терминальным.
\end{example}

Считается, что на каждом шаге взаимодействия агент дополнительно получает для очередного состояния $s$ значение предиката $\done(s) \HM\in \{0, 1\}$, является ли данное состояние терминальным. По сути, после попадания в терминальное состояние дальнейшее взаимодействие бессмысленно (дальнейшие события тривиальны), и, считается, что возможно произвести \emph{reset} среды в $s_0$, то есть начать процесс взаимодействия заново. Введение терминальных состояний именно таким способом позволит всюду в теории писать суммы по времени до бесконечности, не рассматривая отдельно случай завершения за конечное время.

\begin{remark}
Для агента все терминальные состояния в силу постановки задачи неразличимы, и их описания среда обычно не возвращает (вместо этого на практике она обычно проводит ресет и возвращает $s_0$ следующего эпизода).
\end{remark}

\begin{definition}
Один цикл процесса от стартового состояния до терминального называется \emph{эпизодом} (episode).
\end{definition}

Продолжительности эпизодов (количество шагов взаимодействия) при этом, конечно, могут различаться от эпизода к эпизоду.

\begin{definition}
Среда называется \emph{эпизодичной} (episodic), если для любой стратегии процесс взаимодействия гарантированно завершается не более чем за некоторое конечное $T^{\max}$ число шагов.
\end{definition}

\begin{theoremBox}[label=th:episodicmdpistree]{Граф эпизодичных сред есть дерево}
В эпизодичных средах вероятность оказаться в одном и том же состоянии дважды в течение одного эпизода равна нулю.
\begin{proof}
Если для некоторого состояния $s$ при некоторой комбинации действий через $T$ шагов агент с вероятностью $p \HM> 0$ вернётся в $s$, при повторении такой же комбинации действий в силу марковости с вероятностью $p^n \HM> 0$ эпизод будет длиться не менее $nT$ шагов для любого натурального $n$. Иначе говоря, эпизоды могут быть неограниченно долгими. 
\end{proof}
\end{theoremBox}

\begin{exampleBox}[righthand ratio=0.25, sidebyside, sidebyside align=center, lower separated=false]{Cartpole}
К тележке на шарнире крепится стержень с грузиком. Два действия позволяют придать тележке ускорение вправо или влево. Состояние описывается двумя числами: x-координатой тележки и углом, на которой палка отклонилась от вертикального положения.

Состояние считается терминальным, если x-координата стала слишком сильно отличной от нуля (тележка далеко уехала от своего исходного положения), или если палка отклонилась на достаточно большой угол.

Агент получает +1 каждый шаг и должен как можно дольше избегать терминальных состояний. Гарантии завершения эпизодов в этом MDP нет: агент в целом может справляться с задачей бесконечно долго.

\tcblower
\includegraphics[width=\textwidth]{Images/cartpole.png}
\end{exampleBox}

\begin{remark}
На практике, в средах обычно существуют терминальные состояния, но нет гарантии завершения эпизодов за ограниченное число шагов. Это лечат при помощи таймера --- жёсткого ограничения, требующего по истечении $T^{\max}$ шагов проводить в среде ресет. Чтобы не нарушить теоретические предположения, необходимо тогда заложить информацию о таймере в описание состояний, чтобы агент знал точное время до прерывания эпизода. Обычно так не делают; во многих алгоритмах RL будет возможно использовать только <<начала>> траекторий, учитывая, что эпизод не был доведён до конца. Формально при этом нельзя полагать $\done \left( s_{T^{\max}} \right) \HM= 1$, но в коде зачастую так всё равно делают. Например, для Cartpole в реализации OpenAI Gym по умолчанию по истечении 200 шагов выдаётся флаг $\done$, что формально нарушает марковское свойство.
\end{remark}

\subsection{Дисконтирование}

Наша задача заключается в том, чтобы найти стратегию $\pi$, максимизирующую среднюю суммарную награду. Формально, нам явно задан функционал для оптимизации:
\begin{equation}\label{sumreward}
\E_{\Traj \sim \pi} \sum_{t \ge 0} r_t \to \max_{\pi},
\end{equation}
где $r_t \HM\coloneqq r(s_t, a_t)$ --- награда на шаге $t$.

Мы хотим исключить из рассмотрения MDP, где данный функционал может улететь в бесконечность\footnote{если награда может быть бесконечной, начинаются всякие парадоксы, рассмотрения которых мы хотим избежать. Допустим, в некотором MDP без терминальных состояний мы знаем, что оптимальная стратегия способна получать +1 на каждом шаге, однако мы смогли найти стратегию, получающую +1 лишь на каждом втором шаге. Формально, средняя суммарная награда равна бесконечности у обоих стратегий, однако понятно, что найденная стратегия <<неоптимальна>>.} или не существовать вообще. Во-первых, введём ограничение на модуль награды за шаг, подразумевая, что среда не может поощрять или наказывать агента бесконечно сильно:
\begin{equation}\label{reward_limit}
\forall s, a \colon |r(s, a)| \le r^{\max}
\end{equation}

Чтобы избежать парадоксов, этого условия нам не хватит\footnote{могут возникнуть ситуации, где суммарной награды просто не существует (например, если агент в бесконечном процессе всегда получает +1 на чётных шагах и -1 на нечётных).}. Введём \emph{дисконтирование} (discounting), коэффициент которого традиционно обозначают $\gamma$:
 
\begin{definition}
\emph{Дисконтированной кумулятивной наградой} (discounted cumulative reward) или \emph{total return} для траектории $\Traj$ с коэффициентом $\gamma \HM\in (0, 1]$ называется
\begin{equation}\label{return}
R(\Traj) \coloneqq \sum_{t \ge 0} \gamma^t r_t
\end{equation}
\end{definition}

У дисконтирования есть важная интерпретация: мы полагаем, что на каждом шаге с вероятностью $1 \HM- \gamma$ взаимодействие обрывается, и итоговым результатом агента является та награда, которую он успел собрать до прерывания. Это даёт приоритет получению награды в ближайшее время перед получением той же награды через некоторое время. Математически смысл дисконтирования, во-первых, в том, чтобы в совокупности с требованием \eqref{reward_limit} гарантировать ограниченность оптимизируемого функционала, а во-вторых, выполнение условий некоторых теоретических результатов, которые явно требуют $\gamma \HM< 1$. В силу последнего, гамму часто рассматривают как часть MDP.

\begin{definition}
\emph{Скором} (score или performance) стратегии $\pi$ в данном MDP называется
\begin{equation}\label{goal}
J(\pi) \coloneqq \E_{\Traj \sim \pi} R(\Traj)
\end{equation}
\end{definition}

Итак, задачей обучения с подкреплением является оптимизация для заданного MDP средней дисконтированной кумулятивной награды:
\begin{equation*}
J(\pi) \to \max_{\pi}
\end{equation*}

\begin{exampleBox}[label=ex:score]{}
Посчитаем $J(\pi)$ для приведённого на рисунке MDP, $\gamma = \frac{10}{11}$ и начального состояния А. Ясно, что итоговая награда зависит только от стратегии агента в состоянии A, поэтому можно рассмотреть все стратегии, обозначив $\pi(a = \text{\colorsquare{ChadRed}} \mid s = A)$ за параметр стратегии $\theta \in [0, 1]$.

\begin{wrapfigure}{r}{0.3\textwidth}
\centering
\includegraphics[width=0.3\textwidth]{Images/Score.png}
\vspace{-1cm}
\end{wrapfigure}

C вероятностью $\theta$ агент выберет действие \colorsquare{ChadRed}, после чего попадёт в состояние B с вероятностью 0.8. Там вне зависимости от стратегии он начнёт крутится и получит в пределе
$$\gamma + \gamma^2 + \dots + = \sum\limits_{t \ge 1} \gamma^t = \frac{\gamma}{1 - \gamma} = 10.$$
Ещё с вероятностью $1 - \theta$ агент выберет \colorsquare{ChadBlue}, получит +3 и попадёт в терминальное состояние, после чего эпизод завершится. Итого:
$$J(\pi) = \underbrace{\left( 0.8\theta \right) \cdot 10}_{\text{\colorsquare{ChadRed}}} + \underbrace{\left(1 - \theta \right) \cdot 3}_{\text{\colorsquare{ChadBlue}}} = 3 + 5\theta$$

Видно, что оптимально выбрать $\theta = 1$, то есть всегда выбирать действие \colorsquare{ChadRed}. Заметим, что если бы мы рассмотрели другое $\gamma$, мы бы могли получить другую оптимальную стратегию; в частности, при $\gamma = \frac{15}{19}$ значение $J(\pi)$ было бы константным для любых стратегий, и оптимальными были бы все стратегии. 
\end{exampleBox}

Всюду далее подразумевается\footnote{в качестве акта педантичности оговоримся, что также всюду подразумевается измеримость всех функций, необходимая для существования всех рассматриваемых интегралов и мат.ожиданий.} выполнение требования ограниченности награды \eqref{reward_limit}, а также или дисконтирования $\gamma \HM< 1$, или эпизодичности среды.

\begin{proposition}
При сделанных предположениях скор ограничен.
\begin{proof}
Если $\gamma \HM< 1$, то по свойству геометрической прогрессии для любых траекторий $\Traj$:
\begin{equation*}
R(\Traj) = \left| \sum_{t \ge 0} \gamma^t r_t \right| \le \frac{1}{1 - \gamma} r^{\max}
\end{equation*}
Если же $\gamma = 1$, но эпизоды гарантированно заканчиваются не более чем за $T^{\max}$ шагов, то суммарная награда не превосходит по модулю $T^{\max} r^{\max}$. Следовательно, скор как мат.ожидание от ограниченной величины также удовлетворяет этим ограничениям.
\end{proof}
\end{proposition}
