\documentclass[../main.tex]{subfiles}

\begin{document}

\chapter{Задача обучения с подкреплением}

Ясно, что обучение с учителем это не та модель <<обучения>>, которая свойственна интеллектуальным сущностям. Термин <<обучение>> здесь подменяет понятие интерполяции или, если уж на то пошло, построение алгоритмов неявным образом (<<смотрите, оно само обучилось, я сам ручками не прописывал, как кошечек от собачек отличать>>). К полноценному обучению, на которое способен не только человек, но и в принципе живые организмы, задачи классического машинного обучения имеют лишь косвенное отношение. Значит, нужна другая формализация понятия <<задачи, требующей интеллектуального решения>>, в которой обучение будет проводится не на опыте, заданном прецедентно (в виде обучающей выборки).

%\setlength{\columnsep}{0.7cm}
\begin{wrapfigure}{r}{0.4\linewidth}
%\vspace{-0.3cm}
\centering
\includegraphics[width=0.4\textwidth]{Images/agentenv.jpeg}
%\caption{Пример среды с двумя действиями.}
%\label{fig:env}
%\vspace{-2.6cm}
\end{wrapfigure}

Термин \emph{подкрепление} (reinforcement) пришёл из \href{https://ru.wikipedia.org/wiki/\%D0\%91\%D0\%B8\%D1\%85\%D0\%B5\%D0\%B2\%D0\%B8\%D0\%BE\%D1\%80\%D0\%B8\%D0\%B7\%D0\%BC}{поведенческой психологии} и обозначает награду или наказание за некоторый получившийся результат, зависящий не только от самих принятых решений, но и внешних, не обязательно подконтрольных, факторов. Под обучением здесь понимается поиск способов достичь желаемого результата \emph{методом проб и ошибок} (trial and error), то есть попыток решить задачу и использование накопленного опыта для усовершенствования своей стратегии в будущем.

В данной главе будут введены основные определения и описана формальная постановка задачи. Под желаемым результатом мы далее будем понимать максимизацию некоторой скалярной величины, называемой \emph{наградой} (reward). Интеллектуальную сущность (систему/робота/алгоритм), принимающую решения, будем называть \emph{агентом} (agent). Агент взаимодействует с \emph{миром} (world) или \emph{средой} (environment), которая задаётся зависящим от времени \emph{состоянием} (state). Агенту в каждый момент времени в общем случае доступно только некоторое \emph{наблюдение} (observation) текущего состояния мира. Сам агент задаёт процедуру выбора \emph{действия} (action) по доступным наблюдениям; эту процедуру далее будем называть \emph{стратегией} или \emph{политикой} (policy). Процесс взаимодействия агента и среды задаётся \emph{динамикой среды} (world dynamics), определяющей правила смены состояний среды во времени и генерации награды.

Буквы $s$, $a$, $r$ зарезервируем для состояний, действий и наград соответственно; буквой $t$ будем обозначать время в процессе взаимодействия. 

\import{1.Setup/}{1.1.MDP.tex}
\import{1.Setup/}{1.2.Algorithms.tex}

\end{document}