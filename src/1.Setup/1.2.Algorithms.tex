\section{Алгоритмы обучения с подкреплением}

\subsection{Условия задачи RL}

Основной постановкой в обучении с подкреплением является задача нахождения оптимальной стратегии на основе собственного опыта взаимодействия. Это означает, что алгоритму обучения изначально доступно только:

\begin{itemize}
    \item вид пространства состояний --- количество состояний в нём в случае конечного числа, или размерность пространства $\R^d$ в случае признакового описания состояний.
    \item вид пространства действий --- непрерывное или дискретное. Некоторые алгоритмы будут принципиально способны работать только с одним из этих двух видов.
    \item взаимодействие со средой, то есть возможность для предоставленной алгоритмом стратегии $\pi$ генерировать траектории $\Traj \sim \pi$; иными словами, принципиально доступны только сэмплы из trajectory distribution \eqref{traj_expectation}.
\end{itemize}

Примерно так выглядит MDP для начинающего обучение агента:

\begin{center}
    \includegraphics[width=0.9\textwidth]{Images/AgentPOV.png}
\end{center}

Итак, в отличие от обучения с учителем, где датасет <<дан алгоритму на вход>>, здесь агент должен сам собрать данные. Находясь в некотором состоянии, обучающийся агент обязан выбрать ровно одно действие, получить ровно один сэмпл $s'$ и продолжить взаимодействие (накопление опыта --- сбор сэмплов) из $s'$. Собираемые в ходе взаимодействия данные и представляют собой всю доступную агенту информацию для улучшения стратегии.

\begin{definition}
Пятёрки $\T \HM\coloneqq \left( s, a, r, s', \done \right)$, где $r \HM\coloneqq r(s, a)$, $s' \HM\sim p(s' \HM\mid s, a)$, $\done \HM\coloneqq \done(s')$, называются \emph{переходами} (transitions). 
\end{definition}

Таким образом, в RL-алгоритме должна быть прописана стратегия взаимодействия со средой во время обучения (\emph{behavior policy}), которая может отличаться от <<итоговой>> стратегии (\emph{target policy}), предназначенной для использования в среде по итогам обучения. 

% Если среда не является эпизодичной, это задача \emph{непрерывного обучения} (continual learning). Это сильно более сложная задача, хотя бы потому, что к ней можно свести ситуацию с нестационарной функцией переходов\footnote{для этого достаточно добавить в описание состояний информацию о количестве шагов всего прошедшего обучения. Понятно, что в среде с таким пространством состояний терминальных состояний быть не может.}; свойство эпизодичности позволяет RL-алгоритму пробовать разные варианты в <<одних и тех же условиях>> и обучаться за счёт их сравнения.

\subsection{Сравнение с обучением с учителем}

Необходимость собирать данные внутри самого алгоритма --- одно из ключевых отличий задачи RL от обучения с учителем, где выборка подаётся алгоритму на вход. Здесь же <<сигнал>> для обучения предоставляется дизайном функции награды; на практике, чтобы говорить о том, что встретилась задача RL, необходимо задать MDP (предоставить как среду в виде симулятора или интерфейс для взаимодействия настоящего робота в реальном мире, так и функцию награды). Зачастую если какую-то <<репрезентативную>> выборку собрать можно, всегда проще и лучше обратиться к обучению с учителем как менее общей постановке задачи.  

\begin{proposition}
Задача обучения с учителем (с заданной функцией потерь) является частным случаем задачи RL.
\begin{proof}
Пусть дана обучающая выборка в виде пар $(x, y)$, $x$ --- входной объект, $y$ --- целевая переменная. Скажем, что начальное состояние есть описание объекта $x$, случайно сэмплированного из обучающей выборки (начальное состояние будет случайно). Агент выбирает метку класса $\hat{y} \sim \pi(\hat{y} \mid x)$. После этого он получает награду за шаг в в размере $-\Loss(\hat{y}, y)$ и игра завершается. Оптимизация такой функции награды в среднем будет эквивалентно минимизации функции потерь в обучении с учителем. 
\end{proof}
\end{proposition}

Поскольку RL всё-таки предполагает, что в общем случае эпизоды длиннее одного шага, агент будет своими действиями влиять на дальнейшие состояния, которые он встречает. <<Управление>> марковской цепью куда более существенная часть оптимизации RL алгоритмов, нежели максимизация наград за шаг, именно из этого свойства возникает большинство специфических именно для RL особенностей. И зачастую именно в таких задачах появляется такая непреодолимая для обучения с учителем проблема, как то, что правильный ответ никакому человеку, в общем-то, неизвестен. Не знаем мы обычно оптимальный наилучший ход в шахматах или как правильно поворачивать конечности роботу, чтобы начать ходить. 

Да, доступной помощью для алгоритма могут быть \emph{данные от эксперта}, то есть записи взаимодействия со средой некоторой стратегии (или разных стратегий), не обязательно, вообще говоря, оптимальной. Алгоритм RL, возможно, сможет эти данные как-то использовать, или хотя бы как-либо на них предобучиться. В простейшем случае предобучение выглядит так: если в алгоритме присутствует параметрически заданная стратегия $\pi_\theta$, можно \emph{клонировать поведение} (behavior cloning), т.е. восстанавливать по парам $s, a$ из всех собранных экспертом траекторий функцию $\St \HM\to \A$ (это обычная задача обучения с учителем), учиться воспроизводить действия эксперта. Задача обучения по примерам её решения около-оптимальным экспертом называется \emph{имитационным обучением} (imitation learning); её мы обсудим отдельно в разделе \ref{sec:imitationlearning}, однако, если эксперт не оптимален, обученная стратегия вряд ли будет действовать хоть сколько-то лучше. Здесь можно провести прямую аналогию с задачей обучения с учителем, где верхняя граница качества алгоритма определяется качеством разметки; если разметка зашумлена и содержит ошибки, обучение вряд ли удастся. 

В общем же случае мы считаем, что на вход в алгоритм никаких данных эксперта не поступает. Поэтому говорят, что в обучении с подкреплением нам не даны <<правильные действия>>, и, когда мы будем каким-либо образом сводить задачу к задачам регрессии и классификации, нам будет важно обращать внимание на то, как мы <<собираем себе разметку>> из опыта взаимодействия со средой и какое качество мы можем от этой разметки ожидать. В идеале, с каждым шагом алгоритм сможет собирать себе всё более и более <<хорошую>> разметку (получать примеры траекторий с всё большей суммарной наградой), за счёт неё выбирать всё более и более оптимальные действия, и так <<вытягивать сам себя из болота>>.

Здесь важно задать следующий вопрос: а если у нас есть данные из очень плохой стратегии, что можно рассчитывать с них выучить? То есть если мы можем собирать лишь примеры траекторий, в которых агент совершенно случайно себя ведёт (и редко получает набирает положительную награду), можно ли с таких данных выучить, например, оптимальную стратегию? Оказывается, как мы увидим дальше, за счёт структуры задачи RL и формализма MDP ответ положительный. Само собой, такое обучение только будет страшно неэффективным как по времени работы, так и по требуемым объёмам данных.

\subsection{Концепция model-free алгоритмов}

Ещё одним возможным существенным изменением сеттинга задачи является наличие у агента прямого доступа к функции переходов $p(s' \HM\mid s, a)$ и функции награды.

\begin{definition}
Будем говорить, что у агента есть \emph{симулятор} или <<доступ к функции переходов>>, если он знает функцию награды и может в любой момент процесса обучения сэмплировать произвольное число сэмплов из $p(s' \mid s, a)$ для любого набора пар $s, a$.
\end{definition}

Симулятор --- по сути копия среды, которую агент во время обучения может откатывать к произвольному состоянию. Симулятор позволяет агенту строить свою стратегию при помощи \emph{планирования} (planning) --- рассмотрения различных вероятных версий предстоящего будущего и использования их для текущего выбора действия. При использовании планирования в идеальном симуляторе никакого процесса непосредственного обучения в алгоритме может не быть, поскольку если на руках есть симулятор, то данные собирать не нужно: можно из симулятора сколько захотим данных получить.

\begin{exampleBox}[righthand ratio=0.15, sidebyside, sidebyside align=center, lower separated=false]{}
Примером задач, в которых у алгоритма есть симулятор, являются пятнашки или кубик-рубик. То есть, пытаясь создать алгоритм, собирающий кубик-рубик, мы, естественно, можем пользоваться знаниями о том, в какую конфигурацию переводят те или иные действия текущее положение, и таким образом напрашиваются какие-то алгоритмы разумного перебора --- <<планирование>>.

\tcblower
\includegraphics[width=\textwidth]{Images/puzzle.png}
\end{exampleBox}

\begin{example}
Любые задачи, в которых среда реализована виртуально, можно рассматривать как задачи, где у агента есть симулятор. Например, если мы хотим обучить бота в Марио, мы можем сказать: да у нас есть исходники кода Марио, мы можем взять любую игровую ситуацию (установить симулятор в любое состояние) и для любого действия посмотреть, что будет дальше. В RL по умолчанию считается, что в видеоиграх такого доступа нет: динамика среды изначально агенту неизвестна.
\end{example}

\begin{exampleBox}[righthand ratio=0.3, sidebyside, sidebyside align=center, lower separated=false]{}
Примером задач, в которых у алгоритма принципиально нет симулятора, являются любые задачи реальной робототехники. Важно, что даже если окружение реального робота симулируется виртуально, такая симуляция неточна --- отличается от реального мира. В таких ситуациях можно говорить, что имеется \emph{неидеальный симулятор}. Отдельно стоит уточнить, доступен ли симулятор реальному роботу в момент принятия решения (возможно, симулятор реализован на куда более вычислительно мощной отдельной системе) --- тогда он может использоваться во время обучения, но не может использоваться в итоговой стратегии.

\tcblower
\includegraphics[width=\textwidth]{Images/realsim.png}
\end{exampleBox}

По умолчанию всегда считается, что доступа к динамике среды нет, и единственное, что предоставляется алгоритму --- среда, с которой возможно взаимодействовать. Можно ли свести такую задачу к планированию? В принципе, алгоритм может пытаться обучать себе подобный симулятор --- строить генеративную модель, по $s, a$ выдающую $s', r(s, a), \done(s')$ --- и сводить таким образом задачу к планированию. Приближение тогда, естественно, будет неидеальным, да и обучение подобного симулятора сопряжено с рядом других нюансов. Например, в сложных средах в описании состояний может хранится колоссальное количество информации, и построение моделей, предсказывающих будущее, может оказаться вычислительно неподъёмной и неоправданно дорогой задачей. 

Одна из фундаментальных парадигм обучения с подкреплением, вероятно, столь же важная, как парадигма end-to-end обучения для глубокого обучения --- идея model-free обучения. Давайте не будем учить динамику среды и перебирать потенциальные варианты будущего для поиска хороших действий, а выучим напрямую связь между текущим состоянием и оптимальными действиями.

\begin{definition}
Алгоритм RL классифицируется как \emph{model-free}, если он не использует и не пытается выучить модель динамики среды $p(s' \HM\mid s, a)$. 
\end{definition}

\begin{example}
Очень похоже, что когда мы учимся кататься на велосипеде, мы обучаемся как-то в model-free режиме. Мы, находясь в некотором состоянии, не планируем, в какой конфигурации окажется велосипед при разных возможных поворотах наших рук, и вместо этого учимся в точности методом проб и ошибок: мы просто запоминаем, при каком ощущении положения тела как нужно поворачивать руль. 

Здесь очень любопытно пофилософствовать на тему того, почему в таких задачах мы зачастую умеем <<запоминать>> на всю жизнь полученные знания, не разучиваясь ездить на велосипеде после долгих лет без тренировок. Например, нейросетевые модели, которые мы дальше будем обучать для решения задач RL, таким свойством не обладают, и, обучаясь на одном опыте, они старый забывают; если агента RL обучать кататься на велосипеде, а потом долго учить кататься на самокате, стратегия для велосипеда крайне вероятно <<сломается>>.

Оказывается, человек тоже может разучиться ездить на велосипеде при помощи очень хитрой процедуры: нужно придумать такой <<самокат>>, обучение езде на котором требует противоположных навыков, чем велосипед. В качестве такого <<самоката>> можно взять <<обратный велосипед>> (<<The Backwards Bicycle>>): велосипед, в котором поворот руля влево отклоняет колесо вправо, и наоборот. Подробнее про этот эксперимент можно посмотреть в \href{https://www.youtube.com/watch?v=MFzDaBzBlL0}{этом видео}. Интересно, что обе стратегии --- и для езды на велосипеде, и для езды на <<обратном велосипеде>> --- восстанавливаются после некоторой тренировки (причём как-то подозрительно резко, с каким-то <<фазовым переходом>>) и в конечном счёте уживаются вместе.
\end{example}

\subsection{On-policy vs Off-policy}

В model-free алгоритмах сбор данных становится важной составной частью: определяя политику взаимодействия со средой (behavior policy), мы влияем на то, для каких состояний $s, a$ мы получим сэмпл $s'$ из функции переходов. Собираемые данные --- траектории --- алгоритм может запоминать, например, в памяти. Но не каждый алгоритм RL сможет пользоваться такими сохранёнными данными, и поэтому возникает ещё одна важная классификация RL алгоритмов.

\begin{definition}
Алгоритм RL называется \emph{off-policy}, если он может использовать для обучения опыт взаимодействия произвольной стратегии.
\end{definition}

\begin{definition}
Алгоритм RL называется \emph{on-policy}, если для очередной итерации алгоритма ему требуется опыт взаимодействия некоторой конкретной, предоставляемой самим алгоритмом, стратегии.
\end{definition}

Некоторое пояснение названия этих терминов: обычно в нашем алгоритме явно или неявно будет присутствовать некоторая <<текущая>>, <<наилучшая>> найденная стратегия $\pi$: та самая целевая политика (target policy), которую алгоритм выдаст в качестве ответа, если его работу прервать. Если алгоритм умеет улучшать её (проводить очередную итерацию обучения), используя сэмплы любой другой произвольной стратегии $\mu$, то мы проводим <<off-policy>> обучение: обучаем политику <<не по ней же самой>>. On-policy алгоритмам будет необходимо <<отправлять в среду конкретную стратегию>>, поскольку они будут способны улучшать стратегию $\pi$ лишь по сэмплам из неё же самой, будут <<привязаны к ней>>; это существенное ограничение. Другими словами, если обучение с подкреплением --- обучение на основе опыта, то on-policy алгоритмы обучаются на своих ошибках, когда off-policy алгоритмы могут учиться как на своих, так и на чужих ошибках.

Off-policy алгоритм должен уметь проводить очередной шаг обучения на произвольных траекториях, сгенерированных произвольными (возможно, разными, возможно, неоптимальными) стратегиями. Понятие принципиально важно тем, что алгоритм может потенциально переиспользовать траектории, полученные старой версией стратегии со сколь угодно давних итераций. Если алгоритм может переиспользовать опыт, но с ограничениями (например, только с недавних итераций, или только из наилучших траекторий), то мы всё равно будем относить его к on-policy, поскольку для каждой новой итерации алгоритма нужно будет снова собирать сколько-то данных. Это не означает, что для on-policy алгоритма совсем бесполезны данные от (возможно, неоптимального) эксперта; почти всегда можно придумать какую-нибудь эвристику, как воспользоваться ими для хотя бы инициализации (при помощи того же клонирования поведения). Важно, что off-policy алгоритм сможет на данных произвольного эксперта провести <<полное>> обучение, то есть условно сойтись к оптимуму при достаточном объёме и разнообразии экспертной информации, не потребовав вообще никакого дополнительного взаимодействия со средой.

\subsection{Классификация RL-алгоритмов}

При рассмотрении алгоритмов RL мы начнём с рассмотрения именно model-free алгоритмов и большую часть времени посвятим им. Их часто делят на следующие подходы:
\begin{itemize}
    \item \emph{мета-эвристики} (metaheuristic) никак не используют внутреннюю структуру взаимодействия среды и агента, и рассматривают задачу максимизации $J(\pi)$ \eqref{goal} как задачу <<black box оптимизации>>: можно примерно оценивать, чему равно значение функционала для разных стратегий, а структура задачи --- формализм MDP --- не используется; мы рассмотрим мета-эвристики в главе \ref{metaheuristicchapter} как не требующие построения особой теории. 
    \item \emph{value-based} алгоритмы получают оптимальную стратегию неявно через теорию оценочных функций, которую мы рассмотрим в главе \ref{classictheorychapter}. Эта теория позволит нам построить value-based алгоритмы (глава \ref{valuebasedchapter}) и будет использоваться всюду далее.
    \item \emph{policy gradient} алгоритмы максимизируют $J(\pi)$, используя оценки градиента функционала по параметрам стратегии; мы сможем помочь процессу оптимизации, правильно воспользовавшись оценочными функциями (глава \ref{policygradientchapter}).
\end{itemize}

Затем в главе \ref{continuouscontrolchapter} мы отдельно обсудим несколько алгоритмов специально для непрерывных пространств действий, находящихся на стыке value-based и policy gradient подхода, и увидим, что между ними довольно много общего. Наконец, \emph{model-based} алгоритмы, которые учат или используют предоставленную модель среды $p(s' \HM\mid s, a)$, и которые обычно выделяют в отдельную категорию, будут рассмотрены после в главе \ref{modelbasedchapter}.

\subsection{Критерии оценки RL-алгоритмов}

При оценивании алгоритмов принципиально соотношение трёх критериев:
\begin{itemize}
    \item \emph{performance}: Монте-Карло оценка значения $J(\pi)$;
    \item \emph{wall-clock time}: реальное время работы, потребовавшееся алгоритму для достижения такого результата (в полной аналогии с классическими методами оптимизации);
    \item \emph{sample efficiency}: количество сэмплов (или шагов) взаимодействия со средой, потребовавшихся алгоритму. Этот фактор может быть ключевым, если взаимодействие со средой дорого (например, обучается реальный робот); 
\end{itemize}

\begin{remark}Поскольку \href{https://openai.com/projects/five/}{победами над кожаными мешками в дотах} уже никого не удивишь, вторые два фактора начинают играть всё большую роль.
\end{remark}

Конечно, на практике мы хотим получить как можно больший performance при наименьших затратах ресурсов. Время работы алгоритма (wall-clock time) и эффективность по сэмплам связаны между собой, но их следует различать в силу того, что в разных средах сбор данных --- проведение одного шага взаимодействия в среде --- может быть как относительно дешёвым, так и очень дорогим. Например, если мы говорим о реальных роботах, то один шаг взаимодействия со средой --- сверхдорогая операция, по сравнению с которыми любые вычисления на компьютере можно считать очень дешёвыми. Тогда нам выгодно использовать алгоритм, который максимально эффективен по сэмплам. Если же среда задана симулятором, а у нас ещё и есть куча серверов, чтобы параллельно запустить много таких симуляторов, то сбор данных для нас становится дешёвой процедурой. Тогда выгодно не гнаться за sample efficiency и выбирать вычислительно дешёвые алгоритмы.

Очень условно классы RL алгоритмов располагаются по sample efficiency в следующем порядке: самыми неэффективными по требуемым объёмам данных алгоритмами являются мета-эвристики. Зато в них практически не будет никаких вычислений: очень условно, на каждое обновление весов модели будет приходиться сбор нескольких сотен переходов в среде. Их будет иметь смысл применять, если есть возможность параллельно собирать много данных. 

Далее, в Policy Gradient подходе мы будем работать в on-policy режиме: текущая, целевая, политика будет отправляться в среду, собирать сколько-то данных (например, мини-батч переходов, условно проводить порядка 32-64 шагов в среде) и затем делать одно обновление модели. Очень приближённо и с массой условностей говорят, что эффективность по сэмплам будет на порядок выше, чем у эволюционных алгоритмов, за счёт проведения на порядок большего количества вычислений: на одно обновление весов приходится сбор 30-60 переходов.

В value-based подходе за счёт off-policy режима работы можно будет контролировать соотношение числа собираемых переходов к количеству обновлений весов, но по умолчанию обычно полагают, что алгоритм собирает 1 переход и проводит 1 итерацию обучения. Таким образом, вычислений снова на порядок больше, как и (потенциально) sample efficiency.

Наконец, model-based вычислительно страшно тяжеловесные алгоритмы, но за счёт этого можно попытаться добиться максимальной эффективности по сэмплам. 

Отразить четыре класса алгоритмов можно на условной картинке:

\begin{center}
    \includegraphics[width=0.9\textwidth]{Images/SampleEfficiency.png}
\end{center}

В зависимости от скорости работы среды, то есть времени, затрачиваемой на сбор данных, а также от особенностей среды (связанных непосредственно со спецификой этих четырёх классов алгоритмов), оптимальное соотношение ресурсы-качество может достигаться на разных классах алгоритмов. Но, к сожалению, на практике редко бывает очевидно, алгоритмы какого класса окажутся наиболее подходящими в конкретной ситуации.

В отличие от классических методов оптимизации, речи о критерии останова идти не будет, поскольку адекватно разумно проверить около-оптимальность текущей стратегии не представляется возможным в силу слишком общей постановки задачи. Считается, что оптимизация (обучение за счёт получения опыта взаимодействия) происходит, пока доступны вычислительные ресурсы; в качестве итога обучения предоставляется или получившаяся стратегия, или наилучшая встречавшаяся в ходе всего процесса.

\subsection{Сложности задачи RL}\label{RLproblems}

Обсудим несколько <<именованных>> проблем задачи обучения с подкреплением, с которыми сталкивается любой алгоритм решения.

Проблема \emph{застревания в локальных оптимумах} приходит напрямую из методов оптимизации. В оптимизируемом функционале \eqref{goal} может существовать огромное количество стратегий $\pi$, для которых его значение далеко не максимально, но все в некотором смысле <<соседние>> стратегии дают в среднем ещё меньшую награду.

\begin{exampleBox}[righthand ratio=0.25, sidebyside, sidebyside align=center, lower separated=false]{}
Часто агент может выучить тривиальное <<пассивное>> поведение, которое не приносит награды, но позволяет избегать каких-то штрафов за неудачи. Например, агент, который хочет научиться перепрыгивать через грабли, чтобы добраться до тортика (+1), может несколько раз попробовать отправиться за призом, но наступить на грабли (-1), и выучить ничего не делать (+0). Ситуация весьма типична: например, Марио может пару раз попробовать отправиться покорять первый уровень, получить по башке и решить не ходить вправо, а тупить в стену и получать <<безопасный>> +0. Для нашей задачи оптимизации это типичнейшие локальные экстремумы: <<похожие стратегии>> набирают меньше текущей, и, чтобы добраться до большей награды, нужно как-то найти совершенно новую область в пространстве стратегий.

\tcblower
\includegraphics[width=\textwidth]{Images/MarioStupid.png}
\end{exampleBox}

Другие проблемы куда более характерны именно для RL. Допустим, агент совершает какое-то действие, которое запускает в среде некоторый процесс. Процесс протекает сам по себе без какого-либо дальнейшего вмешательства агента и завершается через много шагов, приводя к награде. Это проблема \emph{отложенного сигнала} (delayed reward) --- среда даёт фидбэк агенту спустя какое-то (вообще говоря, неограниченно длительное) время.

\begin{exampleBox}[righthand ratio=0.25, sidebyside, sidebyside align=center, lower separated=false]{}
Пример из видеоигр --- в игре Atari Space Invaders очки даются в момент, когда удачный выстрел попал во вражеское НЛО, а не когда агент этот выстрел, собственно, совершает. Между принятием решения и получением сигнала проходит до нескольких секунд, и за это время агент принимает ещё несколько десятков решений.

\tcblower
\includegraphics[width=\textwidth]{Images/SpaceInvaders.png}
\end{exampleBox}

Если бы этого эффекта вдруг не было, и агент за каждое своё действие мгновенно получал бы всю награду $r(s, a)$, то решением задачи было бы банальное $\pi(s) = \argmax\limits_a r(s, a)$ --- жадная оптимизация функции награды. Очевидно, это никогда не решение: <<часть>> награды сидит в $s'$, в сэмпле следующего состояния, которого мы добились своим выбором действия, и в описании этого следующего состояния в силу свойства марковости обязана хранится информация о том, сколько времени осталось до получения награды за уже совершённые действия.

Смежная проблема --- какое именно из многих совершённых агентом действий привело к сигналу награды? \emph{Credit assignment problem} --- даже для уже собранного опыта может быть тяжело оценить, какие действия были правильными, а какие нет. 

\begin{example}
Вы поймали скунса, сварили яичницу, завезли банку горчицы в ближайший магазин штор, написали научную статью про шпроты, накормили скунса яичницей, сыграли в боулинг глобусом, вернулись домой после тяжёлого дня и получили +1. Вопрос: чему вы научились? 
\end{example}

Поскольку функция награды может быть произвольная, довольно типично, когда сигнал от среды --- неконстантная награда за шаг --- приходит очень редко. Это проблема \emph{разреженной награды} (sparse reward).

\begin{example}[Mountain Car]
\href{https://gym.openai.com/envs/MountainCar-v0/}{Визуализация задачи в OpenAI Gym}: тележка хочет забраться на горку, но для этого необходимо поехать в противоположном направлении, чтобы набрать разгона. Состояния описываются двумя числами (x-координата тележки и её скорость); действий три (придать ускорения вправо, влево, или ничего не делать). Функция награды равна -1 всюду: задачей агента является как можно скорее завершить эпизод. Однако, терминальное состояние --- это вершина горки, и для того, чтобы достичь его, нужно <<уже уметь>> задачу решать.  
\end{example}

Наконец, проблема, обсуждению которой мы посвятим довольно много времени --- \emph{дилемма исследования-использования} (exploration-exploitation trade-off). Пока ограничимся лишь примером для иллюстрации.

\begin{example}
Вы решили пойти сегодня в ресторан. Следует ли отправится в ваш любимый ресторан, или попробовать новый, в котором вы ещё ни разу не были?
\end{example}

Практическая проблема, отчасти связанная с тем, что алгоритму необходимо постоянно <<пробовать новое>> --- проблема \emph{<<безопасного обучения>>} (Safe RL). Грубо говоря, некоторые взаимодействия агента со средой крайне нежелательны даже во время обучения, в том числе когда агент ещё только учится. Эта проблема возникает в первую очередь для реальных роботов.

\begin{example}
Вы хотите научить реального робота мыть посуду. В начале обучения робот ничего не умеет и рандомно размахивает конечностями, бьёт всю посуду, переворачивает стол, сносит все лампочки и <<в исследовательских целях>> самовыкидывается из окна. В результате, начать второй обучающий эпизод становится довольно проблематично.
\end{example}

\begin{remark}
Здесь надо помнить, что безопасный RL не о том, как <<не сбивать пешеходов>> при обучении автономных автомобилей; он о том, как сбивать меньше пешеходов. RL по определению обучается на собственном опыте и в том числе собственных ошибках, и эти ошибки ему либо нужно совершить, либо получить в форме некоторой априорной информации (экспертных данных), хотя последнее всё равно не защищает от дальнейших исследований любых областей пространства состояний. Это означает, что на практике единственная полноценная защита от нежелательного поведения робота может быть проведена исключительно на этапе построения среды. То есть среда должна быть устроена так, что робот в принципе не может совершить нежелательных действий: опасные ситуации должны детектироваться средой (то есть --- внешне, внешним отдельным алгоритмом), а взаимодействие --- прерываться (с выдачей, например, отрицательной награды для агента).
\end{remark}

\begin{remark}
Одна из причин распространения видеоигр для тестирования RL --- отсутствие проблемы Safe RL: не нужно беспокоиться о том, что робот <<что-то сломает>> в процессе сбора опыта, если среда уже задана программной симуляцией.
\end{remark}

\subsection{Дизайн функции награды}

И есть ещё одна, вероятно, главная проблема\footnote{а точнее, в принципе главная проблема всей нашей жизни (what is your reward function?)}. \emph{Откуда берётся награда?} Алгоритмы RL предполагают, что награда, как и среда, заданы, <<поданы на вход>>, и эту проблему наши алгоритмы обучения, в отличие от предыдущих, решать идеологически не должны. Но понятно, что если для практического применения обучения с учителем боттлнеком часто является необходимость размечивать данные --- <<предоставлять обучающий сигнал>> --- то в RL необходимо аккуратно описать задачу при помощи функции награды.

\begin{definition}
<<\emph{Reward hypothesis}>>: любую интеллектуальную задачу можно задать (определить) при помощи функции награды.
\end{definition}

В обучении с подкреплением принято полагать эту гипотезу истинной. Но так ли это? RL будет оптимизировать ту награду, которую ему предоставят, и дизайн функции награды в ряде практических задач оказывается проблемой.

\begin{example}[<<Взлом>> функции награды]
\href{https://www.youtube.com/watch?v=tlOIHko8ySg}{Классический пример того}, как RL оптимизирует не то, что мы ожидаем, а ту награду, которую ему подсунули. Причина зацикливания агента видна в нижнем левом углу, где отображается счёт игры.
\end{example}

\begin{example}
Попробуйте сформулировать функцию награды для следующих интеллектуальных задач:
\begin{itemize}
    \item очистка мебели от пыли;
    \item соблюдение правил дорожного движения автономным автомобилем;
    \item захват мира;
\end{itemize}
\end{example}

Общее практическое правило звучит так: хорошая функция награды поощряет агента за достигнутые результаты, а не за то, каким способом агент этих результатов добивается. Это логично: если вдруг дизайнеру награды кажется, что он знает, как решать задачу, то вероятно, RL не особо и нужен. К сожалению, такая <<хорошая>> функция награды обычно разреженная.

\begin{exampleBox}[righthand ratio=0.35, sidebyside, sidebyside align=center, lower separated=false]{}
Если вы хотите научиться доезжать до дерева, то хорошая функция награды --- выдать агенту +1 в момент, когда он доехал до дерева. Если попытаться поощрять агента каждый шаг в размере <<минус расстояние до дерева>>, то может возникнуть непредвиденная ситуация: агент поедет прямо в забор, который стоит возле дерева, и это может оказаться с точки зрения такой награды выгоднее, чем делать длинный крюк по району вдали от дерева с целью этот самый забор объехать. 

\tcblower
\includegraphics[width=\textwidth]{Images/Tree.png}
\end{exampleBox}

Есть, однако, один способ, как можно функцию награды модифицировать, не изменяя решаемую задачу, то есть эквивалентным образом. К сигналу <<из среды>> можно что-то добавлять при условии, что на следующем шаге это что-то обязательно будет вычтено. Другими словами, можно применять трюк, который называется \emph{телескопирующая сумма} (telescoping sums): для любой последовательности $a_t$, т.~ч. $\lim\limits_{t \to \infty} a_t \HM= 0$, верно
\begin{equation}\label{telescopingsum}
    \sum_{t \ge 0}^\infty \left( a_{t+1} - a_t \right) = -a_0
\end{equation}

\begin{definition}
Пусть дана некоторая функция $\Phi(s) \colon \St \to \R$, которую назовём \emph{потенциалом}, и которая удовлетворяет двум требованиям: она ограничена и равна нулю в терминальных состояниях. Будем говорить, что мы проводим \emph{reward shaping} при помощи потенциала $\Phi(s)$, если мы заменяем функцию награды по следующей формуле:
\begin{equation}\label{rewardshaping}
r^{\mathrm{new}}(s, a, s') \coloneqq r(s, a) + \gamma \Phi(s') - \Phi(s)    
\end{equation}
\end{definition}

\begin{theorem}[Reward Shaping]
Проведение любого reward shaping по формуле \eqref{rewardshaping} не меняет задачи.

\begin{proof} Посчитаем суммарную награду для произвольной траектории. До reward shaping суммарная награда равнялась $\sum\limits_{t \ge 0} \gamma^t r_t$. Теперь же суммарная награда равна
\begin{equation*}
\sum_{t \ge 0} \gamma^t r_t + \sum_{t \ge 0} \left[ \gamma^{t+1} \Phi(s_{t+1}) - \gamma^{t} \Phi(s_t) \right]
\end{equation*}

В силу свойства телескопирующей суммы \eqref{telescopingsum}, все слагаемые во второй сумме посокращаются, кроме первого слагаемого $-\Phi(s_0)$, который не зависит от стратегии взаимодействия и поэтому не влияет на итоговую задачу оптимизации, <<последнего слагаемого>>, которое есть ноль: действительно, $\lim\limits_{t \to \infty} \gamma^{t} \Phi(s_t) \HM= 0$ в силу ограниченности функции потенциала. Если же в игре конечное число шагов $T$, не сокращающееся слагаемое $\Phi(s_T)$ есть значение потенциала в терминальном состоянии $s_T$, которая по условию также равно нулю.
\end{proof}
\end{theorem}

Reward shaping позволяет поменять награду, <<не ломая>> задачу. Это может быть способом борьбы с разреженной функцией награды, если удаётся придумать какой-нибудь хороший потенциал. Конечно, для этого нужно что-то знать о самой задаче, и на практике reward shaping --- это инструмент внесения каких-то априорных знаний, дизайна функции награды. Тем не менее, в будущем в главе \ref{PIsection} мы рассмотрим один хороший универсальный потенциал, который будет работать всегда.

\subsection{Бенчмарки}

Для тестирования RL алгоритмов есть несколько распространившихся бенчмарков. Неистощаемым источником тестов для обучения с подкреплением с дискретным пространством действий являются видеоигры, где, помимо прочего, уже задана функция награды --- счёт игры.

\begin{example}[Игры Atari]
Atari --- набор из 57 игр с дискретным пространством действий. Наблюдением является экран видео-игры (изображение), у агента имеется до 18 действий (в некоторых играх действия, соответствующие бездействующим кнопкам джойстика, по умолчанию убраны). Награда --- счёт в игре. \href{https://gym.openai.com/envs/#atari}{Визуализация игр из OpenAI Gym}.

\needspace{12\baselineskip}
\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\vspace{-0.3cm}
\includegraphics[width=0.4\textwidth]{Images/Atari.png}
\vspace{-0.8cm}
\end{wrapfigure}

При запуске алгоритмов на Atari обычно используется препроцессинг. В общем случае MDP, заданный исходной игрой, не является полностью наблюдаемым: например, в одной из самых простых игр \href{https://gym.openai.com/envs/Pong-v0/}{Pong} текущего экрана недостаточно, чтобы понять, в какую сторону летит шарик. Для борьбы с этим состоянием считают последние 4 кадра игры (\emph{frame stack}), что для большинства игр достаточно.

Чтобы агент не имел возможности менять действие сильно чаще человека, применяют \emph{frame skip} --- агент выбирает следующее действие не каждый кадр, а, скажем, раз в 4 кадра. В течение этих 4 кадров агент нажимает одну и ту же комбинацию кнопок. Если агент <<видит>> из-за этого только кратные кадры, могут встретиться неожиданные последствия: например, в Space Invaders каждые 4 кадра <<исчезают>> все выстрелы на экране, и агент может перестать их видеть.

\begin{remark}
Обычно добавляется и препроцессинг самого входного изображения --- в оригинале оно сжимается до размера 84х84 и переводится в чёрно-белое.
\end{remark}

\begin{remark}
В играх часто встречается понятие <<жизней>>. Полезно считать, что потеря жизни означает конец эпизода, и выдавать в этот момент алгоритму флаг $\done$. 
\end{remark}

Функция переходов в Atari --- детерминированная; рандомизировано только начальное состояние. Есть опасения, что это может приводить к <<запоминанию>> хороших траекторий, поэтому распространено использование \emph{sticky actions} --- текущее выбранное действие повторяется для $k$ кадров, где $k$ определяется случайно (например, действие повторяется только 2 раза, затем для очередного кадра подбрасывается монетка, и с вероятностью 0.5 агент повторяет действие снова; монетка подбрасывается снова, и так далее, пока не выпадет останов).

Для оценки алгоритмов используется \emph{Human normalized score}: пусть $\mathrm{agentScore}$ --- полученная оценка $J(\pi)$ для обучившегося агента, $\mathrm{randomScore}$ --- случайной стратегии, $\mathrm{humanScore}$ --- средний результат человека, тогда Human normalized score равен
$$\frac{\mathrm{agentScore} - \mathrm{randomScore}}{\mathrm{humanScore} - \mathrm{randomScore}}.$$
Эта величина усредняется по 57 играм для получения качества алгоритма. При этом по условию бенчмарка алгоритм должен быть запущен на всех 57 играх с одними и теми же настройками и гиперпараметрами.
\end{example}

\begin{example}[Atari RAM]
Игры Atari представлены в ещё одной версии --- <<RAM-версии>>. Состоянием считается не изображение экрана, а 128 байт памяти Atari-консоли, в которой содержится вся информация, необходимая для расчёта игры (координаты игрока и иные параметры). По определению, такое состояние <<полностью наблюдаемое>>, и также может использоваться для тестирования алгоритмов.
\end{example}

Для задач непрерывного управления за тестовыми средами обращаются к физическим движкам и прочим симуляторам. Здесь нужно оговориться, что схожие задачи в разных физических движках могут оказаться довольно разными, в том числе по сложности для RL алгоритмов.

\begin{exampleBox}[label=ex:locomotion]{Locomotion}
Задача научить ходить какое-нибудь существо весьма разнообразна. Под <<существом>> понимается набор сочленений, каждое из которых оно способно <<напрягать>> или <<расслаблять>> (для каждого выдаётся вещественное число в диапазоне $[-1, 1]$). Состояние обычно описано положением и скоростью всех сочленений, то есть является небольшим компактным векторочком. 

\needspace{15\baselineskip}
\begin{wrapfigure}{r}{0.25\textwidth}
\centering
\vspace{-0.3cm}
\includegraphics[width=0.25\textwidth]{Images/Dancer.png}
\vspace{-0.8cm}
\end{wrapfigure}

Типичная задача заключается в том, чтобы в рамках представленной симуляции физики научиться добираться как можно дальше в двумерном или трёхмерном пространстве. Функция награды, описывающая такую задачу, не так проста: обычно она состоит из ряда слагаемых, дающих бонусы за продолжение движения, награду за скоррелированность вектора скорости центра масс с желаемым направлением движения и штрафы за трату энергии.

\begin{remark}
Такая награда, хоть и является довольно плотной, обычно <<плохо отнормирована>>: суммарное значение за эпизод может быть довольно высоким. Нормировать награду <<автоматически>> могут в ходе самого обучения, считая, например, средний разброс встречаемых наград за шаг и деля награду на посчитанное стандартное отклонение. Распространено считать разброс не наград за шаг, а суммарных наград с начала эпизода, и делить награды за шаг на их посчитанное стандартное отклонение.
\end{remark}

Несмотря на малую размерность пространства состояний и действий (случаи, когда нужно выдавать в качестве действия векторы размерности порядка 20, уже считаются достаточно тяжёлыми), а также информативную функцию награды, дающую постоянную обратную связь агенту, подобные задачи непрерывного управления обычно являются довольно сложными.
\end{exampleBox}

\begin{example}
В робототехнике и симуляциях робот может получать информацию об окружающем мире как с камеры, наблюдая картинку перед собой, так и узнавать о располагающихся вокруг объектах с помощью разного рода сенсоров, например, при помощи \emph{ray cast}-ов --- расстояния до препятствия вдоль некоторого направления, возможно, с указанием типа объекта. Преимущество последнего представления перед видеокамерой в компактности входного описания (роботу не нужно учиться обрабатывать входное изображение). В любом случае, входная информация редко когда полностью описывает состояние всего окружающего мира, и в подобных реальных задачах требуется формализм частично наблюдаемых сред. 
\end{example}
