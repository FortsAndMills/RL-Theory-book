\section{Схемы <<Актёр-критик>>}\label{ActorCriticSection}

\subsection{Введение критика}

Мы хотели научиться оптимизировать параметры стратегии при помощи формулы градиента, не доигрывая эпизоды до конца. Мы уже поняли, что мат.ожидание по траекториям не представляет для нас проблемы. Тогда осталось лишь придумать, как, не доигрывая эпизоды до конца, проводить credit assignment, то есть определять для каждой пары $s, a$ оценку Advantage-функции.

Раз знание функции $Q^\pi$ позволит обучаться, не доигрывая эпизоды до конца, а функцию в готовом виде нам никто не даст, то возникает логичная идея --- аппроксимировать её. Итак, введём вторую сетку, которая будет <<оценивать>> наши собственные решения --- \emph{критика} (critic). Нейросеть, моделирующую стратегию, соответственно будем называть \emph{актёром} (actor), и такие алгоритмы, в которых обучается как модель критика, так и модель актёра, называются \emph{Actor-Critic}.

Здесь возникает принципиальный момент: мы не умеем обучать модели оценочных функций $Q^\pi$ или $V^\pi$, так чтобы они оценивали будущую награду несмещённо (<<выдавали в среднем правильный ответ>>). Что это влечёт? При смещённой оценке Q-функции любые гарантии на несмещённость градиентов мгновенно теряются. Единственный способ оценивать Q-функцию несмещённо --- Монте-Карло, но она требует полных эпизодов и имеет более высокую дисперсию. Любое замешивание нейросетевой аппроксимации в оценку --- и мы сразу теряем несмещённость оценок на градиент, и вместе с ними --- любые гарантии на сходимость стохастической оптимизации к локальному оптимуму или даже вообще хоть куда-нибудь!

В машинном обучении в любых задачах оптимизации всегда необходимо оценивать градиент оптимизируемого функционала несмещённо, и важной необычной особенностью Policy Gradient методов в RL является тот факт, что в них внезапно используются именно смещённые оценки градиента. Надежда на работоспособность алгоритма после замены значения $Q^\pi(s, a)$ на смещённую оценку связана с тем, что формула градиента говорит проводить policy improvement во встречаемых состояниях (см. физический смысл суррогатной функции \eqref{PGisPI}). Коли градиент есть просто policy improvement, то мы знаем из общей идеи алгоритма \ref{generalizedpolicyiteration} Generalized Policy Iteration, что для улучшения политики вовсе не обязательно использовать идеальную $Q^\pi$, достаточно любой аппроксимации критика $Q(s, a) \HM \approx Q^\pi(s, a)$, которая параллельным процессом движется в сторону идеальной $Q^\pi$ для текущей стратегии $\pi$. Фактически, все Actor-Critic методы моделируют именно эту идею.

Следующий существенный момент заключается в том, что в качестве критика обычно учат именно V-функцию. Во-первых, если бы мы учили модель Q-функции и подставили бы её, то градиент
$$\nabla_\theta \log \pi_\theta(a \mid s) Q(s, a)$$
направил бы нашу стратегию в $\argmax\limits_a Q(s, a)$. Помимо того, что это выродило бы стратегию, это бы привело к тому, что качество обучения актёра выродилось бы в качество обучения критика: пока модель $Q(s, a)$ не похожа на истинную $Q^\pi(s, a)$, у нас нет надежды, что актёр выучит хорошую стратегию. 

% Конечно, мы могли бы учить и $Q^{\pi}$ в качестве критика. Для бэйзлайна нам нужно $V^{\pi}(s)$; если у нас есть Q-критик и пространство действий дискретно, то мы можем посчитать $V^\pi$ по формуле \eqref{VQ}. Если пространство действий непрерывно, то возникает затруднение: если бэйзлайн тоже оценивать по Монте-Карло, то от него скорее всего будет не так много толку, и придётся либо вводить третью сетку, учащую $V^\pi(s) \HM= \E_{a \sim \pi(a \mid s)} Q^\pi(s, a)$, либо хотя бы оценивать по Монте-Карло с большим количеством сэмплов, что может быть дороговато. Поскольку без бэйзлайна никуда, это может быть ещё одним аргументом в пользу V-критика. 

\begin{remark}
Можно ли мы в качестве критика напрямую учить сетку, выдающую аппроксимацию $A_{\phi}(s, a) \HM\approx A^\pi(s, a)$, и использовать её выход в качестве нашей оценки $\Psi (s, a)$? Это не очень удобно хотя бы потому, что в отличие от Q-функций и V-функций, advantage --- не абстрактная произвольная функция: она должна подчиняться теореме \ref{pr:advantageiszero}. При этом восстановить по advantage-у Q-функцию без знания V-функции нельзя, а значит, для advantage не получится записать аналога уравнения Беллмана, использующего только advantage-функции.
\end{remark}

Возможность не обучать сложную $Q^*$ является одним из преимуществ подхода прямой оптимизации $J(\theta)$. В DQN было обязательным учить именно Q-функцию, так как, во-первых, мы хотели выводить из неё оптимальную стратегию, во-вторых, для уравнения оптимальности Беллмана для оптимальной V-функции фокус с регрессией не прокатил бы --- там мат.ожидание стоит внутри оператора взятия максимума. Сейчас же мы из соображений эффективности алгоритма (желания не играть полные эпизоды и задачи снижения дисперсии оценок) не сможем обойтись совсем без обучения каких-либо оценочных функций, но нам хватит лишь $V_{\phi}(s) \HM\approx V^{\pi}$, поскольку оценить Q-функцию мы можем хотя бы так:
\begin{equation}\label{Qonlineonestep}
Q^\pi(s, a) = r(s, a) + \gamma \E_{s'} V^\pi(s') \approx r(s, a) + \gamma V_{\phi}(s'), \qquad s' \sim p(s' \mid s, a)
\end{equation}
Иначе говоря, если у нас есть приближение V-функции, то мы можем использовать её как для бэйзлайна, так и для оценки Q-функции. Важно, что V-функция намного проще, чем Q-функция: ей не нужно дифференцировать между действиями, достаточно лишь понимать, какие области пространства состояний --- хорошие, а какие плохие. И поэтому раз можно обойтись ей, то почему бы так не сделать и не упростить критику задачу.

\subsection{Bias-variance trade-off}

Обсудим сначала, как критик $V_{\phi}(s) \HM\approx V^\pi(s)$ с параметрами $\phi$ будет использоваться в формуле градиента по параметрам стратегии \eqref{advantagepg}. Мы собираемся вместо честного advantage подставить некоторую его оценку (advantage estimator) и провести таким образом credit assingment:
\begin{equation}\label{advantageestimation}
\nabla_{\theta} J(\pi) \approx \frac{1}{1 - \gamma}\E_{d_\pi(s)} \E_{a \sim \pi(a \mid s)} \nabla_{\theta} \log \pi_\theta (a \mid s) \underbrace{\Psi (s, a)}_{\approx A^{\pi}(s, a)}
\end{equation}

Мы столкнулись ровно с тем же самым bias-variance trade-off, который мы обсуждали \ref{sec:biasvar}, который как раз и сводится к оцениванию Advantage и определению того, какие действия хорошие или плохие. Только теперь, в контексте policy gradient, речь напрямую идёт о дисперсии и смещении оценок градиента. Мы уже встречались с двумя самыми <<крайними>> вариантами выбора функции $\Psi (s, a)$: Монте-Карло и одношаговая оценка через V-функцию \eqref{Qonlineonestep}:

\vspace{0.2cm}
\begin{center}
\begin{tabular}{ccc}
\toprule
    \textbf{$\Psi (s, a)$} & \textbf{Дисперсия} & \textbf{Смещение}  \\
\midrule
    $R_t - V_{\phi}(s)$ & высокая & нету \\
    \hdashline
    $r(s, a) + \gamma V_{\phi}(s') - V_{\phi}(s)$ & низкая & большое \\
\bottomrule
\end{tabular}
\end{center}
\vspace{0.2cm}

В этой таблице речь идёт именно о дисперсии и смещении оценки градиента $J(\pi)$ при использованной аппроксимации! А то есть, в первом случае оценка Q-функции несмещённая, оценка V-функции смещённая, но поскольку в качестве бэйзлайна в силу \eqref{baseline} может использоваться совершенно произвольная функция от состояний, совершенно несущественно, насколько наша аппроксимация $V^\pi(s)$ вообще похожа на истинную оценочную функцию. Да, если наша аппроксимация V-функции будет неточной, мы, вероятно, не так сильно собьём дисперсию оценок градиента, как могли бы, но ровно на этом недостатки использования смещённой аппроксимации V-функции в качестве бэйзлайна заканчиваются. 

Во втором же случае, аппроксимация $V^\pi(s')$ используется для оценки Q-функции и вызывает смещение в том числе в оценке градиента. Дисперсия же снижается, поскольку в Q-функции аккумулированы мат.ожидания по всему хвосту траектории; она в обоих случаях существенно ниже, чем до введения бэйзлайна, но замена Монте-Карло оценки на бутстрапированную оценку снижает её ещё сильнее.

Вспомним, как можно интерполировать между этими крайними вариантами. Для начала, у нас есть ещё целое семейство промежуточных вариантов --- многошаговых (multi-step) оценок Q-функции, использующих N-шаговое уравнение Беллмана \eqref{NstepBellman}:
$$Q^\pi(s, a) \approx \sum_{t=0}^{N-1} \gamma^{t} r^{(t)} + \gamma^N V_{\phi}(s^{(N)}) $$

Тогда мы пользуемся для credit assingment-а $N$-шаговой оценкой Advantage, или $N$-шаговой временной разностью \eqref{Nstepadvantage}.
$$\Psi_{(N)}(s, a) \coloneqq \sum_{t=0}^{N-1} \gamma^{t} r^{(t)} + \gamma^N V_{\phi}(s^{(N)}) - V_{\phi}(s)$$
С ростом $N$ дисперсия такой оценки увеличивается: всё больший фрагмент траектории мы оцениваем по Монте-Карло, нам становятся нужны сэмплы $a_{t+1} \sim \pi(a_{t+1} \mid s_{t=1})$, $s_{t+2} \sim \pi(s_{t+2} \mid s_{t+1}, a_{t+1})$, \dots , $s_{t+N} \sim \pi(s_{t+N} \mid s_{t+N-1}, a_{t+N-1})$. а при $N \to \infty$ оценка в пределе переходит в полную Монте-Карло оценку оставшейся награды, где дисперсия большая, но зато исчезает смещение в силу отсутствия смещённой аппроксимации награды за хвост траектории. 

Также с ростом $N$ мы начинаем всё меньше опираться на модель критика. Используя сэмплы наград, мы ценой увеличения дисперсии напрямую учим связь между хорошими действиями и высоким откликом от среды, меньше опираясь на промежуточный этап в виде выучивания оценочной функции. Это значит, что нам не нужно будет дожидаться, пока наш критик идеально обучиться: в оценку Advantage попадает сигнал в том числе из далёкого будущего при больших $N$, и актёр поймёт, что удачно совершённое действие надо совершать чаще. Математически это можно объяснить тем, что, увеличивая $N$, мы снижаем смещение наших оценок градиента. Значит, нам в целом даже и не потребуется, чтобы критик оценивал состояния с высокой точностью, поскольку главное, чтобы в итоге получалась более-менее адекватная оценка совершённых нашей стратегией действий.

Trade-off заключается в том, что чем дальше в будущее мы заглядываем, тем выше дисперсия этих оценок; помимо этого, для заглядывания в будущее на $N$ шагов нужно же иметь это самое будущее, то есть из каждой параллельно запущенной среды понадобится собрать для очередного мини-батча не по одному сэмплу, а собрать целый длинный фрагмент траектории. Поэтому, регулируя длину оценок, мы <<размениваем>> смещение на дисперсию, и истина, как всегда, где-то посередине.

Возможность разрешать bias-variance trade-off и выбирать какую-то оценку с промежуточным смещением и дисперсией --- чуть ли не главное преимущество on-policy режима обучения. Напомним, что сэмплы фрагментов траекторий из буфера получить не удастся (действия должны генерироваться из текущей стратегии), и использование многошаговых оценок в off-policy режиме было невозможно.

Пока что в нашем алгоритме роллауты непрактично делать сильно длинными. Дело в том, что пары $s, a$ из длинных роллаутов будут сильно скоррелированы, что потребуется перебивать числом параллельно запущенных сред, а тогда при увеличении длины роллаута начинает раздуваться размер мини-батча. Потом собранные переходы будут использованы всего для одного шага градиентного подъёма, и переиспользовать их будет нельзя; это расточительно и поэтому большой размер мини-батча невыгоден. Поэтому пока можно условно сказать, что самая <<выгодная>> оценка Advantage-а для не очень длинных роллаутов --- оценка максимальной длины: для $s_t$ мы можем построить $N$-шаговую оценку, её и возьмём; для $s_{t+1}$ уже не более чем $N-1$-шаговую; наконец, для $s_{t+N-1}$ нам доступна лишь одношаговая оценка, просто потому что никаких сэмплов после $s_{t+N}$ мы ещё не получали. 

\begin{definition}
Для пар $s_t, a_t$ из роллаута $s_0, a_0, r_0, s_1, a_1, r_1, \dots, s_N$ длины $N$ будем называть \emph{оценкой максимальной длины} (max trace estimation) оценку с максимальным заглядыванием в будущее: для Q-функции
\begin{equation}\label{maxtrace}
y^{\mathrm{MaxTrace}} (s_t, a_t) \coloneqq \sum_{\hat{t}=t}^{N-1} \gamma^{\hat{t}-t} r_{\hat{t}} + \gamma^{N-t} V^\pi(s_N),
\end{equation}
для Advantage функции, соответственно:
\begin{equation*}\label{maxtraceadvantage}
\Psi^{\mathrm{MaxTrace}} (s_t, a_t) \coloneqq y^{\mathrm{MaxTrace}} (s_t, a_t) - V^\pi(s_t)
\end{equation*}
\end{definition}

Заметим, что мы вовсе не обязаны использовать для всех пар $s, a$ оценку одной и той же длины $N$. То есть мы не должны брать для $s_t, a_t$ из $N$-шагового роллаута $N$-шаговую оценку, а остальные пары $s, a$ из роллаута не использовать в обучении лишь потому, что для них $N$-шаговая оценка невозможна; вместо этого для них следует просто использовать ту многошаговую оценку, которая доступна. Это полностью корректно, поскольку любая $N$-шаговая оценка является оценкой Advantage-а для данного слагаемого в нашей формуле градиента. Именно это и говорит оценка максимальной длины: если мы собрали роллаут $s_0, a_0, s_1, a_1 \dots s_5$ длины 5, то одну пару $s_4, a_4$, самую последнюю, нам придётся оценить одношаговой оценкой (поскольку для неё известен сэмпл лишь следующего состояния $s_5$ и только, никаких альтернатив здесь придумать не получится), предпоследнюю пару $s_3, a_3$ --- двухшаговой, и так далее. То есть <<в среднем>> длина оценок будет очень маленькой, такая оценка с точки зрения bias-variance скорее смещена, чем имеет большую дисперсию. 

\subsection{Generalized Advantage Estimation (GAE)}

Пока $N$ не так велико, чтобы дисперсия раздувалась, оценка максимальной длины --- самое разумное решение trade-off-а, и о более умном решении думать не нужно. Однако, впоследствии мы столкнёмся с ситуацией, что on-policy алгоритмы будут собирать достаточно длинные роллауты (порядка тысячи шагов), и тогда брать оценку наибольшей длины уже будет неразумно; с этой же проблемой можно столкнуться и в контексте обсуждаемой Actor-Critic схемы, если длина собираемых роллаутов достаточно большая.

Решение дилеммы bias-variance trade-off подсказывает теория TD($\lambda$) оценки из главы \ref{sec:biasvar}. Нужно применить формулу \eqref{TDlambda} и просто заансамблировать $N$-шаговые оценки разной длины:

\begin{definition}
\emph{GAE-оценкой} Advantage-функции называется ансамбль многошаговых оценок, где оценка длины $N$ \eqref{Nstepadvantage} берётся с весом $\lambda^{N-1}$, где $\lambda \in (0, 1)$ --- гиперпараметр:
\begin{equation}\label{GAEadest}
\Psi_{\mathrm{GAE}}(s, a) \coloneqq (1 - \lambda) \sum_{N > 0} \lambda^{N-1} \Psi_{(N)}(s, a)
\end{equation}
\end{definition}

Как мы помним, при $\lambda \to 0$ такая GAE-оценка соответствует одношаговой оценке; при $\lambda = 1$ GAE-оценка соответствует Монте-Карло оценке Q-функции, которой мы фактически воспользовались, например, в REINFORCE.

\begin{remark}
Как и при дисконтировании, геометрическая прогрессия затухает очень быстро; это означает, что $\lambda \approx 0.9$ больше предпочитает короткие оценки длинным. Поэтому часто $\lambda$ всё-таки близка к 1, типичное значение --- 0.95.
\end{remark}

В текущем виде в формуле суммируются все $N$-шаговые оценки вплоть до конца эпизода. В реальности собранные роллауты могут прерваться в середине эпизода: допустим, для данной пары $s, a$ через $M$ шагов роллаут <<обрывается>>. Тогда на практике используется чуть-чуть другим определением GAE-оценки: если мы знаем $s^{(M)}$, но после этого эпизод ещё не доигран до конца, мы пользуемся формулой \eqref{TDlambda} и оставляем от суммы только <<доступные>> слагаемые:
\begin{equation}\label{truncatedGAE}
\Psi_{\mathrm{GAE}}(s, a) \coloneqq \sum_{t \ge 0}^{M - 1} \gamma^t \lambda^t \Psi_{(1)}(s^{(t)}, a^{(t)})
\end{equation}
Напомним, что это корректно, поскольку соответствует просто занулению следа на $M$-ом шаге, или, что тоже самое, ансамблированию (взятию выпуклой комбинации) первых $M$ многошаговых оценок, где веса <<пропавших>> слишком длинных оценок просто перекладываются в вес самой длинной доступной $M$-шаговой оценки:

\begin{proposition}
Формула \eqref{truncatedGAE} эквивалентна следующему ансамблю $N$-шаговых оценок:
$$\Psi_{\mathrm{GAE}}(s, a) = (1 - \lambda) \sum_{N > 0}^{M-1} \lambda^{N-1} \Psi_{(N)}(s, a) + \lambda^{M-1} \Psi_{(M)}(s, a)$$
\begin{proof}
Следует из доказательства теоремы \ref{th:tdlambda}.
\end{proof}
\end{proposition}

В такой <<обрезанной>> оценке $\lambda \HM= 1$ соответствует оценке максимальной длины \eqref{maxtraceadvantage}, а $\lambda \HM= 0$ всё ещё даст одношаговую оценку.

В коде формула \eqref{truncatedGAE} очень удобна для рекурсивного подсчёта оценки; также для практического алгоритма осталось учесть флаги $\done_t$. Формулы подсчёта GAE-оценки для всех пар $(s, a)$ из роллаута $s_0, a_0, r_0, s_1, a_1, r_1, \cdots s_N$ приобретают такой вид:
\begin{equation*}
\begin{split}
\Psi_{\mathrm{GAE}}(s_{N-1}, a_{N-1}) &\coloneqq \Psi_{(1)}(s_{N-1}, a_{N-1}) \\
\Psi_{\mathrm{GAE}}(s_{N-2}, a_{N-2}) &\coloneqq \Psi_{(1)}(s_{N-2}, a_{N-2}) + \gamma \lambda (1 - \done_{N-2}) \Psi_{\mathrm{GAE}}(s_{N-1}, a_{N-1}) \\
&\vdots \\
\Psi_{\mathrm{GAE}}(s_0, a_0) &\coloneqq \Psi_{(1)}(s_0, a_0) + \gamma \lambda (1 - \done_0) \Psi_{\mathrm{GAE}}(s_1, a_1)
\end{split}
\end{equation*}

Заметим, что эти формулы очень похожи на расчёт кумулятивной награды за эпизод, где <<наградой за шаг>> выступает $\Psi_{(1)}(s, a)$. В среднем наши оценки Advantage должны быть равны нулю, и, если наша аппроксимация этому не удовлетворяет, мы получаем направление корректировки стратегии.

\begin{remark}
Теория говорит, что любые off-policy алгоритмы должны быть более эффективны в плане числа использованных сэмплов, чем on-policy, но на практике может оказаться так, что продвинутые policy gradient алгоритмы, которые мы обсудим позднее, обойдут DQN-подобные алгоритмы из главы \ref{valuebasedchapter} и по sample efficiency. Вероятно, это происходит в ситуациях, когда DQN страдает от проблемы распространения сигнала. При использовании GAE замешивание далёких будущих наград в таргеты позволяет справляться с этой проблемой; если награда разреженная, то имеет смысл выставлять $\lambda$ ближе к единице, если плотная --- ближе к, допустим, 0.9. Распространённый дефолтный вариант 0.95 можно рассматривать как отчасти <<универсальный>>. Если же награда информативная, на каждом шаге поступает какой-то информативный сигнал, то осмыслены и одношаговые обновления; и в таких случаях off-policy алгоритмы на основе value-based подхода скорее всего окажутся более эффективными.
\end{remark}

\subsection{Обучение критика}

Как обучать критика $V_{\phi}(s) \HM\approx V^\pi$? Воспользуемся идеей перехода к регрессии, которую мы обсуждали раньше в контексте DQN (раздел \ref{toregression}). Нам нужно просто решать методом простой итерации уравнение Беллмана \eqref{VV}:
$$V_{\phi_{k+1}}(s) \leftarrow \E_{a} \left[ r + \gamma \E_{s'} V_{\phi_k}(s') \right]$$

Мы можем получить несмещённую оценку правой части $y \HM \coloneqq r \HM+ \gamma V_{\phi_k}(s')$ и с таким таргетом минимизировать MSE. Однако, давайте воспользуемся преимуществами on-policy режима и поймём, что мы можем поступить точно также, как с оценкой Q-функции в формуле градиента: решать многошаговое уравнение Беллмана вместо одношагового. Конечно же, это снова всё тот же самый bias-variance trade-off, и ключевое преимущество on-policy подхода --- разрешать его в том числе при обучении критика. 

В чём заключается bias-variance trade-off при обучении критика? С ростом $N$ таргет в задаче регрессии становится всё более и более шумным, зато мы быстрее распространяем сигнал и меньше опираемся в таргете на свою же собственную аппроксимацию. Это позволяет бороться с проблемой накапливающейся ошибки, от которой страдают off-policy алгоритмы вроде DQN. В пределе --- при $N \HM= +\infty$ --- мы решаем задачу регрессии, где целевая переменная есть reward-to-go, и начинаем учить V-функцию просто по определению \eqref{Vdefinition}. Такая задача регрессии уже является самой обычной задачей регрессии из машинного обучения, целевая переменная будет являться <<ground truth>>: именно теми значениями, среднее которых мы и хотим выучить. Но такая задача регрессии будет обладать очень шумными целевыми переменными, плюс для сбора таких данных понадобится, опять же, полные эпизоды играть.

Мы можем для оценки Q-функции для обучения политики и для построения целевой переменной для критика использовать разные подходы (скажем, оценки разной длины), но особого смысла в этом немного: хороший вариант для одного будет хорошим вариантом и для другого. Соответственно, можно считать решение trade-off одинаковым для актёра и критика. Тогда если мы оцениваем Advantage как
$$\Psi(s, a) = y - V_{\phi}(s),$$
где $y$ --- некоторая оценка Q-функции, то $y$ же является и таргетом для V-функции, и наоборот. Используя функцию потерь MSE с таким таргетом, мы как раз и учим среднее значение наших оценок Q-функции, то есть бэйзлайн.

Конечно же, мы можем использовать и GAE-оценку \eqref{truncatedGAE} Advantage, достаточно <<убрать бэйзлайн>>:
$$Q^\pi(s, a) = A^\pi(s, a) + V^\pi(s) \approx \Psi_{\mathrm{GAE}}(s, a) + V_{\phi}(s)$$
При этом мы как бы будем решать <<заансамблированные>> N-шаговые уравнения Беллмана для V-функции: 

\begin{proposition}
Таргет $\Psi_{\mathrm{GAE}}(s, a) \HM+ V_{\phi}(s)$ является несмещённой оценкой правой части <<ансамбля>> уравнений Беллмана:
$$V_{\phi}(s) = (1 - \lambda) \sum_{N > 0} \lambda^{N-1} \left[ \B^N V_\phi \right] (s),$$
где $\B$ --- оператор Беллмана для V-функции \eqref{bellmanoperatorV}.
\begin{proof}
По определению, поскольку $\Psi_{(N)}(s, a) \HM+ V(s)$ является несмещённой оценкой правой части $N$-шагового уравнения Беллмана (т.~е. несмещённой оценкой $\left[ \B^N V^\pi \right] (s)$), а
$$(1 - \lambda) \sum_{N > 0} \lambda^{N-1} (\Psi_{(N)}(s, a) + V(s)) = \Psi_{\mathrm{GAE}}(s, a) + V(s)$$
по определению \eqref{GAEadest} GAE-оценки.
\end{proof}
\end{proposition}


% Итак, для каждого варианта напишем, что у нас является входом, что искомой функцией (это всегда правая часть соответствующего уравнения Беллмана, которое мы хотим решить методом простой итерации), что целевой переменной (она должна быть вычислима и являться несмещённой оценкой какого-либо мат.ожидания, стоящего в искомой функции) и от каких случайных величин она зависит (это ключевой фактор, так как от этого зависит, сможем ли мы, например, использовать реплей буфер). Функция потерь во всех случаях должна учить мат.ожидания, поэтому MSE.

% \vspace{0.2cm}
% \begin{center}
% \begin{tabular}{ccccc}
% \toprule
%     \textbf{Что учим?} & \textbf{Вход} & \textbf{Искомая функция} & \textbf{Требуемые сэмплы} & \textbf{Целевая переменная}  \\
% \midrule
%     $V^{\pi}(s)$ & $s$ & $\E_{a} \left[ r + \gamma \E_{s'} V^\pi(s', \omega_k) \right]$ & \begin{tabular}{@{}c@{}}$a \sim \pi(a \mid s)$ \\ $s' \sim p(s' \mid s, a)$\end{tabular} & $r + \gamma V^\pi(s', \omega_k)$ \\
%     \hdashline
%     $Q^{\pi}(s, a)$ & $s, a$ & $ r + \gamma \E_{s'} \E_{a'} Q^\pi(s', a', \omega_k)$ & \begin{tabular}{@{}c@{}}$s' \sim p(s' \mid s, a)$ \\ $a' \sim \pi(a' \mid s')$\end{tabular} & $r + \gamma Q^\pi(s', a', \omega_k)$ \\
%     \hdashline
%     $Q^{\pi}(s, a)$ & $s, a$ & $ r + \gamma \E_{s'} \E_{a'} Q^\pi(s', a', \omega_k)$ & $s' \sim p(s' \mid s, a)$ & $r + \gamma \E_{a'} Q^\pi(s', a', \omega_k)$ \\
% \bottomrule
% \end{tabular}
% \end{center}
% \vspace{0.2cm}

% В последнем случае пространство действий должно быть дискретно, чтобы в целевой переменной взялось $\E_{a'}$, то есть выбор между вторым и третьим вариантом определяется только этим.

Брать для обучения критика набор выходных состояний $s$ мы можем откуда угодно. Поэтому для удобства будем брать $s \HM\sim \mu_\pi(s)$, чтобы можно было использовать для обучения критика и актёра один и тот же мини-батч. Итого получаем следующее: делаем несколько шагов взаимодействия со средой, собирая таким образом роллаут некоторой длины $N$; считаем для каждой пары $s, a$ некоторую оценку Q-функции $y(s, a)$, например, оценку максимальной длины \eqref{maxtrace}; оцениваем Advantage каждой пары как $\Psi(s, a) \HM \coloneqq y(s, a) \HM- V_{\phi}(s)$; далее по Монте-Карло оцениваем градиент по параметрам стратегии
$$\nabla_\theta J(\pi) \approx \frac{1}{N} \sum_{s, a} \nabla_{\theta} \log \pi_\theta (a \mid s) \Psi(s, a)$$
и градиент для оптимизации критика (допустим, критик --- Q-функция):
$$\Loss^{\critic}(\phi) = \frac{1}{N} \sum_{s, a} \left( y(s, a) - V_{\phi}(s) \right)^2$$
Естественно, для декорреляции нужно собрать несколько независимых роллаутов из параллельно запущенных сред.

\begin{remark}
Когда состояния представлены в виде картинок, понятно, что и критику, и актёру нужны примерно одни и те же фичи с изображений (положение одних и тех же распознанных объектов). Поэтому кажется, что если критик и актёр будут двумя разными сетками, их первые слои будут обучаться одному и тому же. Логично для ускорения обучения объединить \emph{экстрактор фич} (feature extractor) для критика и актёра и сделать им просто свои индивидуальные головы. Конечно, тогда нужно обучать всю сеть синхронно, но мы специально для этого считаем градиенты для актёра и критика по одному и тому же мини-батчу. Есть у такого \emph{общего позвоночника} (shared backbone) и свои минусы: лоссы придётся отмасштабировать при помощи скалярного гиперпараметра $\alpha$ так, чтобы одна из голов не забивала градиентами другую:
$$\Loss^{\mathrm{ActorCritic}}(\theta) = \Loss^{\actor}(\theta) + \alpha \Loss^{\critic}(\theta)$$
\end{remark}

\subsection{Advantage Actor-Critic (A2C)}

Мы собрали стандартную схему Advantage Actor Critic (A2C): алгоритма, который работает в чистом виде on-policy режиме. Из-за того, что роллауты в этом алгоритме не очень длинные, используются оценки максимальной длины (или, что тоже самое, GAE с $\lambda \HM= 1$). 

\begin{algorithm}{Advantage Actor-Critic (A2C)}
\textbf{Гиперпараметры:} $M$ --- количество параллельных сред, $N$ --- длина роллаутов, $V_\phi(s)$ --- нейросеть с параметрами $\phi$, $\pi_\theta(a \mid s)$ --- нейросеть для стратегии с параметрами $\theta$, $\alpha$ --- коэф. масштабирования лосса критика, SGD оптимизатор.

\vspace{0.3cm}
Инициализировать $\theta, \phi$ \\
\textbf{На каждом шаге:}
\begin{enumerate}
    \item в каждой параллельной среде собрать роллаут длины $N$, используя стратегию $\pi_{\theta}$:
    $$s_0, a_0, r_0, s_1, \dots , s_N$$
    \item для каждой пары $s_t, a_t$ из каждого роллаута посчитать оценку Q-функции максимальной длины, игнорируя зависимость оценки от $\phi$:
    $$Q(s_t, a_t) \coloneqq \sum_{\hat{t} = t}^{N-1} \gamma^{\hat{t} - t} r_{\hat{t}} + \gamma^{N - t} V_{\phi}(s_N)$$
    \item вычислить лосс критика:
    $$\Loss^{\critic}(\phi) \coloneqq \frac{1}{MN}\sum_{s_t, a_t} \left( Q(s_t, a_t) - V_\phi(s_t) \right) ^2$$
    \item делаем шаг градиентного спуска по $\phi$, используя $\nabla_\phi \Loss^{\critic}(\phi)$
    \item вычислить градиент для актёра:
    $$\nabla^{\actor}_\theta \coloneqq \frac{1}{MN}\sum_{s_t, a_t} \nabla_\theta \log \pi_\theta(a_t \mid s_t) \left( Q(s_t, a_t) - V_\phi(s_t) \right) $$
    \item сделать шаг градиентного подъёма по $\theta$, используя $\nabla^{\actor}_\theta$
\end{enumerate}
\end{algorithm}

\begin{remark}
Для того, чтобы поощрить исследование среды, к суррогатной функции потерь актёра добавляют ещё одно слагаемое, <<регуляризатор>>, поощряющий высокую энтропию. Обычно, энтропию распределения $\pi_{\theta}(\cdot \mid s_t)$ для выбранной параметризации распределения можно посчитать аналитически и дифференцировать по $\theta$. Естественно, этот <<дополнительный лосс>> тоже нужно грамотно взвесить на скалярный коэффициент. Позже в разделе \ref{maximumentropyrlsubsection} мы встретимся с постановкой задачи Maximum Entropy RL, в рамках которой появление этого слагаемого можно объяснить небольшим изменением в самом оптимизируемом функционале.
\end{remark}

\begin{remark}
Известно, что Policy Gradient алгоритмы особенно чувствительны к инициализации нейросетей. Рекомендуют ортогональную инициализацию слоёв. Также крайне существенна обрезка градиентов, которая защищает от взрывов в градиентной оптимизации. При обучении policy gradient алгоритмов важно логгировать норму градиентов; большое значение нормы сигнализирует о неудачном подборе параметров.
\end{remark}

\begin{example}
Перерыв на \href{https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752}{чтение комиксов}.
\end{example}

Сравним A2C с value-based алгоритмами на основе DQN. Когда мы моделировали Value Iteration, мы целиком и полностью <<полагались>> на этап оценивания стратегии, то есть на обучение критика: модели актёра в явном виде даже не было в алгоритме, поскольку текущая стратегия всё время считалась жадной по отношению к текущему критику. Это означало, что качество стратегии упиралось в качество критика: пока Q-функция не научится адекватно приближать истинную $Q^*$, стратегия хорошей не будет. 

Достаточно интересно, что идея model-free алгоритмов, отказывающихся обучать модель функции переходов в том числе из соображений end-to-end схемы, пришла к тому, что мы иногда сводимся к обучению оценочных функций --- промежуточных величин, вместо того, чтобы напрямую матчить состояния и действия, понимать, какие действия стоит выбирать чаще других. Вообще, интуиция подсказывает, что обучать актёра проще, чем оценочную функцию\footnote{есть, однако, и ряд теоретических наблюдений, которые говорят, что эти задачи скорее эквивалентны по сложности. На это указывают формулы градиентов для обучения критика и актёра: так, для актёра мы идём по градиенту
$$\nabla_\theta \log \pi_\theta(a \mid s) \Psi(s, a),$$
а для критика мы идём по градиенту (продифференцируйте MSE, чтобы убедиться в этом):
$$\nabla_\phi V_\phi(s) \Psi(s, a)$$
Как видно, это как будто бы один и тот же градиент: он просто говорит увеличивать соответствующий выход нейронной сети, а кредит предоставляет скаляр, сообщающий масштаб изменения и, главное, знак.
}: в реальных задачах человек редко когда думает в терминах будущей награды. 

\begin{example}
Представьте, что вы стоите перед кофеваркой и у вас есть два действия: получить кофе и не получить. Вы прекрасно знаете, какое действие лучше другого, но при этом не оцениваете, сколько кофе вы сможете выпить в будущем при условии, например, что сейчас вы выберете первый или второй вариант: то есть не строите в явном виде прогноз значения оценочной функции.
\end{example}

И формула градиента, идея policy gradient методов, как раз предоставляет возможность обучать актёра напрямую, минуя промежуточный этап в виде критика: так, в алгоритме \ref{REINFORCE} REINFORCE мы могли использовать Монте-Карло оценки Q-функции и обходиться без обучения в явном виде модели критика, то есть полностью опираться на этап policy improvement-а. Поэтому policy gradient алгоритмы ещё иногда называют \emph{policy-based}. Таким образом, DQN и REINFORCE --- это два <<крайних случая>>, первый алгоритм полностью опирается на критика, а второй алгоритм --- на актёра. Только у первого недостатки компенсируются возможностью обучаться в off-policy режиме и использовать реплей буфер, а вот недостатки алгоритма REINFORCE --- высокая дисперсия и необходимость играть целые эпизоды --- не имеют аналогичного противовеса. 

Важно, что в Policy Gradient алгоритмах мы за счёт использования метода REINFORCE при расчёте градиента можем заменять значение Q-функции (необходимую для улучшения политики) на какую-то заглядывающую в будущее оценку. И насколько сильно при обучении актёра опираться на критика (<<насколько далеко в будущее заглядывать>>) --- это и есть bias-variance trade-off, который в on-policy алгоритмах возможно разрешать. За счёт этого A2C в отличие от DQN может использовать и в качестве таргета для обучения критика, и в качестве оценки для обучения Q-функции GAE-оценку \eqref{truncatedGAE}. Да, пока что в A2C роллауты (зачастую) получаются слишком короткими, и GAE ансамбль <<бедный>>, приходится $\lambda \HM= 1$ использовать, но главное, что есть такая возможность технически, и даже короткие роллауты уже позволяют существенно справиться с проблемой распространения сигнала: это помогает и критику лучше учиться, и актёр может не полностью на критика опираться. В частности, нам не нужна модель Q-функции, достаточно более простой V-функции. Всё это делает Policy Gradient алгоритмы куда более эффективными в средах с сильно отложенным сигналом.

Наконец, ещё одно небольшое, но важное преимущество Policy Gradient --- обучение стохастичной политики. Хотя мы знаем, что оптимальная политика детерминирована, обучение стохастичной политики позволяет использовать её для сбора данных, и стохастичность --- ненулевая вероятность засэмплировать любое действие --- частично решает проблему исследования. Да, это, конечно, далёкое от идеала решение, но намного лучшее, чем, например, $\eps$-жадная стратегия: можно рассчитывать на то, что если градиент в какой-то области пространства состояний постоянно указывает на то, что одни действия хорошие, а другие плохие, актёр выучит с вероятностью, близкой к единице, выбирать хорошие действия; если же о действиях в каких-то состояниях приходит противоречивая информация, или же такие состояния являются для агента новыми, то можно надеяться, что стратегия будет близка к равномерной, и агент будет пробовать все действия.

Все эти преимущества неразрывно связаны с on-policy режимом. За счёт <<свежести>> данных, можно делать, так сказать, наилучшие возможные шаги обучения стратегия, практически идти по градиенту оптимизируемого функционала. Но из него проистекает и главный недостаток подхода: неэффективность по количеству затрачиваемых сэмплов. Нам всё время нужны роллауты, сгенерированные при помощи текущей политики с параметрами $\theta$, чтобы посчитать оценку градиента в точке $\theta$ и сделать шаг оптимизации. После этого будут нужны уже сэмплы из новой стратегии, поэтому единственное, что мы можем сделать с уже собранными данными --- почистить оперативную память.

Понятно, что это полное безобразие: допустим, агент долго вёл себя псевдослучайно и наконец наткнулся на какой-то хороший исход с существенным откликом среды. Схема сделает по ценному роллауту всего один градиентный шаг, что скорее всего не поможет модели выучить удачное поведение или значение отклика. После этого ценная информация никак не может быть использована и придётся дожидаться следующей удачи, что может случиться нескоро. Это очень sample inefficient и основная причина, почему от любого on-policy обучения в чистом виде нужно пытаться отойти.

\begin{example}
Допустим, вы играете в видео-игру, и в начале обучения мало что умеете, всё время действуя примерно случайно и падая в первую же яму. Среда выдаёт вам всё время ноль, и вы продолжаете вести себя случайно. Вдруг в силу стохастичности стратегии вы перепрыгиваете первую яму и получаете монетку +1. В DQN этот ценнейший опыт будет сохранён в буфере, и постепенно критик выучит, какие действия привели к награде. В A2C же агент сделает один малюююсенький шаг изменения весов моделей и тут же выкинет все собранные данные в мусорку, потому что на следующих итерациях он никак не может переиспользовать их. Агенту придётся ждать ещё много-много сессий в самой игре, пока он не перепрыгнет яму снова, чтобы сделать следующий шаг обучения перепрыгиванию ям.
\end{example}

Частично с этой проблемой нам удастся побороться в следующей главе, что позволит построить алгоритмы лучше A2C. Но важно, что полностью решить эту проблему, полностью перейти в off-policy режим, нам не удастся точно также, как в off-policy мы не смогли адекватно разрешать bias-variance trade-off (и там максимум наших возможностей была Retrace-оценка). Поэтому построить алгоритм, берущий <<лучшее от двух миров>>, нельзя, и поэтому среди model-free алгоритмов выделяют два класса алгоритмов: off-policy, работающие с реплей буфером и потому потенциально более sample efficient, и on-policy алгоритмы, где есть ряд вышеуказанных преимуществ.