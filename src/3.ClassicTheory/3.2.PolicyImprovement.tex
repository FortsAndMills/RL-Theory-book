\section{Улучшение политики}\label{PIsection} 

\subsection{Advantage-функция}\label{subsection:advantage}

Допустим, мы находились в некотором состоянии $s$, и засэмплировали $a \HM\sim \pi(a \HM\mid s)$ такое, что $Q^\pi(s, a) \HM> V^\pi(s)$. Что можно сказать о таком действии? Мы знаем, что вообще в среднем политика $\pi$ набирает из данного состояния $V^\pi(s)$, но какой-то выбор действий даст в итоге награду больше $V^\pi(s)$, а какой-то меньше. Если $Q^\pi(s, a) \HM> V^\pi(s)$, то после того, как мы выбрали действие $a$, <<приняли решение>>, наша средняя будущая награда вдруг увеличилась.

Мы ранее обсуждали в разделе \ref{RLproblems} такую особую проблему обучения с подкреплением, как credit assingment, которая звучит примерно так: допустим, мы засэмплировали траекторию $s, a, s', a', \dots $ до конца эпизода, и в конце в финальном состоянии через $T$ шагов получили сигнал (награду) +1. Мы приняли $T$ решений, но какое из всех этих действий повлекло получение этого +1? <<За что нас наградили?>> Повлияло ли на получение +1 именно то действие $a$, которое мы засэмплировали в стартовом $s$? Вопрос нетривиальный, потому что в RL есть отложенный сигнал: возможно, именно действие $a$ в состоянии $s$ запустило какую-нибудь цепочку действий, которая дальше при любом выборе $a', a'', \cdots$ приводит к награде +1. Возможно, конечно, что первое действие и не имело никакого отношения к этой награде, и это поощрение именно за последний выбор. А ещё может быть такое, что имело место везение, и просто среда в какой-то момент перекинула нас в удачное состояние.

Но мы понимаем, что если какое-то действие <<затриггерило>> получение награды через сто шагов, в промежуточных состояниях будет информация о том, сколько времени осталось до получения этой отложенной награды. Например, если мы выстрелили во вражеский инопланетный корабль, и через 100 шагов выстрел попадает во врага, давая агенту +1, мы будем видеть в состояниях расстояние от летящего выстрела до цели, и знать, что через такое-то время нас ждёт +1. Другими словами, вся необходимая информация лежит в идеальных оценочных функциях $Q^\pi$ и $V^\pi$.

Так, если в некотором состоянии $s$ засэмплировалось такое $a$, что $Q^\pi(s, a) \HM= V^\pi(s)$, то мы можем заключить, что выбор действия на этом шаге не привёл ни к какой <<неожиданной>> награде. Если же $Q^\pi(s, a) \HM> V^\pi(s)$ --- то мы приняли удачное решение, $Q^\pi(s, a) \HM< V^\pi(s)$ --- менее удачное, чем обычно. Если, например, $r(s, a) \HM+ V^\pi(s') \HM> Q^\pi(s, a)$, то мы можем заключить, что имело место везение: среда засэмплировала такое $s'$, что теперь мы получим больше награды, чем ожидали после выбора $a$ в состоянии $s$. И так далее: мы сможем отследить, в какой конкретно момент случилось то событие (сэмплирование действия или ответ среды), за счёт которого получена награда.

Таким образом, идеальный <<кредит>> влияния действия $a$, выбранного в состоянии $s$, на будущую награду равен
$$Q^\pi(s, a) - V^\pi(s),$$
и именно эта величина на самом деле будет для нас ключевой. Поэтому из соображений удобства вводится ещё одно обозначение:

\begin{definition} 
Для данного MDP \emph{Advantage-функцией} политики $\pi$ называется
\begin{equation}\label{advantage}
A^\pi(s, a) \coloneqq Q^\pi(s, a) - V^\pi(s)
\end{equation}
\end{definition}

\begin{proposition}\label{pr:advantageiszero}
Для любой политики $\pi$ и любого состояния $s$:
$$\E_{\pi(a \mid s)} A^\pi(s, a) = 0$$
\beginproof
\begin{align*}
\E_{\pi(a \mid s)} A^\pi(s, a) &= \E_{\pi(a \mid s)} Q^\pi(s, a) - \E_{\pi(a \mid s)} V^\pi(s) = \\
\{ \text{$V^\pi$ не зависит от $a$} \} &= \E_{\pi(a \mid s)} Q^\pi(s, a) - V^\pi(s) = \\
\{ \text{связь $V$ через $Q$ \eqref{VQ}} \} &= V^\pi(s) - V^\pi(s) = 0   \tagqed
\end{align*}
\end{proposition}

\begin{proposition}
\label{adv_is_positive}
Для любой политики $\pi$ и любого состояния $s$:
$$\max_a A^\pi(s, a) \ge 0$$
\end{proposition}

Advantage --- это, если угодно, <<центрированная>> Q-функция. Если $A^\pi(s, a) \HM> 0$ --- действие $a$ <<лучше среднего>> для нашей текущей политики в состоянии $s$, меньше нуля --- хуже.  И интуиция, что процесс обучения нужно строить на той простой идее, что первые действия надо выбирать чаще, а вторые --- реже, нас не обманывает. 

Естественно, подвох в том, что на практике мы не будем знать точное значение оценочных функций, а значит, и истинное значение Advantage. Решая вопрос оценки значения Advantage для данной пары $s, a$, мы фактически будем проводить credit assingment --- это одна и та же задача.

\subsection{Relative Performance Identity (RPI)}

Мы сейчас докажем одну очень интересную лемму, которая не так часто нам будет нужна в будущем, но которая прям открывает глаза на мир. Для этого вспомним формулу reward shaping-а \eqref{rewardshaping} и заметим, что мы можем выбрать в качестве потенциала V-функцию произвольной стратегии $\pi_2$:
$$\Phi(s) \coloneqq V^{\pi_2}(s)$$
Действительно, требований к потенциалу два: ограниченность (для V-функций это выполняется в силу наших ограничений на рассматриваемые MDP) и равенство нулю в терминальных состояниях (для V-функций это верно по определению). Подставив такой потенциал, мы получим связь между performance-ом $J(\pi) = V^\pi(s_0)$ двух разных стратегий. В общем виде лемма сравнивает V-функции двух стратегий в одном состоянии:

\begin{theoremBox}[label=th:rpi]{Relative Performance Identity}
Для любых двух политик $\textcolor{ChadBlue}{\pi_1}$, $\textcolor{ChadPurple}{\pi_2}$:
\begin{equation}\label{RPI}
\textcolor{ChadPurple}{V^{\pi_2}}(s) - \textcolor{ChadBlue}{V^{\pi_1}}(s) = \E_{\textcolor{ChadPurple}{\Traj \sim \pi_2} \mid s_0 = s} \sum_{t \ge 0} \gamma^t \textcolor{ChadBlue}{A^{\pi_1}}(s_t, a_t)
\end{equation}
\beginproof
\begin{align*}
\textcolor{ChadPurple}{V^{\pi_2}}(s) - \textcolor{ChadBlue}{V^{\pi_1}}(s) &= \E_{\textcolor{ChadPurple}{\Traj \sim \pi_2} \mid s_0 = s} \sum_{t \ge 0} \gamma^t r_t - \textcolor{ChadBlue}{V^{\pi_1}}(s) = \\
&= \E_{\textcolor{ChadPurple}{\Traj \sim \pi_2} \mid s_0 = s} \left[\sum_{t \ge 0} \gamma^t r_t - \textcolor{ChadBlue}{V^{\pi_1}}(s_0) \right] = \\
\{ \text{телескопирующая сумма \eqref{telescopingsum}} \} &= \E_{\textcolor{ChadPurple}{\Traj \sim \pi_2} \mid s_0 = s} \left[\sum_{t \ge 0} \gamma^t r_t + \sum_{t \ge 0} \left[ \gamma^{t+1} \textcolor{ChadBlue}{V^{\pi_1}}(s_{t+1}) - \gamma^t \textcolor{ChadBlue}{V^{\pi_1}}(s_t) \right] \right] = \\
\{ \text{перегруппируем слагаемые} \} &= \E_{\textcolor{ChadPurple}{\Traj \sim \pi_2} \mid s_0 = s} \sum_{t \ge 0} \gamma^t \left( r_t + \gamma \textcolor{ChadBlue}{V^{\pi_1}}(s_{t+1}) - \textcolor{ChadBlue}{V^{\pi_1}}(s_t) \right) = \\
\{ \text{фокус $\E_x f(x) = \E_x \E_x f(x)$} \} &= \E_{\textcolor{ChadPurple}{\Traj \sim \pi_2} \mid s_0 = s} \sum_{t \ge 0} \gamma^t \left( r_t + \gamma \E_{s_{t+1}} \textcolor{ChadBlue}{V^{\pi_1}}(s_{t+1}) - \textcolor{ChadBlue}{V^{\pi_1}}(s_t) \right) = \\
\{ \text{выделяем Q-функцию \eqref{QV}} \} &= \E_{\textcolor{ChadPurple}{\Traj \sim \pi_2} \mid s_0 = s} \sum_{t \ge 0} \gamma^t \left( \textcolor{ChadBlue}{Q^{\pi_1}}(s_t, a_t) - \textcolor{ChadBlue}{V^{\pi_1}}(s_t) \right) \\
\{ \text{по определению \eqref{advantage}} \} &= \E_{\textcolor{ChadPurple}{\Traj \sim \pi_2} \mid s_0 = s} \sum_{t \ge 0} \gamma^t \textcolor{ChadBlue}{A^{\pi_1}}(s_t, a_t)   \tagqed
\end{align*}
\end{theoremBox}

Мы смогли записать наш функционал как мат.ожидание по траекториям, сгенерированным одной политикой, по оценочной функции другой стратегии. Фактически, мы можем награду заменить Advantage-функцией произвольной другой стратегии, и это сдвинет оптимизируемый функционал на константу! Прикольно.

Конечно, это теоретическое утверждение, поскольку на практике узнать точно оценочную функцию какой-то другой стратегии достаточно сложно (хотя ничто не мешает в качестве потенциала использовать произвольную функцию, приближающую $\textcolor{ChadBlue}{V^{\pi_1}}(s)$). Однако в этой <<новой>> награде замешаны сигналы из будущего, награды, которые будут получены через много шагов, и эта <<новая>> награда априори информативнее исходной $r(s, a)$.

Представим, что мы оптимизировали исходный функционал
$$\E_{\textcolor{ChadPurple}{\Traj \sim \pi_2} \mid s_0 = s} \sum_{t \ge 0} \gamma^t r(s_t, a_t) \to \max_{\textcolor{ChadPurple}{\pi_2}}$$
и сказали: слушайте, мы не знаем, как управлять марковской цепью, не очень понимаем, как выбор тех или иных действий в состоянии влияет на структуру траектории $p(\textcolor{ChadPurple}{\Traj \mid \pi_2})$. А давайте мы притворимся, что у нас нет в задаче отложенного сигнала (что очень существенное упрощение), и будем просто во всех состояниях $s$ оптимизировать $r(s, a)$: выбирать <<хорошие>> действия $a$, где функция награды высокая. То есть будем просто выбирать $\textcolor{ChadPurple}{\pi_2}(s) = \argmax\limits_{a} r(s, a)$. Смысла в этом будет мало.

Теперь же мы преобразовали функционал, сменив функцию награды:
$$\E_{\textcolor{ChadPurple}{\Traj \sim \pi_2} \mid s_0 = s} \sum_{t \ge 0} \gamma^t \textcolor{ChadBlue}{A^{\pi_1}}(s_t, a_t) \to \max_{\textcolor{ChadPurple}{\pi_2}}$$

Что, если мы поступим также с новой наградой? Мы, например, знаем, что Advantage --- не произвольная функция, и она обязана в среднем равняться нулю (утв. \ref{pr:advantageiszero}). Значит, если мы выберем
$$\textcolor{ChadPurple}{\pi_2}(s) = \argmax_{a} \textcolor{ChadBlue}{A^{\pi_1}}(s, a),$$
то все встречаемые пары $(s, a)$ в траекториях из $\pi_2$ будут обязательно с неотрицательными наградами за шаг $\textcolor{ChadBlue}{A^{\pi_1}}(s, a) \ge 0$. Значит и вся сумма наград будет неотрицательна для любого стартового состояния:
$$\E_{\textcolor{ChadPurple}{\Traj \sim \pi_2} \mid s_0 = s} \sum_{t \ge 0} \gamma^t \underbrace{\textcolor{ChadBlue}{A^{\pi_1}}(s_t, a_t)}_{\ge 0} \ge 0$$

И тогда из теоремы \ref{th:rpi} об RPI мы можем заключить, что для любого $s$:
$$\textcolor{ChadPurple}{V^{\pi_2}}(s) - \textcolor{ChadBlue}{V^{\pi_1}}(s) \ge 0$$
Это наблюдение - ключ к оптимизации стратегии при известной оценочной функции другой стратегии.

\subsection{Policy Improvement}

\begin{definition}
Будем говорить, что стратегия $\pi_2$ <<\emph{не хуже}>> $\pi_1$ (запись: $\pi_2 \succeq \pi_1$), если $\forall s \colon$
$$V^{\pi_2}(s) \ge V^{\pi_1}(s),$$
и \emph{лучше} (запись: $\pi_2 \succ \pi_1$), если также найдётся $s$, для которого неравенство выполнено строго:
$$V^{\pi_2}(s) > V^{\pi_1}(s)$$
\end{definition}

Мы ввели частичный порядок на множестве стратегий (понятно, что можно придумать две стратегии, которые будут <<не сравнимы>>: когда в одном состоянии одна будет набирать больше второй, в другом состоянии вторая будет набирать больше первой).

Зададимся следующим вопросом. Пусть для стратегии $\pi_1$ мы знаем оценочную функцию $Q^{\pi_1}$; тогда мы знаем и $V^{\pi_1}$ из VQ уравнения \eqref{VQ} и $A^{\pi_1}$ по определению \eqref{advantage}. Давайте попробуем построить $\pi_2 \succ \pi_1$. Для этого покажем более <<классическим>> способом, что стратегии $\pi_2$ достаточно лишь в среднем выбирать действия, дающие неотрицательный Advantage стратегии $\pi_1$, чтобы быть не хуже.

\begin{theoremBox}[label=th:policyimprovement]{Policy Improvement}
Пусть стратегии $\textcolor{ChadBlue}{\pi_1}$ и $\textcolor{ChadPurple}{\pi_2}$ таковы, что для всех состояний $s$ выполняется:
$$\E_{\textcolor{ChadPurple}{\pi_2}(a \mid s)} \textcolor{ChadBlue}{Q^{\pi_1}}(s, a) \ge \textcolor{ChadBlue}{V^{\pi_1}}(s),$$
или, в эквивалентной форме:
$$\E_{\textcolor{ChadPurple}{\pi_2}(a \mid s)} \textcolor{ChadBlue}{A^{\pi_1}}(s, a) \ge 0.$$
Тогда $\textcolor{ChadPurple}{\pi_2} \succeq \textcolor{ChadBlue}{\pi_1}$; если хотя бы для одного $s$ неравенство выполнено строго, то $\textcolor{ChadPurple}{\pi_2} \succ \textcolor{ChadBlue}{\pi_1}$.
\begin{proof}
Покажем, что $\textcolor{ChadPurple}{V^{\pi_2}}(s) \ge \textcolor{ChadBlue}{V^{\pi_1}}(s)$ для любого $s$:
\begin{align*}
\textcolor{ChadBlue}{V^{\pi_1}}(s) = \{ \text{связь VQ \eqref{VQ}} \} &= \E_{\textcolor{ChadBlue}{\pi_1}(a \mid s)} \textcolor{ChadBlue}{Q^{\pi_1}}(s, a) \le \\
= \{ \text{по построению $\textcolor{ChadPurple}{\pi_2}$} \} &= \E_{\textcolor{ChadPurple}{\pi_2}(a \mid s)} \textcolor{ChadBlue}{Q^{\pi_1}}(s, a) = \\
= \{ \text{связь QV \eqref{QV}} \} &= \E_{\textcolor{ChadPurple}{\pi_2}(a \mid s)} \left[ r + \gamma \E_{s'} \textcolor{ChadBlue}{V^{\pi_1}}(s') \right] \le \\
\le \{ \text{применяем это же неравенство рекурсивно} \} &= \E_{\textcolor{ChadPurple}{\pi_2}(a \mid s)} \left[ r + \E_{s'} \E_{\textcolor{ChadPurple}{\pi_2}(a' \mid s')} \left[ \gamma r' + \gamma^2 \E_{s''} \textcolor{ChadBlue}{V^{\pi_1}}(s'') \right] \right] \le \\
\le \{ \text{раскручиваем цепочку далее} \} &\le \dots \le \E_{\textcolor{ChadPurple}{\Traj \sim \pi_2} \mid s_0 = s} \sum_{t \ge 0} \gamma^t r_t = \\
= \{ \text{по определению \eqref{Vdefinition}} \} &= \textcolor{ChadPurple}{V^{\pi_2}}(s)
\end{align*}
Если для какого-то $s$ неравенство из условия теоремы было выполнено строго, то для него первое неравенство в этой цепочке рассуждений выполняется строго, и, значит, $\textcolor{ChadPurple}{V^{\pi_2}}(s) > \textcolor{ChadBlue}{V^{\pi_1}}(s)$.
\end{proof}
\end{theoremBox}

Что означает эта теорема? Знание оценочной функции позволяет улучшить стратегию. Улучшать стратегию можно прямо в отдельных состояниях, например, выбрав некоторое состояние $s$ и сказав: неважно, как это повлияет на частоты посещения состояний, но будем конкретно в этом состоянии $s$ выбирать действия так, что значение
\begin{equation}\label{pi_optimization}
\E_{\textcolor{ChadPurple}{\pi_2}(a \mid s)} \textcolor{ChadBlue}{Q^{\pi_1}}(s, a)    
\end{equation}
как можно больше. Тогда, если в $s$ действие выбирается <<новой>> стратегией $\textcolor{ChadPurple}{\pi_2}$, а в будущем агент будет вести себя \textit{не хуже}, чем $\textcolor{ChadBlue}{\pi_1}$, то и наберёт он в будущем не меньше $\textcolor{ChadBlue}{Q^{\pi_1}}(s, a)$. Доказательство теоремы \ref{th:policyimprovement} показывает, что выражение \eqref{pi_optimization} является нижней оценкой на награду, которую соберёт <<новый>> агент со стратегией $\textcolor{ChadPurple}{\pi_2}$. 

Если эта нижняя оценка поднята выше $\textcolor{ChadBlue}{V^{\pi_1}}(s)$, то стратегию удалось улучшить: и тогда какой бы ни была $\pi_1$, мы точно имеем гарантии $\pi_2 \succeq \pi_1$. Важно, что такой policy improvement работает всегда: и для <<тупых>> стратегий, близких к случайному поведению, и для уже умеющих что-то разумное делать.

В частности, мы можем попробовать нижнюю оценку \eqref{pi_optimization} максимально поднять, то есть провести \emph{жадный} (greedy) policy improvement. Для этого мы формально решаем такую задачу оптимизации:
$$
\E_{\textcolor{ChadPurple}{\pi_2}(a \mid s)} \textcolor{ChadBlue}{Q^{\pi_1}}(s, a) \to \max_{\textcolor{ChadPurple}{\pi_2}},
$$
и понятно, что решение находится в детерминированной $\pi_2$:
$$\textcolor{ChadPurple}{\pi_2}(s) = \argmax_{a} \textcolor{ChadBlue}{Q^{\pi_1}}(s, a)= \argmax_{a} \textcolor{ChadBlue}{A^{\pi_1}}(s, a)$$

% \begin{proposition}
% Пусть $\pi_2$ такова, что $\forall s, a \colon \pi_2(a \mid s) > 0 \Rightarrow a \in \Argmax\limits_a A^{\pi_1}(s, a)$. Тогда $\pi_2 \succeq \pi_1$.
% \end{proposition}

Конечно, мы так не получим <<за один ход>> сразу оптимальную стратегию, поскольку выбор $\textcolor{ChadPurple}{\pi_2}(a \mid s)$ сколь угодно хитро может изменить распределение траекторий, но тем не менее.

\needspace{15\baselineskip}
\begin{example}
Попробуем улучшить стратегию $\pi$ из примера \ref{ex:vfunction}, $\gamma = 0.8$. Например, в состоянии C она выбирает \colorsquare{ChadRed} с вероятностью 1 и получает -1; попробуем посчитать $Q^{\pi}(s \HM= C, \colorsquare{ChadBlue})$:
$$
Q^{\pi}(s = C, \colorsquare{ChadBlue}) = 0.2\gamma V^{\pi}(C) + 0.8 \gamma V^{\pi}(B)
$$
\needspace{13\baselineskip}
\begin{wrapfigure}{r}{0.45\textwidth}
\vspace{-0.8cm}
\centering
\includegraphics[width=0.4\textwidth]{Images/Value.png}
%\vspace{-0.4cm}
\end{wrapfigure}
Подставляя ранее подсчитанные $V^{\pi}(C) \HM= -1, V^{\pi}(B) \HM= 5$, видим, что действие \colorsquare{ChadBlue} принесло бы нашей стратегии $\pi$ куда больше -1, а именно $Q^{\pi}(s = C, \colorsquare{ChadBlue}) \HM= 3.04$. Давайте построим $\pi_2$, скопировав $\pi$ в A и B, а в C будем с вероятностью 1 выбирать \colorsquare{ChadBlue}.

Что говорит нам теория? Важно, что она не даёт нам значение $V^{\pi_2}(C)$; в частности, нельзя утверждать, что $Q^{\pi_2}(s = C, \colorsquare{ChadBlue}) \HM= 3.04$, и повторение вычислений подтвердит, что это не так. Однако у нас есть гарантии, что, во-первых, $Q^{\pi_2}(s = C, \colorsquare{ChadBlue}) \ge 3.04$, и, что важнее, из состояния C мы начали набирать больше награды: $V^{\pi_2}(C) \HM> V^{\pi_1}(C)$ строго. Во-вторых, есть гарантии, что мы не <<сломали>> стратегию в других состояниях: во всех остальных состояниях гарантированно $V^{\pi_2}(s) \HM\ge V^{\pi_1}(s)$. Для Q-функции, как можно показать, выполняются аналогичные неравенства.
\end{example}

\subsection{Вид оптимальной стратегии (доказательство через PI)}

Что, если для некоторой $\textcolor{ChadBlue}{\pi_1}$ мы <<не можем>> провести Policy Improvement? Под этим будем понимать, что мы не можем выбрать $\textcolor{ChadPurple}{\pi_{2}}$ так, что $\E_{\textcolor{ChadPurple}{\pi_{2}}(a \mid s)} \textcolor{ChadBlue}{Q^{\pi_{1}}}(s, a) \HM> \textcolor{ChadBlue}{V^{\pi_{1}}}(s)$ строго хотя бы для одного состояния $s$ (ну, равенства в любом состоянии $s$ мы добьёмся всегда, скопировав $\textcolor{ChadBlue}{\pi_1}(\cdot \HM\mid s)$). Такое может случиться, если и только если $\textcolor{ChadBlue}{\pi_1}$ удовлетворяет следующему свойству:
$$\max_a \textcolor{ChadBlue}{Q^{\pi_{1}}}(s, a) = \textcolor{ChadBlue}{V^{\pi_{1}}}(s) \quad \Leftrightarrow \quad \max\limits_a \textcolor{ChadBlue}{A^{\pi_1}}(s, a) = 0$$

Но это в точности критерий оптимальности Беллмана, теорема \ref{th:optimalitycriterion}! Причём мы можем, воспользовавшись теоремами RPI \ref{th:rpi} и о Policy Improvement \ref{th:policyimprovement}, теперь доказать этот критерий альтернативным способом, не прибегая к формализму оптимальных оценочных функций\footnote{в доказательствах RPI и Policy Improvement мы не использовали понятия $Q^*$ и $V^*$ и их свойства; тем не менее, из этих теорем все свойства оптимальных оценочных функций следуют: например, пусть $A^*, Q^*, V^*$ --- оценочные функции оптимальных стратегий, тогда в силу выводимого из RPI критерия оптимальности (теорема \ref{th:optimalitycriterion_pi}) $\forall s \colon \max\limits_a A^*(s, a) \HM= 0$, или, что тоже самое, $\max\limits_a \left[ Q^*(s, a) \HM- V^*(s) \right] \HM= 0$; отсюда $V^*(s) \HM= \max\limits_a Q^*(s, a)$. Аналогично достаточно просто можно получить все остальные утверждения об оптимальных оценочных функциях, не прибегая к рассуждению с отказом от стационарности.} и не требуя рассуждения про отказ от стационарности и обоснования единственности решения уравнений оптимальности Беллмана.

\begin{theoremBox}[label=th:optimalitycriterion_pi]{Критерий оптимальности (альт. доказательство)}
$\pi$ оптимальна тогда и только тогда, когда $\forall s \colon \max\limits_a A^\pi(s, a) = 0$.
\begin{proof}[Достаточность]
Допустим, это не так, и существует $\pi_2, s \colon V^{\pi_2}(s) > V^{\pi}(s)$. Тогда по RPI \eqref{RPI}
$$\E_{\Traj \sim \pi_2 \mid s_0 = s} \sum_{t \ge 0} \gamma^t A^{\pi}(s_t, a_t) > 0,$$
однако все слагаемые в сумме неположительны. Противоречие.
\end{proof}
\begin{proof}[Необходимость]
Допустим, что $\pi$ оптимальна, но для некоторого $\hat{s}$ условие не выполнено, и $\max\limits_{a} A^\pi(\hat{s}, a) > 0$ (меньше нуля он, ещё раз, быть не может в силу утв.~ \ref{adv_is_positive}). Рассмотрим детерминированную $\pi_2$, которая в состоянии $\hat{s}$ выбирает какое-нибудь $\hat{a}$, такое что $A^\pi(\hat{s}, \hat{a}) > 0$ (это можно сделать по условию утверждения --- сам максимум может вдруг оказаться недостижим для сложных пространств действий, но какое-то действие с положительным advantage-ем мы найдём), а в остальных состояниях выбирает какое-нибудь действие, т.ч. advantage-функция неотрицательна. Тогда  
$$V^{\pi_2}(\hat{s}) - V^{\pi}(\hat{s}) = \E_{\Traj \sim \pi_2 | s_0 = \hat{s}} \sum_{t \ge 0} \gamma^t A^{\pi}(s_t, a_t) > 0$$
поскольку все слагаемые неотрицательны, и во всех траекториях с вероятностью\footnote[*]{мы специально стартовали из $\hat{s}$, чтобы пары $\hat{s}, \hat{a}$ <<встретились>> в траекториях, иначе могло бы быть такое, что агент в это состояние $\hat{s}$ <<никогда не попадает>>, и отделиться от нуля не получилось бы.} 1 верно $s_0 \HM= \hat{s}, a_0 \HM= \hat{a}$, то есть первое слагаемое равно $A(\hat{s}, \hat{a}) \HM> 0$. 
\end{proof} 
\end{theoremBox}

Итак, мораль полученных результатов такая: зная $Q^{\pi}$, мы можем придумать стратегию лучше. Не можем --- значит, наша текущая стратегия $\pi$ уже оптимальная.

% \begin{theorem}[Критерий оптимальности Беллмана]
% $\pi$ оптимальна тогда и только тогда, когда $ \forall s, a \colon \pi(a \mid s) > 0$ верно:
% $$a \in \Argmax_a Q^\pi(s, a)$$

% \begin{proof}
% $$\Argmax_a Q^\pi(s, a) = \Argmax_a \left[ Q^\pi(s, a) - V^\pi(s) \right] = \Argmax_a A^\pi(s, a)$$

% При этом в силу свойства Advantage-функции \ref{pr:advantageiszero} выбор стратегией только действий с максимальным Advantage-м $a \in \Argmax\limits_a A^\pi(s, a)$ эквивалентно равенству $0 \HM= \E_{a} A^{\pi}(s, a) \HM= \max\limits_a A^\pi(s, a)$ во всех состояниях. Поэтому утверждение эквивалентно предыдущей теореме.
% \end{proof}
% \end{theorem}

