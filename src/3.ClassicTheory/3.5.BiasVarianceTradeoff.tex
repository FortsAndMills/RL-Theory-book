\section{Bias-Variance Trade-Off}\label{sec:biasvar}

\subsection{Дилемма смещения-разброса}

Мы обсудили два вида бэкапов, доступных в model-free обучении: Монте-Карло бэкап и Temporal-Difference бэкап. На самом деле, они очень похожи, поскольку делают обновление вида
\begin{equation*}
Q(s, a) \leftarrow Q(s, a) + \alpha \left( y_Q - Q(s, a) \right),
\end{equation*}
и отличаются лишь выбором $y_Q$: Монте-Карло берёт reward-to-go, а TD-backup --- одношаговую бутстрапированную оценку с использованием уже имеющейся аппроксимации Q-функции:
$$y_Q \coloneqq r(s, a) + \gamma Q(s', a')$$

Какой из этих двух вариантов лучше? Мы уже обсуждали недостатки Монте-Карло оценок: высокая дисперсия, необходимость играть до конца эпизодов, игнорирование структуры получаемой награды и потеря информации о соединениях состояний. Но не то, чтобы одношаговые оценки сильно лучше: на самом деле, они обладают полностью противоположными свойствами и проблемами. 

Да, одношаговые оценки аппроксимируют решение одношаговых уравнений Беллмана и приближают алгоритм динамического программирования: поэтому они не теряют информации о том, на каком шаге какой сигнал от среды был получен, и сохраняют информацию о сэмплах $s'$ из функции переходов; в том числе, как мы видели, одношаговые алгоритмы могут использовать реплей буфер, по сути и хранящий собранную выборку таких сэмплов. Взамен в одношаговых алгоритмах возникает \emph{проблема распространения сигнала}.

\begin{example}
Представьте, что за 100 шагов вы можете добраться до сыра (+1). Пока вы не добьётесь успеха, сигнала нет, и ваша аппроксимация Q-функции остаётся всюду нулём. Допустим, вы учитесь с одношаговых оценок с онлайн опыта. После первого успеха +1 распространится лишь в пару $s, a$, непосредственно предшествующей получению сыра; после второго успеха +1 распространится из пары $s, a$ в предыдущую и так далее. Итого, чтобы распространить сигнал на 100 шагов, понадобится сделать 100 обновлений. С другой стороны, если бы использовалась Монте-Карло оценка, после первого же успеха +1 распространился бы во все пары $s, a$ из успешной траектории.
\end{example}

Вместо высокой дисперсии Монте-Карло оценок в одношаговых оценках нас ждёт большое \emph{смещение} (bias): если $y_Q$ оценено через нашу же текущую аппроксимацию через бутстрапирование, то оно не является несмещённой оценкой искомой $Q^{\pi}(s, a)$ и может быть сколь угодно <<неправильным>>. Как мы увидели, гарантии сходимости остаются, но естественно, что методы стохастической аппроксимации из-за смещения будут сходиться сильно дольше экспоненциального сглаживания, которому на вход поступают несмещённые оценки искомой величины. Но дисперсия $y_Q$ в temporal difference обновлениях, например, в алгоритме Q-learning \ref{alg:qlearning}, конечно, сильно меньше дисперсии Монте-Карло оценок: внутри нашей аппроксимации Q-функции уже усреднены все будущие награды, то есть <<взяты>> все интегралы, относящиеся к будущим после первого шага наградам. Итого одношаговая оценка $y_Q$ --- случайная величина только от $s'$, а не от всего хвоста траектории $s', a', s'' \dots$. Выбор между оценками с высокой дисперсией и отсутствием смещения (Монте-Карло) и оценками с низкой дисперсией и большим смещением (одношаговые оценки) --- особая задача в обучении с подкреплением, называемая \emph{bias-variance trade-off}.

\subsection{N-step Temporal Difference}

Какие есть промежуточные варианты между одношаговыми оценками и Монте-Карло оценками? Давайте заглядывать в будущее не на один шаг и не до самого конца, а на $N$ шагов. Итак, пусть у нас есть целый фрагмент траектории:

\begin{definition}
Фрагмент траектории $s, a, r, s', a', r', s'', a'', r'', \dots s^{(N)}, a^{(N)}$, где $s^{(N)}, a^{(N)}$ --- состояние и действие, которое агент встретил через $N$ шагов, будем называть \emph{роллаутом} (rollout) длины $N$.
\end{definition}

\begin{definition}
\emph{$N$-шаговой оценкой} (N-step estimation) для $Q^{\pi}(s, a)$ назовём следующий таргет:
$$y_Q \coloneqq r + \gamma r' + \gamma^2 r'' + \dots + \gamma^{N-1} r^{(N-1)} + \gamma^N Q(s^{(N)}, a^{(N)}),$$
\end{definition}

Такой таргет является стохастической аппроксимацией правой части $N$-шагового уравнения Беллмана \eqref{NstepBellman}, и формула \eqref{generalTD} с таким таргетом позволяет эту систему уравнений решать в model-free режиме. По каким переменным мы заменили интегралы на Монте-Карло приближения в такой оценке? По переменным $s', a', s'', a'' \dots s^{(N)}, a^{(N)}$, которые, пусть и не все присутствуют явно в формуле, но неявно задают то уравнение, которое мы решаем. Соответственно, чтобы выучить $Q^{\pi}(s, a)$, нужно, чтобы состояния приходили из функции переходов, а действия --- из оцениваемой стратегии $\pi$. Другими словами, роллаут, использованный для построения таргета, должен быть порождён оцениваемой стратегией.

Почему гиперпараметр $N$ отвечает за bias-variance trade-off? Понятно, что при $N \HM\to \infty$ оценка переходит в Монте-Карло оценку. С увеличением $N$ всё больше интегралов заменяется на Монте-Карло оценки, и растёт дисперсия; наше же смещённое приближение будущих наград $Q^{\pi}(s^{(N)}, a^{(N)})$, которое может быть сколь угодно неверным, домножается на $\gamma^N$, и во столько же раз сбивается потенциальное смещение; с ростом $N$ оно уходит в ноль. Замешивая в оценку слагаемое $r + \gamma r' + \gamma^2 r'' + \dots + \gamma^{N-1} r^{(N-1)}$ мы теряем информацию о том, что из этого в какой момент было получено, но и начинаем распространять сигнал в $N$ раз <<быстрее>> одношаговой оценки.

Сразу заметим, что при $N \HM> 1$ нам необходимо иметь в $N$-шаговой оценке сэмплы $a' \HM\sim \pi(a' \HM\mid s'), s'' \HM\sim p(s'' \mid s', a')$. Это означает, что мы не можем получить такую оценку с буфера: действительно, в буфере для данной четвёрки $s, a, r, s'$ не лежит сэмпл $a' \HM\sim \pi(a \HM\mid s)$, ведь в произвольном буфере $a'$ генерируется стратегией сбора данных (старой версией стратегией или <<экспертом>>). Само по себе это, вообще говоря, не беда: мы могли бы взять из буфера четвёрку, прогнать стратегию $\pi$, которую хотим оценить, на $s'$ и сгенерировать себе сэмпл $a'$; но для него мы не сможем получить сэмпл $s''$ из функции переходов! Поэтому обучаться на многошаговые оценки с буфера не выйдет; по крайней мере, без какой-либо коррекции.

Также отметим, что все рассуждения одинаковы применимы как для обучения $Q^\pi$, так и $V^\pi$. Для V-функции общая формула обновления выглядит так:
\begin{equation}\label{generalTD}
V(s) \leftarrow V(s) + \alpha \left( y_V - V(s) \right),
\end{equation}
где $y_V$ --- reward-to-go при использовании Монте-Карло оценки, и $y_V \HM \coloneqq r(s, a) \HM+ \gamma V(s')$ для одношагового метода временных разностей, где $a \HM \sim \pi(a \HM \mid s)$ (сэмплирован из текущей оцениваемой стратегии), $s' \HM \sim p(s' \HM\mid s, a)$. Соответственно, для V-функции обучаться с буфера (без каких-либо коррекций) невозможно даже при $N \HM= 1$, поскольку лежащий в буфере $a$ сэмплирован из стратегии сбора данных, а не оцениваемой $\pi$.

Для простоты и наглядности будем обсуждать обучение $V^\pi$. Также введём следующие обозначения:

\begin{definition}
Введём такое обозначение \emph{$N$-шаговой временной разности} (N-step temporal difference) для пары $s, a$:
\begin{equation}\label{Nstepadvantage}
\Psi_{(N)} (s, a) \coloneqq \sum_{t=0}^{N-1} \gamma^{t} r^{(t)} + \gamma^N V(s^{(N)}) - V(s)
\end{equation}
где $V$ --- текущая аппроксимация V-функции, $s, a, \dots s^{(N)}$ --- роллаут, порождённый $\pi$.
\end{definition}

Ранее мы обсуждали temporal difference, в котором мы сдвигали нашу аппроксимацию на $\Psi_{(1)}(s, a)$:
\begin{align*}
V(s) &\leftarrow V(s) + \alpha \left( r + \gamma V(s') - V(s) \right) = \\ &= V(s) + \alpha \Psi_{(1)}(s, a)
\end{align*}
Теперь же мы можем обобщить наш метод, заменив оценку V-функции на многошаговую оценку:
$$V(s) \leftarrow V(s) + \alpha \Psi_{(N)}(s, a)$$

Но какое $N$ выбрать?

\subsection{Интерпретация через Credit Assingment}

Вопрос, обучаться ли со смещённых оценок, или с тех, которые имеют большую дисперсию, имеет прямое отношение к одной из центральных проблем RL --- credit assingment. На самом деле, это ровно та же самая проблема. 

Рассмотрим проблему credit assingment-а: за какие будущие награды <<в ответе>> то действие, которое было выполнено в некотором состоянии $s$? Как мы обсуждали в разделе \ref{subsection:advantage}, <<идеальное>> решение задачи --- значение $A^\pi(s, a)$, но на практике у нас нет точных значений оценочных функций, а есть лишь аппроксимация, допустим, $V \HM \approx V^\pi$. Положим, мы знаем сэмпл траектории $a, r, s', a', r', s'', a'', \dots$ до конца эпизода. 

% \vspace{0.2cm}
% \begin{center}
% \begin{tabular}{ccccccccccc}
% \toprule
%     & $s'$ & $s''$ & $\cdots$ & $s^{(23)}$ & $s^{(24)}$ & $s^{(25)}$ & $s^{(26)}$ $\cdots$ & $s^{(T - 1)}$ & $s^{(T)}$  \\
% \midrule
%     $r$ & $0$ & $0$ & $\cdots$ & $0$ & $0$ & $0$ & $0$ $\cdots$ & $0$ & $+1$  \\
%     \hdashline
%     $V(s)$ & $0$ & $0$ & $\cdots$ & $0$ & $0$ & $1$ & $1$ $\cdots$ & $1$ &  \\
% \bottomrule
% \end{tabular}
% \end{center}
% \vspace{0.2cm}

С одной стороны, мы можем выдавать кредит, полностью опираясь на аппроксимацию:
$$Q^\pi(s, a) = r(s, a) + \gamma \E_{s'} V^\pi(s') \approx r(s, a) + \gamma V(s')$$
\begin{equation}\label{onestepcredit}
A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s) \approx r(s, a) + \gamma V(s') - V(s)
\end{equation}
Проблема в том, что наша аппроксимация может быть сколь угодно неверна, и выдавать полную ерунду. Более <<безопасный>> с этой точки зрения способ --- в приближении Q-функции не опираться на аппроксимацию и использовать reward-to-go:
\begin{equation}\label{infinitestepcredit}
A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s) \approx \sum_{t \ge 0} \gamma^t r^{(t)} - V(s)
\end{equation}
У этого второго способа выдавать кредит есть важное свойство, которого нет у первого: в среднем такой кредит будет больше у тех действий, которые действительно приводят к более высокой награде. То есть, если $a_1, a_2$ таковы, что $Q^\pi(s, a_1) \HM> Q^\pi(s, a_2)$, то при любой аппроксимации $V(s)$ среднее значение кредита для $s, a_1$ будет больше $s, a_2$. Это и есть свойство несмещённости Монте-Карло оценок в контексте проблемы выдачи кредита.

Беда Монте-Карло в том, что в этот кредит закладывается награда не только за выбор $a$ в $s$, но и за будущие решения. То есть: представьте, что через десять шагов агент выбрал действие, которое привело к +100. Эта награда +100 попадёт и в кредит всех предыдущих действий, хотя те не имели к нему отношения.

\begin{example}
Допустим, мы ведём машину, и в состоянии $s$, где аппроксимация V-функции прогнозирует будущую награду ноль, решили выехать на встречку. Среда не сообщает нам никакого сигнала (пока не произошло никакой аварии), но аппроксимация V-функции резко упала; если мы проводим credit assingment одношаговым способом \eqref{onestepcredit}, то мы получаем сильно отрицательный кредит, который сообщает, что это действие было явно плохим.

Дальше агент следующими действиями исправил ситуацию, вернулся на правильную полосу и после решил поехать в магазин тортиков, где оказался юбилейным покупателем и получил бесплатный тортик +10. Если бы мы проводили credit assingment методом Монте-Карло \eqref{infinitestepcredit}, кредит получился бы сильно положительным: $+10 - 0 \HM= +10$: мы положили, что выезд на встречку привёл к тортику.
\end{example}

Видно, что эти два крайних способа выдачи кредита есть в точности <<градиент>> $y_V - V(s)$, по которому учится V-функция в формуле \eqref{generalTD}. Фактически, занимаясь обучением V-функции и используя формулу
\begin{equation*}
V(s) \leftarrow V(s) + \alpha \Psi(s, a),
\end{equation*}
мы выбором функции $\Psi(s, a)$ по-разному проводим credit assingment. 

Таким образом видна новая интерпретация bias-variance trade-off-а: и <<дисперсия>> с этой точки зрения имеет смысл возложения на действие ответственности за те будущие награды, к которым это действие отношения на самом деле не имеет. Одношаговая оценка $\Psi_{(1)}$ говорит: действие влияет только на награду, которую агент получит тут же, и весь остальной сигнал будет учтён в аппроксимации V-функции. Монте-Карло оценка $\Psi_{(\infty)}$ говорит, что действие влияет на все будущие награды. А $N$-шаговая оценка $\Psi_{(N)}$ говорит странную вещь: действие влияет на события, который происходят с агентом в течение следующих $N$ шагов.

Как и в любом trade-off, истина лежит где-то по середине. Однако подбирать на практике <<хорошее>> $N$, чтобы получить оценки с промежуточным смещением и дисперсией, затруднительно. Но что ещё важнее, все $N$-шаговые оценки на самом деле неудачные. Во-первых, они плохи тем, что не используют всю доступную информацию: если мы знаем будущее на $M$ шагов вперёд, то странно не использовать награды за шаг за все эти $M$ шагов, и странно не учесть прогноз нашей аппроксимации оценочной функции $V(s^{(t)})$ для всех доступных $t$. Во-вторых, странно, что в стационарной задаче, где всё инвариантно относительно времени, у нас появляется гиперпараметр, имеющий смысл количества шагов --- времени. Поэтому нас будет интересовать далее альтернативный способ <<интерполировать>> между Монте-Карло и одношаговыми оценками. Мы всё равно оттолкнёмся именно от $N$-шаговых оценок, поскольку понятно, что эти оценки <<корректны>>: они направляют нас к решению уравнений Беллмана, для которых искомая $V^\pi$ является единственной неподвижной точкой.

\needspace{5\baselineskip}
\begin{wrapfigure}{r}{0.35\textwidth}
%\vspace{-0.3cm}
\centering
\includegraphics[width=0.35\textwidth]{Images/forward_view.png}
\vspace{-0.5cm}
\end{wrapfigure}
Мы придумали эти $N$-шаговые оценки, посмотрев на задачу под следующим углом: мы знаем сэмпл будущего (хвост траектории) и хотим выдать кредит <<настоящему>>: самому первому действию. Такой взгляд на оценки называется <<\emph{forward view}>>: мы после выполнения $a$ из $s$ знаем <<вперёд>> своё будущее и можем обновить оценочную функцию для этой пары.

\subsection{Backward View}

Оказывается, мы можем посмотреть на задачу немного по-другому: можно, используя настоящее, обновлять кредит для прошлого. Рассмотрим эту идею, развив пример с кафе \ref{ex:cafe}. 

\begin{example}
Ещё раз сядем в кафе ($s$) и захотим вернуться домой. Текущая аппроксимация даёт $-V(s) \HM= 30$ минут. Делаем один шаг в среде: тратим одну минуту ($-r$) и обнаруживаем пробку ($s'$). Новая оценка времени возвращения даёт: $-V(s') \HM= 40$ минут, соответственно с нами случилась одношаговая временная разность $\Psi_{(1)}(s, a) \HM= 41 - 30 \HM= 11$ минут, которая позволяет нам корректировать $V(s)$.

\needspace{7\baselineskip}
\begin{wrapfigure}{r}{0.3\textwidth}
\vspace{-0.3cm}
\centering
\includegraphics[width=0.3\textwidth]{Images/MultiStepErrors3.png}
\vspace{-0.3cm}
\end{wrapfigure}

Давайте сделаем ещё один шаг в среде: тратим одну минуту $-r'$, видим пожар $s''$ и получаем новую оценку $-V(s'') \HM= 60$ минут. Тогда мы можем посчитать как одношаговую временную разность для пары $s', a'$, равную $\Psi_{(1)}(s', a') \HM= 61 \HM- 40 \HM= 21$ минуте, и уточнить свою аппроксимацию V-функции для состояния с пробкой; а можем посчитать и двухшаговую временную разность для кафе: мы потратили две минуты $r + r'$ на два шага и наша нынешнее приближение равно 60 минутам. Итого двухшаговая временная разность равна $\Psi_{(2)}(s, a) \HM= 62 - 30 \HM= 32$ минуты. Forward view говорит следующее: если мы хотим учиться на двухшаговые оценки вместо одношаговых, то нам не следовало на первом шаге использовать 11 минут для обновления $V(s)$, а нужно было дождаться второго шага, узнать двухшаговую ошибку в 32 минуты и воспользоваться ей.

Но понятно, что двухшаговая ошибка это сумма двух одношаговых ошибок! Наши 32 минуты ошибки --- это 11 минут ошибки после выхода из кафе в пробку плюс 21 минута ошибки от выхода из пробки в пожар. Давайте после первого шага используем уже известную часть ошибки в 11 минут, а на втором шаге, если мы готовы обучаться с двухшаговых ошибок, возьмём и добавим недостающие 21 минуту.
\end{example}

Формализуем эту идею. Пусть мы взаимодействуем в среде при помощи стратегии $\pi$, которую хотим оценить; также будем считать learning rate $\alpha$ константным. После совершения первого шага <<из кафе>> мы можем, зная $s, a, r, s'$ сразу же обновить нашу V-функцию:
\begin{equation}\label{twostepsupdatefirstpart}
V(s) \leftarrow V(s) + \alpha \Psi_{(1)}(s, a)    
\end{equation}
Затем мы делаем ещё один шаг в среде, узнавая $a', r', s''$ и временную разность за этот случившийся шаг $\Psi_{(1)}(s', a')$. Тогда мы можем просто ещё в нашу оценку V-функции добавить слагаемое:
\begin{equation}\label{twostepsupdatesecondpart}
V(s) \leftarrow V(s) + \alpha \gamma \Psi_{(1)}(s', a')
\end{equation}
Непосредственной проверкой легко убедиться, что суммарное обновление V-функции получится эквивалентным двухшаговому обновлению:
\begin{proposition}
Последовательное применение обновлений \eqref{twostepsupdatefirstpart} и \eqref{twostepsupdatesecondpart} эквивалентно двухшаговому обновлению
$$V(s) \leftarrow V(s) + \alpha \Psi_{(2)}(s, a)$$
\beginproof
\begin{align*}
V(s) &\leftarrow V(s) + \alpha \left( \Psi_{(1)}(s, a) + \gamma \Psi_{(1)}(s', a') \right) = \\ 
&= V(s) + \alpha \left( r + \gamma V(s') - V(s) + \gamma r' + \gamma^2 V(s'') - \gamma V(s') \right) = \\
&= V(s) + \alpha \left( r + \gamma r' + \gamma^2 V(s'') - V(s) \right) = \\
&= V(s) + \alpha \Psi_{(2)}(s, a)   \tagqed
\end{align*}
\end{proposition}

Понятно, что можно обобщить эту идею с двухшаговых ошибок на $N$-шаговые: действительно, ошибка за $N$ шагов равна сумме одношаговых ошибок за эти шаги.

\begin{theorem}\,
\begin{equation}\label{NstepAdvSimplified}
\Psi_{(N)}(s, a) = \sum_{t = 0}^{N - 1} \gamma^t \Psi_{(1)}(s^{(t)}, a^{(t)})
\end{equation}

\needspace{7\baselineskip}
\begin{wrapfigure}{r}{0.3\textwidth}
\vspace{-0.3cm}
\centering
\includegraphics[width=0.3\textwidth]{Images/MultiStepErrors4.png}
\vspace{-0.3cm}
\end{wrapfigure}
\beginproof
Докажем по индукции. Для $N = 1$ справа стоит только одно слагаемое, $\Psi_{(1)}(s, a)$, то есть одношаговая оценка.

Пусть утверждение верно для $N$, докажем для $N + 1$. В правой части при увеличении $N$ на единицу появляется одно слагаемое, то есть для доказательства достаточно показать, что
\begin{equation}\label{NtrasformtoNplusone}
\Psi_{(N + 1)}(s, a) = \Psi_{(N)}(s, a) + \gamma^N \Psi_{(1)}(s^{(N)}, a^{(N)})    
\end{equation}
Убедимся в этом, подставив определения:
\begin{align*}
\Psi_{(N + 1)}(s, a) &= \sum_{t = 0}^{N}\gamma^t r^{(t)} + \gamma^{N+1}V(s^{(N+1)}) - V(s) = \\
 &= \sum_{t = 0}^{N - 1}\gamma^t r^{(t)} + \gamma^N V(s^{(N)}) - V(s) + \gamma^N \left( r^{(N)} + \gamma V(s^{(N+1)}) - V(s^{(N)}) \right) = \\
&= \Psi_{(N)}(s, a) + \gamma^N \Psi_{(1)}(s^{(N)}, a^{(N)})   \tagqed
\end{align*}
\end{theorem}

Это наблюдение открывает, что все наши формулы обновления выражаются через одношаговые ошибки --- $\Psi_{(1)}(s, a)$. Это интересный факт, поскольку одношаговая временная разность
$$\Psi_{(1)}(s, a) = r + \gamma V(s') - V(s)$$
очень похожа на reward shaping \eqref{rewardshaping}, где в качестве потенциала выбрана наша текущая аппроксимация $V(s)$. Поэтому эти \emph{дельты}, как их ещё иногда называют, можно интерпретировать как некие <<новые награды>>, центрированные --- которые в среднем должны быть равны нулю, если аппроксимация V-функции точная.

\needspace{7\baselineskip}
\begin{wrapfigure}[11]{l}{0.35\textwidth}
%\vspace{-0.3cm}
\centering
\includegraphics[width=0.35\textwidth]{Images/backward_view.png}
%\vspace{-0.3cm}
\end{wrapfigure}
Итого, оказывается, мы можем на каждом шаге добавлять к оценкам V-функции ранее встречавшихся в эпизоде пар $s, a$ только что случившуюся одношаговую ошибку $\Psi_{(1)}(s, a)$ и таким образом получать $N$-шаговые обновления: достаточно пару $s, a$, посещённую $K$ шагов назад, обновить с весом $\gamma^K$. То есть мы начинаем действовать по-другому: зная, что было в прошлом, мы правильным образом обновляем оценочную функцию посещённых состояний из прошлого, используя временную разность за один последний шаг (<<настоящее>>). Такой подход к обновлению оценочной функции называется, соответственно, <<\emph{backward view}>>, и он позволяет взглянуть на обучение оценочных функций под другим углом.

\subsection{Eligibility Trace}

Рассмотрим случай $N = +\infty$, то есть допустим, что мы готовы обновлять V-функцию с reward-to-go. Наше рассуждение, можно сказать, позволяет теперь это делать, не доигрывая эпизоды до конца: мы сразу же в ходе эпизода можем уже <<начинать>> сдвигать V-функцию в правильном направлении. Это позволяет запустить <<как бы Монте-Карло>> алгоритм даже в неэпизодичных средах. Однако, на каждом шаге нам нужно перебирать все встретившиеся ранее в эпизоде состояния, чтобы обновить их, и, если эпизоды длинные (или среда неэпизодична), это хранение истории на самом деле становится избыточным. 

Допустим, мы сделали шаг в среде и получили на этом одном шаге какую-то одношаговую ошибку $\Psi_{(1)}$. Рассмотрим какое-нибудь состояние $s$. С каким весом, помимо learning rate, нужно добавить эту ошибку к нашей текущей аппроксимации? Это состояние в течение прошлого эпизода было, возможно, посещено несколько раз, и за каждое посещение вес увеличивается на $\gamma^K$, где $K$ --- число шагов с момента посещения. Такой счётчик можно сохранить в памяти: заведём вектор $e(s)$ размером с число состояний, проинициализируем его нулём и далее на $t$-ом шаге будем обновлять следующим образом:
\begin{equation}\label{montecarloeligibility}
e_t(s) \coloneqq \begin{cases}
\gamma e_{t - 1}(s) + 1 \quad & \text{если } s = s_t \\
\gamma e_{t - 1}(s) \quad & \text{иначе}
\end{cases}
\end{equation}
После этого на каждом шаге мы будем добавлять текущую одношаговую ошибку, временную разность $\Psi_{(1)}(s_t, a_t) \HM= r_t \HM+ \gamma V^(s_{t+1}) \HM- V(s_t)$, ко \textit{всем} состояниям с коэффициентом $e_t(s)$.

\begin{definition}
Будем называть $e_t(s)$ \emph{следом} (eligibility trace) для состояния $s$ в момент времени $t$ эпизода коэффициент, с которым алгоритм обновляет оценочную функцию $V(s)$ на $t$-ом шаге при помощи текущей одношаговой ошибки $\Psi_{(1)}(s_t, a_t)$:
\begin{equation}\label{eligibilitytraceupdate}
    V(s) \leftarrow V(s) + \alpha e_t(s) \Psi_{(1)}(s_t, a_t)
\end{equation}
\end{definition}

\needspace{7\baselineskip}
\begin{wrapfigure}{r}{0.3\textwidth}
\vspace{-0.8cm}
\centering
\includegraphics[width=0.25\textwidth]{Images/EligibilityTrace.png}
\vspace{-0.6cm}
\end{wrapfigure}

Допустим, эпизод доигран до конца, и мы в алгоритме используем формулу \eqref{eligibilitytraceupdate} со следом \eqref{montecarloeligibility}. Одношаговое обновление будет превращено в двухшаговое, двухшаговое --- в трёхшаговое и так далее до $N$-шагового, где $N$ --- количество шагов до конца эпизода. Таким образом (если в ходе таких обновлений learning rate и оцениваемая стратегия не меняется) наши обновления в точности соответствуют Монте-Карло.

Важно, что eligibility trace имеет физический смысл <<кредита>>, который выдан решениям, принятым в состоянии $s$: это та степень ответственности, с которой выбранные в этом состоянии действия влияют на события настоящего, на получаемые сейчас награды (временные разности). Действительно, обновление \eqref{eligibilitytraceupdate} говорит следующее: прогноз будущей награды в состоянии $s$ нужно увеличить с некоторым learning rate-ом на получаемую награду (<<$\Psi_{(1)}(s_t, a_t)$>>), домноженную на степень ответственности решений в $s$ за события настоящего (<<$e_t(s)$>>). И пока мы используем Монте-Карло обновления, кредит ведёт себя так: как только мы принимаем в $s$ решение, он увеличивается на единичку и дальше не затухает. Домножение на $\gamma$ можно не интерпретировать как затухание, поскольку это вызвано дисконтированием награды в нашей задаче, <<затуханием>> самой награды со временем. То есть мы рисуем мелом на стене <<я здесь был>>, и выдаём постоянный кредит этому состоянию.

А что, если мы не хотим использовать бесконечношаговые (Монте-Карло) обновления? Мы помним, что это одна крайность, в которой обновления имеют большую дисперсию. Можно броситься в другую крайность: если мы хотим ограничиться лишь, допустим, одношаговыми, то мы можем использовать просто <<другое>> определение следа:
$$e_t(s) \coloneqq \begin{cases}
1 \quad & \text{если } s = s_t \\
0 \quad & \text{иначе}
\end{cases}$$

То есть для одношаговых оценок нам нужно домножать след не на $\gamma$, а на ноль. Тогда вектор $e(s)$ на каждой итерации будет нулевым за исключением одного лишь только что встреченного состояния $s_t$, для которого он будет равен единице, и обновление \ref{eligibilitytraceupdate} будет эквивалентно обычному одношаговому temporal difference. В таком кредите решение, принятое в $s$, влияет только на то, что произойдёт на непосредственно том же шаге; <<мел на стене испаряется мгновенно>>.

\subsection{TD($\lambda$)}

Как можно интерполировать между Монте-Карло обновлениями и одношаговыми с точки зрения backward view? Раз eligibility trace может затухать в $\gamma$ раз, а может в 0, то, вероятно, можно тушить его и любым другим промежуточным способом. Так мы теперь можем придумать другой вид <<промежуточных оценок>> между Монте-Карло и одношаговыми. Пусть на очередном моменте времени след для состояния $s$ затухает сильнее, чем в $\gamma$ раз, но больше, чем в ноль; что тогда произойдёт?

\needspace{15\baselineskip}
\begin{wrapfigure}{r}{0.3\textwidth}
%\vspace{-0.5cm}
\centering
\includegraphics[width=0.3\textwidth]{Images/Traces.png}
\vspace{0.2cm}
\end{wrapfigure}

После момента посещения состояния $s$ след для него вырастет на единичку и оценочная функция обновится следующим образом:
$$V(s) \leftarrow V(s) + \alpha \Psi_{(1)}(s, a)$$
Допустим, на следующем шаге мы выбрали $\lambda_1 \HM \in [0, 1]$ --- <<коэффициент затухания>> --- и потушили след не с коэффициентом $\gamma$, а с коэффициентом $\gamma \lambda_1$. Тогда дальше мы добавим новую текущую временную разность $\Psi_{(1)}(s', a')$ с коэффициентом $\lambda \gamma$, получая суммарно следующее обновление:
$$V(s) \leftarrow V(s) + \alpha \left( \Psi_{(1)}(s, a) + \lambda_1 \gamma \Psi_{(1)}(s', a') \right),$$
которое в силу \eqref{NstepAdvSimplified} для $N \HM= 2$ преобразуется в:
$$V(s) \leftarrow V(s) + \alpha \left( (1 - \lambda_1) \Psi_{(1)}(s, a) + \lambda_1 \Psi_{(2)}(s, a) \right)$$

Таким образом, мы \emph{заансамблировали} одношаговое и двухшаговое обновление. Из этого видно, что такая процедура корректна: $V^\pi$ является неподвижной точкой как одношагового, так и двухшагового уравнения Беллмана, и значит неподвижной точкой любой их выпуклой комбинации.

С точки зрения кредита, мы сказали, что решение, принятое в $s$, влияет на то, что случится через 2 шага, но не так сильно, как на то, что случится через 1 шаг: степень ответственности за один шаг затухла в $\lambda_1$ раз. Мы пользуемся здесь следующим прайором из реальной жизни: решения более вероятно влияют на ближайшее будущее, нежели чем на далёкое.

Для понятности проведём ещё один шаг рассуждений. Допустим, мы сделали ещё один шаг в среде и увидели $\Psi_{(1)}(s'', a'')$; потушили eligibility trace $e(s)$, на этот раз, в $\gamma \lambda_2$ раз, где $\lambda_2 \HM \in [0, 1]$. Тогда след стал равен $e(s) \HM= \gamma^2 \lambda_1 \lambda_2$, и мы получим следующее обновление:
$$V(s) \leftarrow V(s) + \alpha \left( (1 - \lambda_1) \Psi_{(1)}(s, a) + \lambda_1 \Psi_{(2)}(s, a) + \gamma^2 \lambda_1 \lambda_2 \Psi_{(1)}(s'', a'') \right)$$
Вспоминая формулу \eqref{NtrasformtoNplusone}, последние два слагаемых преобразуются:
$$\Psi_{(2)}(s, a) + \gamma^2 \lambda_2 \Psi_{(1)}(s'', a'') = (1 - \lambda_2) \Psi_{(2)}(s, a) + \lambda_2 \Psi_{(3)}(s, a)$$
То есть долю $\lambda_2$ двухшаговой ошибки мы превращаем в трёхшаговое, а долю $1 \HM- \lambda_2$ --- нет. Суммарное обновление становится таким ансамблем:
$$V(s) \leftarrow V(s) + \alpha \left( (1 - \lambda_1) \Psi_{(1)}(s, a) + \lambda_1 (1 - \lambda_2) \Psi_{(2)}(s, a) + \lambda_1 \lambda_2 \Psi_{(3)}(s, a) \right)$$

И так далее. Интерпретировать это можно так. Если мы тушим след в $\gamma$ раз, то на очередном шаге обновления мы <<превращаем>> $N$-шаговое обновление в $N \HM+ 1$-шаговое. Если мы тушим след полностью, зануляя его, то мы <<отказываемся превращать>> $N$-шаговое обновление в $N \HM+ 1$-шаговое. Оба варианта корректны, и поэтому мы, выбирая $\lambda_t \in [0, 1]$, решаем заансамблировать их: мы возьмём и долю $\lambda_t$ $N$-шагового обновления <<превратим>> в $N \HM+ 1$-шаговое, а долю $1 \HM- \lambda_t$ трогать не будем и оставим без изменения. Поэтому $\lambda_t$ ещё называют <<коэффициентом смешивания>>.

Мы уже обсуждали, что странно проводить credit assingment нестационарно, то есть чтобы в процедуре была какая-то зависимость от времени, прошедшего с момента посещения состояния, поэтому коэффициент затухания $\lambda \HM\in [0, 1]$ обычно полагают не зависящем от момента времени, каким-то константным гиперпараметром, отвечающим за bias-variance trade-off. Естественно, $\lambda \HM= 1$ соответствует Монте-Карло обновлениям, $\lambda \HM= 0$ --- одношаговым.

\begin{definition}
Будем называть \emph{temporal difference обновлением} с параметром $\lambda \HM\in [0, 1]$ (<<обновление TD($\lambda$)) обновление \eqref{eligibilitytraceupdate} со следом $e_t(s)$, проинициализированным нулём и далее определённым следующим образом:
$$e_t(s) \coloneqq \begin{cases}
\gamma \lambda e_{t - 1}(s) + 1 \quad & \text{если } s = s_t \\
\gamma \lambda e_{t - 1}(s) \quad & \text{иначе}
\end{cases}$$
\end{definition}

Формула следа задаёт алгоритм в парадигме backward view. Естественно, что любая оценка, придуманная в терминах backward view, то есть записанная в терминах следа (в явном виде хранящего <<кредит>> ответственности решений для каждого состояния), переделывается в парадигме forward view (как и наоборот), когда мы, используя сэмплы будущего, строим некоторую оценку Advantage $\Psi(s, a) \HM \approx A^\pi(s, a)$ и просто сдвигаем по нему значение V-функции:
\begin{equation}\label{generalforwardviewupdate}
V(s) \leftarrow V(s) + \alpha \Psi(s, a)
\end{equation}
Какому обновлению в парадигме forward view соответствует обновление TD($\lambda$)?

\begin{theoremBox}[label=th:tdlambda]{Эквивалентные формы TD($\lambda$)}
Обновление TD($\lambda$) эквивалентно \eqref{generalforwardviewupdate} с оценкой
\begin{equation}\label{TDlambda} 
\Psi(s, a) \coloneqq \sum_{t \ge 0} \gamma^t \lambda^t \Psi_{(1)}(s^{(t)}, a^{(t)}) = (1 - \lambda) \sum_{t > 0} \lambda^{t-1} \Psi_{(t)}(s, a)
\end{equation}
\begin{proof}
Составим такую табличку: какое суммарное обновление у нас получается в TD($\lambda$) после $t$ шагов в среде. Справа запишем, с какими весами входят разные $N$-шаговые оценки в получающийся ансамбль.
\begin{center}
\begin{tabular}{|c|c||c|c|c|c|c|}
\hline
\small \textbf{Step} & \small \textbf{Update} & \small \textbf{$\Psi_{(1)}(s, a)$} & \small \textbf{$\Psi_{(2)}(s, a)$} &
\textbf{$\Psi_{(3)}(s, a)$} & 
\textbf{ \dots } & \small \textbf{$\Psi_{(N)}(s, a)$}\\
\hline
1 & $\Psi_{(1)}(s, a)$ &
1 & 0 & 0 & & 0 \\
\hline
2 & $\Psi_{(1)}(s, a) + \gamma \lambda \Psi_{(1)}(s', a')$ &
$1 - \lambda$ & $\lambda$ & 0 & & \\
\hline
\multirow{2}{*}{3} & $\Psi_{(1)}(s, a) + \gamma \lambda \Psi_{(1)}(s', a') +$ &
\multirow{2}{*}{$1 - \lambda$} & \multirow{2}{*}{$(1 - \lambda)\lambda$} & \multirow{2}{*}{$\lambda^2$} & & \multirow{2}{*}{0} \\
& $+ (\gamma \lambda)^2 \Psi_{(1)}(s'', a'')$ &
 & & & &  \\
\hline
$\vdots$ & & & & &  $\ddots$ & \\
\hline
$N$ & $\sum^N_{t \ge 0} (\gamma \lambda)^t \Psi_{(1)}(s^{(t)}, a^{(t)})$ &
$1 - \lambda$ & $(1 - \lambda)\lambda$ & $(1 - \lambda)\lambda^2$ & & $\lambda^N$ \\
\hline
\end{tabular}
\end{center}

Продолжая строить такую табличку, можно по индукции увидеть, что после окончания эпизода суммарное обновление V-функции получится следующим:
$$V(s) \leftarrow V(s) + \alpha \sum_{t \ge 0} (\gamma \lambda)^t \Psi_{(1)}(s^{(t)}, a^{(t)})$$
Это и есть суммарное обновление V-функции по завершении эпизода.
\end{proof}
% \begin{proof} И справа, и слева в сумме стоит слагаемые двух видов: награды за шаг $r^{(t)}$ (для $t \ge 0$) и Q-оценки $Q(s^{(t)}, a^{(t)})$ с какими-то весами. Поймём, что они стоят с одними и теми же весами.

% Справа награда за шаг $r^{(t)}$ участвует во всех слагаемых, начиная с $t + 1$-го, во всех них дисконтирована на $\gamma^t$ и входит в ансамбль с весом $(1 - \lambda)(\lambda^t + \lambda^{t+1} + \dots)$. Итого её вес --- $\gamma^t\lambda^t$. А слева эта награда входит только в одно слагаемое: в одношаговую оценку для $\Psi_{(1)}(s^{(t)}, a^{(t)})$, и с тем же весом.

% Справа аппроксимация $Q(s^{(t)}, a^{(t)})$ при $t > 0$ входит только в одно слагаемое, в $\Psi_{(t)}(s, a)$, с весом $(1 - \lambda)\lambda^{t - 1}\gamma^t$. Слева $Q(s^{(t)}, a^{(t)})$ входит в два слагаемых: в $\Psi_{(1)}(s^{(t-1)}, a^{(t-1)})$ с положительным весом $\gamma^t\lambda^{t-1}$ и в $\Psi_{(1)}(s^{(t)}, a^{(t)})$ с отрицательным весом $-\gamma^t\lambda^t$. Сумма этих двух коэффициентов совпадает с $(1 - \lambda)\lambda^{t - 1}\gamma^t$.

% Наконец, $-Q(s, a)$ входит справа во все слагаемые с суммарным весом $(1 - \lambda)(1 + \lambda + \lambda^2 + \dots) = 1$, а слева он присутствует только в $\Psi_{(1)}(s, a)$, тоже с весом 1.
% \end{proof}
\end{theoremBox}

Итак, полученная формула обновления имеет две интерпретации. Получается, что подобный ансамбль многошаговых оценок эквивалентен дисконтированной сумме будущих одношаговых ошибок (<<модифицированных наград>> с потенциалом $V(s)$), где коэффициент дисконтирования равен $\gamma \lambda$; исходный коэффициент $\gamma$ отвечает за затухание ценности наград со временем из исходной постановки задачи, а $\lambda$ соответствует затуханию кредита ответственности действия за полученные в будущем награды.

\needspace{5\baselineskip}
\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-0.4cm}
\centering
\includegraphics[width=0.4\textwidth]{Images/td_lambda.png}
\vspace{-0.7cm}
\end{wrapfigure}

А с другой стороны можно интерпретировать TD($\lambda$) как ансамбль многошаговых оценок разной длины. Мы взяли одношаговую оценку с весом $\lambda$, двухшаговую с весом $\lambda^2$, $N$-шаговую с весом $\lambda^N$ и так далее. Сумма весов в ансамбле, как водится, должна равняться единице, отсюда в формуле домножение на $1 - \lambda$.

Если через $N$ шагов после оцениваемого состояния $s$ эпизод закончился, то все многошаговые оценки длины больше $N$, понятно, совпадают с $N$-шаговой и равны reward-to-go. Следовательно, в такой ситуации reward-to-go по определению замешивается в оценку с весом $(1 - \lambda)(\lambda^{N-1} + \lambda^{N} + \dots) \HM= \lambda^{N-1}$.

Мы привыкли, что любая формула обновления для нас --- стохастическая аппроксимация решения какого-то уравнения. TD($\lambda$) не исключение. Если $N$-шаговая оценка направляет нас в сторону решения $N$-шагового уравнения Беллмана $V^{\pi} \HM= \B^N V^{\pi}$, то ансамбль из оценок направляет нас в сторону решения ансамбля $N$-шаговых уравнений Беллмана:
\begin{equation}\label{bellman_ensemble}
V^{\pi} = (1 - \lambda)(\B V^{\pi} + \lambda \B^2 V^{\pi} + \lambda^2 \B^3 V^{\pi} + \dots ) = (1 - \lambda) \sum_{N > 0} \lambda^{N-1} \B^N V^{\pi}
\end{equation}

Поскольку $V^{\pi}$ является неподвижной точкой для операторов $\B^N$ для всех $N$, то и для их выпуклой комбинации, <<ансамбля>>, он тоже будет неподвижной точкой. Итого TD($\lambda$) дало нам прикольную идею: мы не могли выбрать одну из многошаговых оценок, и поэтому взяли их все сразу.

Итак, мы получили алгоритм TD($\lambda$) оценивания стратегии, или temporal difference с параметром $\lambda$, который при $\lambda \HM= 1$ эквивалентен Монте-Карло алгоритму (с постоянным обновлением V-функции <<по ходу эпизода>>), а при $\lambda \HM = 0$ эквивалентен одношаговому temporal difference методу, который мы, в частности, применяли в Q-learning и SARSA для оценивания стратегии.

\begin{algorithm}[label=alg:tdlambda]{TD($\lambda$)}
\textbf{Вход:} $\pi$ --- стратегия \\
\textbf{Гиперпараметры:} $\alpha$ --- параметр экспоненциального сглаживания, $\lambda \in [0, 1]$ --- степень затухания следа

\vspace{0.3cm}
Инициализируем $V(s)$ произвольно для всех $s \in \St$ \\
Инициализируем $e(s)$ нулём для всех $s \in \St$ \\
Наблюдаем $s_0$ \\
\textbf{На $k$-ом шаге:}
\begin{enumerate}
    \item выбираем $a_{k} \sim \pi(a_{k} \mid s_{k})$
    \item играем $a_k$ и наблюдаем $r_k, s_{k+1}$
    \item обновляем след $e(s_k) \leftarrow e(s_k) + 1$
    \item считаем одношаговую ошибку $\Psi_{(1)} \coloneqq r_k + \gamma V(s_{k+1}) - V(s_k)$
    \item для всех $s$ обновляем $V(s) \leftarrow V(s) + \alpha e(s) \Psi_{(1)}$
    \item для всех $s$ обновляем $e(s) \leftarrow \gamma \lambda e(s)$
\end{enumerate}

\vspace{0.3cm}
\textbf{Выход:} $V(s)$
\end{algorithm}

Мы обсуждали и выписали этот алгоритм для V-функции для задачи именно оценивания стратегии; естественно, мы могли бы сделать это для Q-функции или добавить policy improvement после, например, каждого шага в среде, получив табличный алгоритм обучения стратегии. Позже в разделе \ref{subsec:retrace} мы рассмотрим формулировку теоремы о сходимости таких алгоритмов для ещё более общей ситуации.

Очевидно, TD($\lambda$) обновление не эквивалентно никаким $N$-шаговым temporal difference формулам: в нём замешана как Монте-Карло оценка, то есть замешана вся дальнейшая награда (весь будущий сигнал), так и приближения V-функции во всех промежуточных состояний (при любом $\lambda \in (0, 1)$). Гиперпараметр $\lambda$ также не имеет смысла времени, и поэтому на практике его легче подбирать.

\begin{remark}
Полезность TD($\lambda$) в том, что $\lambda$ непрерывно и позволяет более гладкую настройку <<длины следа>>. На практике алгоритмы будут чувстительны к выбору $\lambda$ в намного меньшей степени, чем к выбору $N$. При этом даже если $\lambda < 1$, в оценку <<поступает>> информация о далёкой награде, и использование TD($\lambda$) позволит бороться с проблемой распространения сигнала.
\end{remark}

Всюду далее в ситуациях, когда нам понадобится разрешать bias-variance trade-off, мы будем обращаться к формуле forward view TD($\lambda$) обновления \eqref{TDlambda}. Однако как отмечалось ранее, для работы с многошаговыми оценками, а следовательно и с обновлением \eqref{TDlambda} TD($\lambda$), необходимо работать в on-policy режиме, то есть иметь сэмплы взаимодействия со средой именно оцениваемой стратегии $\pi(a \mid s)$, и поэтому возможность разрешать bias-variance trade-off у нас будет только в on-policy алгоритмах.

\subsection{Retrace($\lambda$)}\label{subsec:retrace}

Что же тогда делать в off-policy режиме? Итак, пусть дан роллаут $s, a, r, s', a', r', s'', \dots$, где действия сэмплированы из стратегии $\mu$. Мы хотим при этом провести credit assingment для стратегии $\pi$, то есть понять, как обучать $V \HM\approx V^\pi$ или $Q \HM\approx Q^\pi$. 

Проблема в том, что на самом деле это не всегда даже в принципе возможно. Представим, что $a$ таково, что $\pi(a \HM\mid s) \HM= 0$. Тогда в роллауте хранится информация о событиях, которые произойдут с агентом, использующем стратегию $\pi$, с вероятностью 0. Понятно, что никакой полезной информации о том, как менять аппроксимацию V-функции, мы из таких данных не получим. Поэтому для детерминированных стратегий задача обучения с многошаговых оценок в off-policy режиме может запросто оказаться бессмысленной.

\begin{example}
Допустим, стратегия $\mu$ с вероятностью один в первом же состоянии прыгает в лаву. Мы же хотим посчитать $V^\pi(s)$, будущую награду, для стратегии $\pi$, которая с вероятностью один не прыгает в лаву, а кушает тортики. Поскольку в задаче RL функция переходов $p(s' \HM \mid s, a)$ для разных $a$ может быть произвольно разной, мы ничего не можем сказать о ценности одного действия по информации о другом действии.
\end{example}

Возможность в принципе обучаться off-policy в Q-learning обеспечивалась тем, что, когда мы учим Q-функцию с одношаговых оценок, нам для любых $s, a$, которые можно взять из буфера, достаточно лишь сэмпла $s'$. При этом сэмпл $a' \HM \sim \pi(a' \HM\mid s')$ мы всегда сможем сгенерировать <<онлайн>>, прогнав на взятом из буфера $s'$ оцениваемую стратегию. Обычно в off-policy режиме учат именно Q-функцию, что важно в том числе тем, что по крайней мере одношаговая оценка будет доступна всегда. Если же $a'$ из буфера такого, что $\pi(a' \HM \mid s') \HM= 0$, то любые наши коррекции схлопнутся в одношаговую оценку (или же будут теоретически некорректны), но по крайней мере хоть что-то мы сможем сделать.

Итак, попробуем разрешить bias-variance trade-off-а для Q-функции. Для этого снова вернёмся к идеи следа. Допустим, для взятых из буфера $s, a, r, s'$ мы составили одношаговое обновление:
$$Q(s, a) \leftarrow Q(s, a) + \alpha (r + \gamma Q(s', a'_\pi) - Q(s, a)),$$
где $a'_\pi \HM \sim \pi(\cdot \HM \mid s')$, положив неявно значение следа $e(s, a) \HM= 1$ (след при обучении Q-функции зависит от пары $s, a$). Также лучше, если есть такая возможность, использовать не сэмпл $a'_\pi$, а усреднить по нему (аналогично тому, как было проделано в формуле \eqref{expectedsarsa} Expected SARSA):
\begin{equation}\label{offpolicytwostepupdatefirstpart}
Q(s, a) \leftarrow Q(s, a) + \alpha (r + \gamma \E_{a'_\pi \sim \pi} Q(s', a'_\pi) - Q(s, a))
\end{equation}

Затем возьмём из буфера $a', r', s''$. Какое значение следа мы можем выбрать? Можно $e(s, a) \HM= 0$, оставив одношаговое обновление и <<не превращая>> его в двухшаговое, это одна крайность. А как превратить обновление в двухшаговое целиком? Хотелось бы прибавить
$$\gamma (r' + \gamma \E_{a''_\pi \sim \pi} Q(s'', a''_\pi) - \E_{a'_\pi \sim \pi} Q(s', a'_\pi)),$$
где $s'' \HM\sim p(s'' \mid s', a'_\pi)$. Однако в буфере у нас нет такого сэмпла, а вместо него есть сэмпл $s'' \HM\sim p(s'' \mid s', a')$. Технически, ошибка за второй шаг является случайной величиной от $a'$, который должен приходить из $\pi$, когда у нас есть сэмпл лишь из $\mu$. Поэтому здесь необходимо применить importance sampling коррекцию, после которой ошибка за второй шаг принимает следующий вид\footnote{видно, что в отношении последнего слагаемого возможны вариации, например, ошибку за второй шаг можно оценить как
$$\gamma \frac{\pi(a' \mid s')}{\mu(a' \mid s')} \left( r' + \gamma \E_{a''_\pi \sim \pi} Q(s'', a''_\pi) - \E_{a'_\pi \sim \pi} Q(s', a'_\pi) \right),$$
или даже не корректировать последнее слагаемое importance sampling коррекцией, так как в нём не требуется брать $a'$ обязательно из буфера:
$$\gamma \frac{\pi(a' \mid s')}{\mu(a' \mid s')} \left( r' + \gamma \E_{a''_\pi \sim \pi} Q(s'', a''_\pi)\right) - \gamma \E_{a'_\pi \sim \pi} Q(s', a'_\pi).$$
Далее в формулах предполагается вариант, рассматриваемый в статье про Retrace($\lambda$). 
}:
\begin{equation}\label{offpolicytwostepupdatesecondpart}
Q(s, a) \leftarrow Q(s, a) + \alpha \gamma \frac{\pi(a' \mid s')}{\mu(a' \mid s')} \left( r' + \gamma \E_{a''_\pi \sim \pi} Q(s'', a''_\pi) - Q(s', a') \right),
\end{equation}
где $a', r', s''$ --- взяты из буфера. 

\begin{proposition}
Последовательное применение обновлений \eqref{offpolicytwostepupdatefirstpart} и \eqref{offpolicytwostepupdatesecondpart} является корректным двухшаговым обновлением, то есть эквивалентно
$$Q(s, a) \leftarrow Q(s, a) + \alpha (y(Q) - Q(s, a)),$$
где среднее значение $y(Q)$ по всей заложенной в ней стохастике является правой частью двухшагового уравнения Беллмана для Q-функции:
$$\E y(Q) = \E_{s'}\E_{a' \sim \pi} \E_{s''}\E_{a'' \sim \pi} \left[ r(s, a) + \gamma r(s', a') + \gamma^2 Q(s'', a'') \right]$$
\begin{proof}
После двух рассматриваемых обновлений получается, что <<целевая переменная>>, таргет, равен:
$$y(Q) = r + \gamma \E_{a'_\pi \sim \pi} Q(s', a'_\pi) + \gamma \frac{\pi(a' \mid s')}{\mu(a' \mid s')} \left( r' + \gamma \E_{a''_\pi \sim \pi} Q(s'', a''_\pi) - Q(s', a') \right),$$
где случайными величинами являются $s', a', s''$, и $a' \sim \mu(a' \mid s')$. Возьмём среднее по этим величинам:
$$\E y(Q) = \E_{s'} \left[ r + \gamma \E_{a'_\pi \sim \pi} Q(s', a'_\pi) + \gamma \E_{a' \sim \mu} \frac{\pi(a' \mid s')}{\mu(a' \mid s')} \left( r' + \gamma \E_{s''} \E_{a''_\pi \sim \pi} Q(s'', a''_\pi) - Q(s', a') \right) \right]$$
Раскроем importance sampling коррекцию, воспользовавшись
$$\E_{a' \sim \mu} \frac{\pi(a' \mid s')}{\mu(a' \mid s')} f(a') = \E_{a' \sim \pi} f(a'),$$
получим:
$$\E y(Q) = \E_{s'} \E_{a' \sim \pi} \E_{s''} \E_{a''_\pi \sim \pi} \left[ r + \gamma Q(s', a') + \gamma \left( r' + \gamma  Q(s'', a''_\pi) - Q(s', a') \right) \right]$$
Осталось заметить, что слагаемое $\gamma Q(s', a')$ по аналогии с on-policy режимом сокращается.
\end{proof}
\end{proposition}

Таким образом, одношаговую ошибку за второй шаг для получения полного превращения одношагового обновления в двухшаговое необходимо добавить не с весом $\gamma$, как в on-policy режиме, а с весом $\gamma \frac{\pi(a' \mid s')}{\mu(a' \mid s')}$.

Продолжая рассуждение дальше, можно получить, что одношаговая ошибка через $t$ шагов после выбора оцениваемого действия $a$ в состоянии $s$ в off-policy режиме зависит от случайных величин $s', a', s'', \dots s^{(t + 1)}$, и поэтому importance sampling коррекция для неё будет равна:
$$\prod_{i = 1}^{\hat{t} = t} 
\frac{\pi(a^{(i)} \mid s^{(i)}) p(s^{(i + 1)} \mid s^{(i)}, a^{(i)})}{\mu(a^{(i)} \mid s^{(i)}) p(s^{(i + 1)} \mid s^{(i)}, a^{(i)})} = \prod_{i = 1}^{i = t} \frac{\pi(a^{(i)} \mid s^{(i)}) }{\mu(a^{(i)} \mid s^{(i)})}$$
Здесь и далее считается, что при $t \HM= 0$ подобные произведения равны единице.

Получается, что для того, чтобы строить оценку максимальной длины, нужно на $t$-ом шаге домножать след на
$\gamma \frac{\pi(a^{(t)} \mid s^{(t)})}{\mu(a^{(t)} \mid s^{(t)})}$. Итоговую формулу обновления часто записывают в следующем виде:

\begin{equation}\label{importancesamplingoffpolicycredit}
Q(s, a) \leftarrow Q(s, a) + \alpha 
\sum_{t \ge 0} \gamma^t  \left( \prod_{i = 1}^{i = t} \frac{\pi(a^{(i)} \mid s^{(i)}) }{\mu(a^{(i)} \mid s^{(i)})} \right) \Psi_{(1)}(s^{(t)}, a^{(t)}),
\end{equation}
где
\begin{equation}\label{onestepoffpolicyQdelta}
\Psi_{(1)}(s^{(t)}, a^{(t)}) = r^{(t)} + \gamma \E_{a^{(t+1)}_\pi \sim \pi} Q(s^{(t+1)}, a^{(t+1)}_\pi) - Q(s^{(t)}, a^{(t)})    
\end{equation}


Мы получили <<off-policy>> Монте-Карло оценку в терминах следа. Теперь по аналогии с TD($\lambda$) проинтерполируем между одношаговыми (где след зануляется после первого шага) и бесконечношаговыми обновлениями (где след есть importance sampling дробь): оказывается, оценка будет корректна при любом промежуточном значении следа. Чтобы записать это формально, перепишем формулу \eqref{importancesamplingoffpolicycredit} в следующем виде:
\begin{equation}\label{offpolicyestimators}
Q(s, a) \leftarrow Q(s, a) + \alpha 
\sum_{t \ge 0} \gamma^t  \left( \prod_{i = 1}^{i = t}  c_{i} \right) \Psi_{(1)}(s^{(t)}, a^{(t)}),
\end{equation}
где $c_i$ --- коэффициенты затухания следа: в on-policy они могли быть в диапазоне $[0, 1]$ (и мы выбирали его равным гиперпараметру $\lambda$), а здесь, в off-policy режиме, он может быть в диапазоне
$$c_i \in \left[ 0, \frac{\pi(a^{(i)} \mid s^{(i)}) }{\mu(a^{(i)} \mid s^{(i)})} \right]$$

То, что при любом выборе способа затухания следа табличные алгоритмы, использующие оценку \eqref{offpolicyestimators}, будут сходиться --- один из ключевых и самых общих результатов табличного RL. Приведём несколько нестрогую формулировку этой теоремы:

\begin{theorem}[Retrace]
Пусть число состояний и действий конечно, таблица $Q_0(s, a)$ проинициализирована произвольно. Пусть на $k$-ом шаге алгоритма для каждой пары $s, a$ ячейка таблицы обновляется по формуле
$$Q_{k+1}(s, a) \leftarrow Q_k(s, a) + \alpha_k(s, a)
\sum_{t \ge 0} \gamma^t \left( \prod_{i = 1}^{i = t}  c_{i} \right) \Psi_{(1)}(s^{(t)}, a^{(t)}),$$
$$\Psi_{(1)}(s^{(t)}, a^{(t)}) = r^{(t)} + \gamma \E_{a^{(t+1)}_\pi \sim \pi_k} Q_k(s^{(t+1)}, a^{(t+1)}_\pi) - Q(s^{(t)}, a^{(t)})$$
где $\Traj \HM\sim \mu_k \HM\mid s_0 \HM= s, a_0 \HM= a$ сгенерирована произвольной стратегией сбора данных $\mu_k$ (причём не обязательно стационарной), learning rate $\alpha_k(s, a)$ --- случайно, $\pi_k$ произвольно, и коэффициенты следа --- любые в диапазоне
$$c_i \in \left[ 0, \frac{\pi_k(a^{(i)} \mid s^{(i)}) }{\mu_k(a^{(i)} \mid s^{(i)})} \right].$$
Тогда, если с вероятностью 1 learning rate удовлетворяет условиям Роббинса-Монро \eqref{RobbinsMonro}, а стратегия $\pi_k$ c вероятностью 1 становится жадной по отношению к $Q_k$ в пределе $k \HM\to \infty$, то при некоторых технических ограничениях с вероятностью 1 $Q_k$ сходится к оптимальной Q-функции $Q^*$, а $\pi_k$, соответственно, к оптимальной стратегии.
\begin{proof}[Без доказательства; интересующиеся могут обратиться к оригинальной \href{https://arxiv.org/abs/1606.02647}{статье по Retrace}]
\end{proof}
\end{theorem}

Пользуясь этой теоремой, мы можем в полной аналогии с TD($\lambda$) выбрать гиперпараметр $\lambda \in [0, 1]$, и считать след по формуле
$$
    c_i = \lambda \frac{\pi(a^{(i)} \mid s^{(i)}) }{\mu(a^{(i)} \mid s^{(i)})}
$$
В частности, в on-policy режиме $\pi \HM \equiv \mu$ мы получим коэффициент затухания следа, равный просто $\lambda$. Это очень удобно, но на практике неприменимо.

Такая оценка страдает сразу от двух проблем. Первая проблема --- типичная: \emph{затухающий след} (vanishing trace), когда $\mu(a^{(t)} \mid s^{(t)}) \HM\gg \pi(a^{(t)} \mid s^{(t)})$ для какого-то $t$, и соответствующий множитель близок к нулю. Такое случится, если, например, $\mu$ выбирает какие-то действия, которые $\pi$ выбирает редко, что типично. В предельном случае для детерминированных стратегий может быть такое, что числитель коэффициента равен нулю ($\pi$ не выбирает такого действия никогда), и коррекция скажет, что вся информация, начиная с этого шага, полностью неактуальна. Эта проблема неизбежна.

Вторая проблема --- \emph{взрывающийся след} (exploding trace): $\mu(a^{(t)} \mid s^{(t)}) \HM\ll \pi(a^{(t)} \mid s^{(t)})$. Такое может случиться, если в роллауте попалось редкое для $\mu$ действие, которое тем не менее часто выполняется оцениваемой стратегией $\pi$. С одной стороны, кажется, что как раз такие роллауты наиболее ценны для обучения $Q^\pi$, ведь в них описывается шаг взаимодействия со средой, который для $\pi$ как раз достаточно вероятен. Но на практике взрывающийся importance sampling коэффициент --- источник большой дисперсии рассматриваемой оценки.

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Название} & \textbf{Коффициенты $c_i$} & \textbf{Проблема}\\
\hline
TD($\lambda$) & $\lambda$ &
только on-policy режим \\
\hline
Одношаговые & 0 &
сильное смещение \\
\hline
Importance Sampling & $\lambda \frac{\pi(a^{(i)} \mid s^{(i)}) }{\mu(a^{(i)} \mid s^{(i)})}$ &
легко взрываются \\
\hline
\end{tabular}
\end{center}

Идея борьбы со взрывающимся коэффициентом, предложенной в оценке Retrace($\lambda$), заключается в следующем: если importance sampling коррекция для $t$-го шага взорвалась, давайте воспользуемся тем, что мы теоретически обоснованно можем выбрать любой коэффициент меньше (<<быстрее>> потушить след), и выберем единицу:
\begin{equation}\label{retracecoeff}
c_i \coloneqq \lambda \min \left( 1, \frac{\pi(a^{(i)} \mid s^{(i)}) }{\mu(a^{(i)} \mid s^{(i)})} \right) 
\end{equation}

Коррекция с такими коэффициентами корректна, стабильна, но, конечно, никак не помогает с затуханием следа. Важно помнить, что если $\pi$ и $\mu$ сильно отличаются, то велика вероятность, что коэффициенты затухания будут очень близки к нулю, и мы получим что-то, парктически всегда похожее на одношаговое обновление. С этим мы фундаментально не можем ничего сделать, хотя из-за этого итоговая формула обновления получается сильно смещённой. По этой причине смысла выбирать $\lambda \HM< 1$ в off-policy режиме обычно мало, и его почти всегда полагают равным единицей.

Также обсудим, что говорит формула Retrace в ситуации, когда оцениваемая политика $\pi$ детерминирована. Если на шаге $t$ политика сбора данных $\mu$ выбрала ровно то действие, которое выбирает политика $\pi$, то коэффициент $c_i \HM= \lambda$, то есть ситуация совпадает с on-policy режимом (действительно, в дискретных пространствах действий в числителе единица, а в знаменателе что-то меньшее единицы, когда в непрерывных пространствах действий числитель технически равен бесконечности, поэтому мы обрежем дробь до единицы). В противном же случае дробь $\frac{\pi(a^{(i)} \mid s^{(i)}) }{\mu(a^{(i)} \mid s^{(i)})}$ имеет ноль в числителе, и след занулится. Таким образом, пока в буфере в цепочке $s', a', s'', a'', \cdots$ записанные действия совпадают с теми, которые выбирает детерминированная $\pi$, мы <<пользуемся TD($\lambda$)>>, но вынуждены оборвать след, как только очередное действие разойдётся. Есть некоторая польза в том, чтобы в алгоритме $\pi$ и $\mu$ были стохастичны: тогда след по крайней мере полностью никогда не затухнет.

Мы дальше в off-policy будем обсуждать в основном одношаговые оценки, в том числе для простоты. Из одношаговости будут вытекать все ключевые недостатки таких алгоритмов, связанные со смещённостью подобных оценок и невозможностью полноценно разрешать bias-variance trade-off. Во всех этих алгоритмах с этой проблемой можно будет частично побороться при помощи идей Retrace($\lambda$).  

